;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;; paper;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;    (paper.instance    abiteboul)    (paper.bibtex      abiteboul book)    (paper.author      abiteboul "Serge Abiteboul and Richard Hull and Victor Vianu")    (paper.title       abiteboul "Foundations of Databases")    (paper.publisher   abiteboul "Addison-Wesley Publishing Company")    (paper.link        abiteboul "http://www.amazon.com/Foundations-Databases-Logical-Serge-Abiteboul/dp/0201537710")    (paper.year        abiteboul 1995)    (paper.description abiteboul "
The definitive guide for comparing various database query languages.
")    (paper.topic       abiteboul "")    (paper.instance    agrawal2000)    (paper.bibtex      agrawal2000 inproceedings)    (paper.author      agrawal2000 "Agrawal and Chaudrhuri and Narasavya")    (paper.title       agrawal2000 "Automatic selection of materialized views and indexes in Microsoft SQL Server")    (paper.publication agrawal2000 "In proceedings of Very Large Databases (VLDB)")    (paper.startpage   agrawal2000 496)    (paper.endpage     agrawal2000 505)    (paper.year        agrawal2000 2000)    (paper.description agrawal2000 "
Another paper on view selection problem in databases.
")    (paper.topic       agrawal2000 "Databases")    (paper.instance    aiello80)    (paper.bibtex      aiello80 inproceedings)    (paper.author      aiello80 "Luigia Aiello")    (paper.title       aiello80 "Automatic Generation of Semantic Attachments in FOL")    (paper.publication aiello80 "AAAI")    (paper.year        aiello80 1980)    (paper.rank        aiello80 "[**]")    (paper.description aiello80 "
Aiello describes a process of generating new semantic attachments from old
ones by compiling FOL into Lisp.
")    (paper.topic       aiello80 "Theorem Proving with Attachments")    (paper.instance    alferes94)    (paper.author      alferes94 "J.J. Alferes, R. Li and L.M. Pereira")    (paper.title       alferes94 "Concurrent Actions and Changes in the Situation Calculus")    (paper.publication alferes94 "Proceedings of IBERAMIA 94")    (paper.startpage   alferes94 93)    (paper.endpage     alferes94 104)    (paper.link        alferes94 "http://centria.fct.unl.pt/~lmp/publications/Biblio.html")    (paper.year        alferes94 1994)    (paper.description alferes94 "
Alferes shows that Situation Calculus extended with concurrent actions (e.g. buytoothpaste||buyshampoo) is no more expressive than standard Situation Calculus.  Intuitively, give names to every subset of actions since there are only 2^n such subsets of n actions.  Use those names as the actions in situation calculus.  The trouble here is how do we produce the effect axioms of a concurrent action set given the effect axioms of the individual actions?  Some of these axioms simply must be given by the user, e.g. buying toothpaste and shampoo at the same time might result in a free bar of soap, though neither effect axiom for buying toothpaste or shampoo would mention soap.  For those actions whose composition (in Alferes terms) can be determined automatically, the authors offer a solution using nonmonotonic logic: if Holds(p, Result(a, Result(b,s))) and Holds(p, Result(b, Result(a,s))) and there is no definition for Holds(p, Result(a||b),s), then conclude Holds(p, Result(a||b,s)).  Alferes elaborates the rule to include preconditions and subsets of size other than 2 (though some technical details arise in doing the latter).
")    (paper.topic       alferes94 "Fundamentals")    (paper.instance    allen90)    (paper.author      allen90 "J. Allen, J. Hendler and A. Tate")    (paper.title       allen90 "Readings in Planning")    (paper.link        allen90 "http://www.amazon.com/exec/obidos/ASIN/1558601309/qid%3D954347291/sr%3D1-13/002-1714614-2412067")    (paper.year        allen90 1990)    (paper.description allen90 "
Survey book on planning through 1990.
")    (paper.topic       allen90 "Historical")    (paper.instance    allwein96)    (paper.bibtex      allwein96 book)    (paper.author      allwein96 "Gerard Allwein and Jon Barwise")    (paper.title       allwein96 "Logical Reasoning with Diagrams")    (paper.publisher   allwein96 "Oxford University Press")    (paper.link        allwein96 "http://www.amazon.com/exec/obidos/tg/detail/-/0195104277/104-6992918-7685512?vi=glance")    (paper.year        allwein96 1996)    (paper.rank        allwein96 "[***]")    (paper.description allwein96 "
This book advocates the appropriate use of diagrams in formal, rigorous
proofs.  It formalizes the notions of entailment from a diagram and 
adjustments to a diagram.  The formalization is pretty complex, but
it is not obvious how to simplify it.  The book also looks at various
logics that can incorporate diagramatic methods.  The last two sections
are non-theoretical: a series of case studies and a series of heterogeneous
systems.  The bibiliography looks helpful.
")    (paper.topic       allwein96 "Graphical Reasoning Techniques")    (paper.instance    alsac2002)    (paper.author      alsac2002 "Guray Alsac and Chitta Baral ")    (paper.title       alsac2002 "Reasoning in description logics using declarative logic programming")    (paper.link        alsac2002 "http://citeseer.ist.psu.edu/context/2398354/0")    (paper.year        alsac2002 2002)    (paper.description alsac2002 "
The authors discuss a nontrivial translation of a particular 
description logic into logic programming.
")    (paper.topic       alsac2002 "Logic Programming")    (paper.instance    amir2001)    (paper.bibtex      amir2001 phdthesis)    (paper.author      amir2001 "Eyal Amir")    (paper.title       amir2001 "Dividing and Conquering Logic")    (paper.publisher   amir2001 "Stanford University")    (paper.link        amir2001 "")    (paper.year        amir2001 2001)    (paper.description amir2001 "
Amir's thesis considers several problems related to the construction and
reasoning about large knowledge bases.  First he discusses object-oriented
first-order logic, a synthesis of the object oriented paradigm and
FOL.  Second he applies this logic to theories of action, most notably
situation calculus.  Third he shows how to parallelize automated
theorem proving where a theory has been partitioned into pieces.  Fourth
he gives algorithms for automatically decomposing theories, based on
the syntactic character of the axioms.  Last he describes a logic-based
subsumption architecture for automated robot control, similar in spirit
to Golog.  An appendix on logic, one on reactive control systems, and
30 pages of references round out the thesis.  It should be noted
that while he does augment various resolution procedures to work
in this parallel setting, the rest of the results are independent of
a particular calculus.
")    (paper.topic       amir2001 "Logical Knowledge Representation")    (paper.instance    arenas98)    (paper.author      arenas98 "Marcello Arenas and Leopoldo Bertossi")    (paper.title       arenas98 "Hypothetical Temporal Reasoning with History Encoding")    (paper.publication arenas98 "Knowledge Representation Meets Database")    (paper.volume      arenas98 "4.1-4.8")    (paper.link        arenas98 "http://citeseer.ist.psu.edu/529093.html")    (paper.year        arenas98 1998)    (paper.rank        arenas98 "[***]")    (paper.description arenas98 "
Arenas and Bertossi build on J. Chomicki's work of progressing a history of a database transaction's effects through the execution of a transaction to save on the costs of doing temporal constraint checking.  They also use Reiter's work on evaluating a query in the state that results after executing a sequence of actions.  The authors meld these two approaches to answer queries about the evolution of a database that contain quantification over states (for two specific types of queries).  This work also leads to a process for transforming dynamic integrity constraints into static ones and transforming history dependent action preconditions into local preconditions. 
")    (paper.topic       arenas98 "Hypothetical Temporal DB Queries")    (paper.related     arenas98 chomicki95)    (paper.instance    arenas98b)    (paper.author      arenas98b "Marcelo Arenas, Leopoldo Bertossi and Javier Pinto")    (paper.title       arenas98b "Representation of Temporal Knowledge in the Situation Calculus")    (paper.link        arenas98b "http://citeseer.ist.psu.edu/527517.html")    (paper.year        arenas98b 1998)    (paper.rank        arenas98b "[***]")    (paper.description arenas98b "
This incomplete paper describes translating a first-order modal temporal logic, Metric Time Temporal Logic, into situation calculus in the context of database updates.  The key insight is the creation of new relations that summarize information about the past.  Doing this gives us the power to write non-Markovian axioms (those that depend on a history of states instead of a just the last state).  
")    (paper.topic       arenas98b "Hypothetical Temporal DB Queries")    (paper.instance    armando93)    (paper.bibtex      armando93 article)    (paper.author      armando93 "A. Armando and E. Giunchiglia")    (paper.title       armando93 "Embedding complex decision procedures inside an interative theorem prover")    (paper.publication armando93 "Annals of Mathematics and Artificial Intelligence")    (paper.volume      armando93 "8(3-4)")    (paper.startpage   armando93 475)    (paper.endpage     armando93 502)    (paper.link        armando93 "http://citeseer.ist.psu.edu/context/552111/0")    (paper.year        armando93 1993)    (paper.description armando93 "
Building a Non-CNF SAT solver out of a CNF SAT-solver.  
")    (paper.topic       armando93 "SAT Solving")    (paper.instance    armando97)    (paper.bibtex      armando97 techreport)    (paper.author      armando97 "A. Armando and S. Ranise")    (paper.title       armando97 "From Integrated Reasoning Specialists to Plug-and-Play Reasoning Components")    (paper.publication armando97 "Technical Report
           ")    (paper.link        armando97 "http://citeseer.ist.psu.edu/armando98from.html")    (paper.year        armando97 1997)    (paper.rank        armando97 "[***]")    (paper.description armando97 "
Armando and Ranise investigate breaking apart a reasoner into its
constituent pieces.  This investigation aims at learning how to
describe reasoners in an effort to promote the heterogeneous integration
of reasoners.  Their case study is NQTHM, Boyer and Moore's linear
arithmetic decision procedure.
")    (paper.topic       armando97 "Architectures")    (paper.instance    astrachan92)    (paper.bibtex      astrachan92 inproceedings)    (paper.author      astrachan92 "Owen Astrachan and Mark Stickel")    (paper.title       astrachan92 "Caching and Lemmaizing in Model Elimination Theorem Provers")    (paper.publication astrachan92 "Proceedings of CADE")    (paper.link        astrachan92 "http://citeseer.ist.psu.edu/astrachan92caching.html")    (paper.year        astrachan92 1992)    (paper.rank        astrachan92 "[***]")    (paper.description astrachan92 "
Caching essentially saves intermediate results of a proof in a lookup table and
checks that table to avoid repeating the search for a result already computed.
It keeps track of the depth at which solutions are found to ensure
the same solutions are found given the appropriate depth cutoff using
iterative deepening.  Lemmaizing adds a derived result to the set of input 
clauses, which hopefully will decrease the depth of the proof by more 
than it increases the branching factor. This seminal paper deals with the 
Horn case.
")    (paper.topic       astrachan92 "Theorem Proving with Attachments")    (paper.instance    audemard2000)    (paper.bibtex      audemard2000 inproceedings)    (paper.author      audemard2000 "Gilles Audemard and Belaid Benhamou and Laurent Henocque")    (paper.title       audemard2000 "Two techniques to improve Finite Model Search")    (paper.publication audemard2000 "Proceedings of the 17th International Conference on Automated Deduction")    (paper.link        audemard2000 "http://citeseer.ist.psu.edu/386981.html")    (paper.year        audemard2000 2000)    (paper.description audemard2000 "
Two techniques are introduced to speed up SEM-style model finding.
")    (paper.topic       audemard2000 "Model Building")    (paper.instance    bachmair97)    (paper.author      bachmair97 "Leo Bachmair and Harald Ganzinger and Andrei Voronkov")    (paper.title       bachmair97 "Elimination of Equality via Transformation with Ordering Constraints")    (paper.publication bachmair97 "CADE")    (paper.link        bachmair97 "http://portal.acm.org/citation.cfm?id=648234.753446")    (paper.year        bachmair97 1997)    (paper.description bachmair97 "
This paper  builds on Brand's method for eliminating the substitution,
reflexivity, symmetry, and transitivity axioms of equality by rewriting
the rules.  It adds ordering constraints in the process of transformation,
which seems to be one of the big benefits.
")    (paper.topic       bachmair97 "Transformations")    (paper.instance    baralis97)    (paper.bibtex      baralis97 inproceedings)    (paper.author      baralis97 "Baralis and Paraboschi and Teniente")    (paper.title       baralis97 "Materialized view selection in a multidimensional database")    (paper.publication baralis97 "In proceedings of Very Large Databases (VLDB)")    (paper.startpage   baralis97 155)    (paper.endpage     baralis97 165)    (paper.year        baralis97 1997)    (paper.description baralis97 "
Paper on view selection problem in databases
")    (paper.topic       baralis97 "Databases")    (paper.instance    barrett2000)    (paper.bibtex      barrett2000 inproceedings)    (paper.author      barrett2000 "Clark Barrett and David Dill and Aaron Stump")    (paper.title       barrett2000 "A Framework for Cooperating Decision Procedures")    (paper.publication barrett2000 "17th International Conference on Automated Deduction
           ")    (paper.link        barrett2000 "http://citeseer.ist.psu.edu/barrett00framework.html")    (paper.year        barrett2000 2000)    (paper.rank        barrett2000 "[**]")    (paper.description barrett2000 "
Barrett et. al. describe a framework approach for building reasoners
that combine decision procedures.  This work is a precurser to janicic2002.
They introduce half a dozen operations that can be used to interrelate
these decision procedures; they further show how Nelson Oppen and
Shostak procedures can be built in their framework.
")    (paper.topic       barrett2000 "Architectures")    (paper.related     barrett2000 janicic2002)    (paper.instance    baumgartner2003)    (paper.bibtex      baumgartner2003 article)    (paper.author      baumgartner2003 "Peter Baumgartner and Cesare Tinelli")    (paper.title       baumgartner2003 "The Model Evolution Calculus")    (paper.publication baumgartner2003 "19th International Conference on Automated
    Deduction")    (paper.startpage   baumgartner2003 350)    (paper.endpage     baumgartner2003 364)    (paper.link        baumgartner2003 "http://sherry.ifi.unizh.ch/577584.html")    (paper.year        baumgartner2003 2003)    (paper.rank        baumgartner2003 "[****]")    (paper.description baumgartner2003 "
Baumgartner and Tinelli describe a first-order version of the DPLL 
(Davis-Putnam-Logemann-Loveland) SAT solver.  In the case of
propositional logic, ME reduces to DPLL.  It differs in the first-order
case because of additional inference rules and skolemization.  The procedure
works by exaustively trying to build a first-order model.  If the search 
fails, the sentences are unsatisfiable.  But if the sentences are satisfiable,
the procedure may run forever trying to construct one.  (Obviously if there
are only infinite models this technique may have issues.) 
")    (paper.topic       baumgartner2003 "The Model Evolution Calculus")    (paper.instance    baumgartner2007)    (paper.bibtex      baumgartner2007 article)    (paper.author      baumgartner2007 "Peter Baumgartner and Alexander Fuchs and Hans Nivelle and Cesare Tinelli")    (paper.title       baumgartner2007 "Computing Finite Models by Reduction to Function-Free Clause Logic")    (paper.publication baumgartner2007 "Journal of Applied Logic")    (paper.link        baumgartner2007 "http://goedel.cs.uiowa.edu/Darwin/")    (paper.year        baumgartner2007 2007)    (paper.description baumgartner2007 "
Paper on FMDarwin for building finite models of clause sets.  Unlike
techniques for finding a model directly and techniques that ground out
sentences and invoke a SAT solver, FMDarwin reduces a set of clauses
to function-free clause logic in a way that preserves satisfiability
for a particular size domain.  Then it uses Darwin, an implementation
of the model evolution calculus, to check the satisfiability of
the function-free clauses, iterating through domain sizes.  
")    (paper.topic       baumgartner2007 "Model Building")    (paper.instance    baumgartner94)    (paper.author      baumgartner94 "Peter Baumgartner and Ulrich Furbach")    (paper.title       baumgartner94 "Model Elimination Without Contrapositives")    (paper.publication baumgartner94 "Proceedings of Conference on Automated Deduction")    (paper.volume      baumgartner94 "814")    (paper.startpage   baumgartner94 87)    (paper.endpage     baumgartner94 101)    (paper.link        baumgartner94 "http://portal.acm.org/citation.cfm?id=752966")    (paper.year        baumgartner94 1994)    (paper.description baumgartner94 "
Baumgartner and Furbach introduce Restart Model Elimination and a couple of refinements.
RME obviates the need for contrapositives; all the rules are written in the form
A1 | ... | An <= B1 ^ ... ^ Bm, i.e. the only extension steps that can be performed
are on one of the Ai.  The price we pay is that whenever the literal on the top of the 
stack is positive, we put the goal literal onto the top of the stack (forcibly in
strict RME), hence the name Restart ME.  Also, this means that ancestry pruning cannot
be used with RME if completeness is to be preserved.  Soundness and refutational 
completeness results.  Some experimental results and fairly lengthy comparison
to other calculi.
")    (paper.topic       baumgartner94 "Calculi")    (paper.instance    baumgartner95)    (paper.bibtex      baumgartner95 article)    (paper.author      baumgartner95 "Peter Baumgartner and Ulrich Furbach and Frieder Stolzenburg")    (paper.title       baumgartner95 "Model Elimination, Logic Programming and Computing Answers")    (paper.publication baumgartner95 "Technical Report")    (paper.link        baumgartner95 "http://citeseer.ist.psu.edu/144755.html")    (paper.year        baumgartner95 1995)    (paper.description baumgartner95 "
The authors demonstrate that restart model elimination can be used as a complete
interpreter for positive disjunctive logic programming.  That is, they show how
to adjust RME so that it is complete for (definite) answer extraction.  With proofs.
Also, they introduce a limited form of ancestry pruning.  More experiments.
")    (paper.topic       baumgartner95 "Calculi")    (paper.instance    bell95)    (paper.bibtex      bell95 article)    (paper.author      bell95 "John Bell")    (paper.title       bell95 "Pragmatic Reasoning: A model-based theory")    (paper.publisher   bell95 "Kluwer Academic Publishers")    (paper.publication bell95 "Applied Logic: How, What and Why")    (paper.link        bell95 "http://citeseer.ist.psu.edu/bell92pragmatic.html")    (paper.year        bell95 1995)    (paper.rank        bell95 "[****]")    (paper.description bell95 "
Bell defines pragmatic reasoning as context-dependent reasoning, in
contrast to deduction which is context-free.  He mathematizes
the two model-theoretically and discusses different versions
of pragmatic entailment.  He compares it to mental models, and
looks at different techniques that have been developed for
expressing pragmatic reasoning tasks.
")    (paper.topic       bell95 "Logical Knowledge Representation")    (paper.instance    berezin2002)    (paper.bibtex      berezin2002 phdthesis)    (paper.author      berezin2002 "Sergey Berezin")    (paper.title       berezin2002 "Model Checking and Theorem Proving: A Unified Framework")    (paper.publication berezin2002 "CMU Ph.D. Thesis")    (paper.link        berezin2002 "http://chicory.stanford.edu/~berezin/thesis/")    (paper.year        berezin2002 2002)    (paper.rank        berezin2002 "[**]")    (paper.description berezin2002 "
This thesis builds a framework for integrating model checking and 
theorem proving, focusing on formal verification applications.  
The basic framework proves first-order branching time mu-calculus
properties of Kripke models, but the inference rules can be easily
changed to facilitate the fast development of new proof systems
(called model provers).  Related work is not so thorough. 
")    (paper.topic       berezin2002 "Model Checking")    (paper.instance    blackburn)    (paper.bibtex      blackburn book)    (paper.author      blackburn "Patrick Blackburn and Johan van Benthem and Frank Wolter")    (paper.title       blackburn "Handbook of Modal Logic")    (paper.publisher   blackburn "Elsevier Science")    (paper.link        blackburn "http://www.elsevier.com/wps/find/bookdescription.cws_home/708884/description#description")    (paper.year        blackburn 2006)    (paper.rank        blackburn "[*****]")    (paper.description blackburn "
A handbook for modal logic, covering basic and advanced theory, variations and extensions, and applications.
")    (paper.topic       blackburn "")    (paper.instance    bodirsky2004)    (paper.bibtex      bodirsky2004 phdthesis)    (paper.author      bodirsky2004 "Manuel Bodirsky")    (paper.title       bodirsky2004 "Constraint Satisfaction with Infinite Domains")    (paper.link        bodirsky2004 "http://www.informatik.hu-berlin.de/~bodirsky/publications/diss.html")    (paper.year        bodirsky2004 2004)    (paper.description bodirsky2004 "
Bodirsky examines the problem of the homomorphic CSP: given one structure,
is it homomorphic to the target structure?  He for the first time
considers target structures that are countably large; more precisely,
those that are countably large but are omega categorical, i.e. 
all countable models of the first-order theory of that model are isomorphic.
In this work, he produces an algorithm for constructing a tree that
satisfies a given set of constraints, where those constraints require
the existence of nodes that meet certain conditions.  Three possible conditions:
x is an ancestor of y, x and y are equal, and x occurs in a subtree to
the left of the subtree that includes y.  While these constraints force
nodes to exist that satisfy particular relationships, OOCSPs force 
particular nodes to have a particular relationship.
")    (paper.topic       bodirsky2004 "Miscellaneous")    (paper.instance    bonacina97)    (paper.bibtex      bonacina97 inproceedings)    (paper.author      bonacina97 "Maria Bonacina")    (paper.title       bonacina97 "Machine-independent evaluation of theorem-proving strategies")    (paper.publication bonacina97 "Workshop on Theorem Proving Strategies")    (paper.startpage   bonacina97 37)    (paper.endpage     bonacina97 39)    (paper.link        bonacina97 "http://citeseer.ist.psu.edu/bonacina97machineindependent.html")    (paper.year        bonacina97 1997)    (paper.rank        bonacina97 "[***]")    (paper.description bonacina97 "
Bonacina's position piece explains the need for 'strategy analysis' of
theorem proving techniques.  'Strategy analysis' focuses on theorem-proving
complexity as in plaisted94, but also incorporates indexing, data-structures,
unification complexity, etc.   Bonacina gives a summary of another paper 
that confronts the problem representing deletion strategies in a search
space.  Cites kowalski69 as handling the case where results are never
deleted.  She also explains how a sizable body of related work 
concentrates on the complexity of a logical formalism without reference
to any automated proof-finding system, e.g. goubault94.  
")    (paper.topic       bonacina97 "Comparative Analysis")    (paper.related     bonacina97 kowalski69)    (paper.related     bonacina97 goubault94)    (paper.instance    bonacina98)    (paper.bibtex      bonacina98 misc)    (paper.author      bonacina98 "Maria Bonacina")    (paper.title       bonacina98 "Strategy analysis: from Sequential to parallel strategies")    (paper.startpage   bonacina98 21)    (paper.endpage     bonacina98 23)    (paper.link        bonacina98 "http://citeseer.ist.psu.edu/36838.html")    (paper.year        bonacina98 1998)    (paper.rank        bonacina98 "[*]")    (paper.description bonacina98 "
Bonacina's position paper describes the formalization of the
search space for parallelized theorem proving with contraction
operations.  This paper covers the main ideas, but the meat looks
to be cited in two technical reports.
")    (paper.topic       bonacina98 "Comparative Analysis")    (paper.instance    borger)    (paper.bibtex      borger book)    (paper.author      borger "Egon Borger and Erich Gradel and Yuri Gurevich")    (paper.title       borger "The Classical Decision Problem")    (paper.publisher   borger "Springer-Verlag")    (paper.link        borger "http://books.google.com/books?id=3po2Tv_UVcMC&dq=&pg=PP1&ots=FMOqyLGt4N&sig=7pB4XtFUSbGbiUuF4XlKGattRts&prev=http://www.google.com/search%3Fclient%3Dsafari%26rls%3Den%26q%3Dthe%2Bclassical%2Bdecision%2Bproblem%26ie%3DUTF-8%26oe%3DUTF-8&sa=X&oi=print&ct=title#PPP1,M1")    (paper.year        borger 1997)    (paper.description borger "
The book detailing decidability results for entailment in various prefix
classes of first-order logic, along with complexity results for the 
decidable classes.
")    (paper.topic       borger "")    (paper.instance    borning92)    (paper.bibtex      borning92 article)    (paper.author      borning92 "A. Borning and B.N. Freeman-Benson and M. Wilson")    (paper.title       borning92 "Constraint hierarchies")    (paper.publication borning92 "Lisp and Symbolic Computation")    (paper.volume      borning92 "5")    (paper.startpage   borning92 223)    (paper.endpage     borning92 270)    (paper.link        borning92 "http://citeseer.ist.psu.edu/borning92constraint.html")    (paper.year        borning92 1992)    (paper.rank        borning92 "[*]")    (paper.description borning92 "
Constraint hierarchies, aka Hierarchical Constraint Satisfaction Problems,
impose a precedence hierarchy on constraints.  Only the highest 
precedence constraints must be satisfied; the others constraints
form an ordered set of preferences on the solutions.
")    (paper.topic       borning92 "Object Oriented Constraint Satisfaction")    (paper.instance    borrett2001)    (paper.author      borrett2001 "James Borrett and Edward Tsang")    (paper.title       borrett2001 "A Context for Constraint Satisfaction Problem Formulation Selection")    (paper.publication borrett2001 "Constraints")    (paper.volume      borrett2001 "6")    (paper.startpage   borrett2001 299)    (paper.endpage     borrett2001 327)    (paper.link        borrett2001 "http://citeseer.ist.psu.edu/borret99context.html")    (paper.year        borrett2001 2001)    (paper.rank        borrett2001 "[***]")    (paper.description borrett2001 "
Borrett and Tsang construct a framework for searching through various constraint satisfaction problem formulations and selecting one.  They only consider
formulations of the form <V,D,C>, i.e. typical CSP formulations.  They break the problem into 4 parts: generating an initial formulation, constructing operators to change that formulation, constructing heuristics for choosing which operator to apply, and constructing an evaluation function to that determines the cost of a given formulation.  They go on to demonstrate a particular type of operator: the introduction of redundant constraints and show how searching through CSP formulations with this type of operator works in their framework.
")    (paper.topic       borrett2001 "Reformulation")    (paper.instance    bowen91)    (paper.bibtex      bowen91 article)    (paper.author      bowen91 "J. Bowen and D. Bahler")    (paper.title       bowen91 "Conditional Existence of Variables in Generalized Constraint Networks
")    (paper.publication bowen91 "AAAI ")    (paper.link        bowen91 "http://www.csc.ncsu.edu/faculty/bahler/aaai91/aaai91.pdf")    (paper.year        bowen91 1991)    (paper.description bowen91 "
Bowen and Bahler employ Free Logic to construct a constraint network
where not all variables must be assigned values.  A constraint network
is defined as a set of logical sentences with a DCA; 
a solution is an interpretation
of the vocabulary in a model so that the sentences are satisfied.  
Free logic differs from FOL in that not every element in the vocabulary
needs to be mapped into the model.
")    (paper.topic       bowen91 "Miscellaneous")    (paper.instance    bry2007)    (paper.bibtex      bry2007 article)    (paper.author      bry2007 "Francois Bry and Norbert Eisinger and Thomas Eiter and Tim Furche and Georg Gottlob and Clemens Ley and Benedikt Linse and Reinhard Pichler and Fang Wei")    (paper.title       bry2007 "Foundations of Rule-Based Query Answering")    (paper.publication bry2007 "Reasoning Web, Third International Summer School 2007, Grigoris Antoniou, Uwe Assmann, Cristina Baroglio, Stefan Decker, Nicola Henze, Paula-Lavinia Patranjan, Robert Tolksdorf (editors)")    (paper.volume      bry2007 "LNCS 4636")    (paper.link        bry2007 "http://www.pms.ifi.lmu.de/publikationen/index.html#PMS-FB-2007-7")    (paper.year        bry2007 2007)    (paper.description bry2007 "
The mathematical foundations of logical query languages.
")    (paper.topic       bry2007 "Logical Knowledge Representation")    (paper.instance    bundy73)    (paper.bibtex      bundy73 inproceedings)    (paper.author      bundy73 "Alan Bundy")    (paper.title       bundy73 "Doing Arithmetic With Diagrams")    (paper.publication bundy73 "IJCAI")    (paper.year        bundy73 1973)    (paper.rank        bundy73 "[**]")    (paper.description bundy73 "
Bundy describes SUMS (System which Understands Mathematical
Symbols), a theorem prover for theorems over natural numbers.
The SUMS proof process represents the natural numbers in
a diagram, and by manipulating the diagram, determines the
truth of a conjecture.  Bundy makes an interesting point at the 
end of the paper: mathematicians do not usually manipulate
logic to prove theorems.  Rather, they convince themselves
of the correctness of a conjecture through diagrams or other
semantic insights and then use logic as a language for
communicating to others the proof of the theorem.  SUMS
is an attempt at building a machine that simply 
convinces itself of the truth of a conjecture.
")    (paper.topic       bundy73 "Graphical Reasoning Techniques")    (paper.instance    bundy99)    (paper.author      bundy99 "Alan Bundy")    (paper.title       bundy99 "A Survey of Automated Deduction")    (paper.link        bundy99 "http://citeseer.ist.psu.edu/bundy99survey.html")    (paper.year        bundy99 1999)    (paper.rank        bundy99 "")    (paper.description bundy99 "
Bundy surveys research in automated deduction.  He focuses on resolution,
term rewriting, built-in unification, higher-order logics and type-theory,
induction, interactive theorem-proving, meta-reasoning, common sense
reasoning, and logic programming.
")    (paper.topic       bundy99 "Theorem Proving")    (paper.instance    bylander94)    (paper.author      bylander94 "Tom Bylander")    (paper.title       bylander94 "The Computational Complexity of Propositional STRIPS Planning")    (paper.publication bylander94 "Artificial Intelligence")    (paper.volume      bylander94 "69(1-2)")    (paper.startpage   bylander94 165)    (paper.endpage     bylander94 204)    (paper.link        bylander94 "http://citeseer.ist.psu.edu/bylander94computational.html")    (paper.year        bylander94 1994)    (paper.description bylander94 "
Bylander discusses complexity results for propositional STRIPS; he also mentions that a class of first-order STRIPS can be reduced in polynomial time to propositional STRIPS, making these results widely applicable.  PLANSAT (decision problem for determining whether an instance of prop STRIPS planning is satisfiable) is PSPACE-complete.  PLANSAT[2+,2] (PLANSAT where all operators have at least 2 positive preconditions and 2 postconditions) is also PSPACE-complete.  Results are shown for a variety of restrictions, concluding that without severe restrictions, PLANSAT is NP-complete or PSPACE-complete.
These results carry over to PLANMIN (the decision problem of determining whether an instance has a solution of k or fewer operators.  The new results here show a variety of PLANMIN are NP-complete, and a couple, very restricted versions are polynomial.  He also shows results for Propositional Strips with formulas (Extended Propositional STRIPS).  EPLANSAT, EPLANSAT[0,1+] restricted to define Horn clauses are PSPACE-complete.  More results are shown for EPLANSAT and EPLANSATMIN.  See paper for details.
")    (paper.topic       bylander94 "Historical")    (paper.instance    byrne2000)    (paper.bibtex      byrne2000 misc)    (paper.author      byrne2000 "Ruth Byrne and Lisa Gilroy")    (paper.title       byrne2000 "Mental Models Website")    (paper.link        byrne2000 "http://www.tcd.ie/Psychology/Ruth_Byrne/mental_models/index.html")    (paper.year        byrne2000 2000)    (paper.description byrne2000 "
Mental Models website--the Psychology version of model-based reasoning.
")    (paper.topic       byrne2000 "Model-Based Reasoning")    (paper.instance    cadoli2000)    (paper.bibtex      cadoli2000 misc)    (paper.author      cadoli2000 "Marco Cadoli and Francesco M. Donini and Paolo Liberatore and Marco Schaerf")    (paper.title       cadoli2000 "Preprocessing of Intractable Problems")    (paper.publication cadoli2000 "Information and Computation")    (paper.volume      cadoli2000 "176(2)")    (paper.startpage   cadoli2000 89)    (paper.endpage     cadoli2000 120)    (paper.link        cadoli2000 "http://citeseer.ist.psu.edu/cadoli00preprocessing.html")    (paper.year        cadoli2000 2000)    (paper.rank        cadoli2000 "[**]")    (paper.description cadoli2000 "
This paper describes complexity classes for algorithms after they
have been preprocessed, or compiled.  The problems targeted have two
parts, one that is fixed, and one that changes frequently.  For
example, determining whether Delta entails a sentence phi has a
fixed part (Delta) and a changing part (phi).  If we could compile
Delta into a more vivid representation, computing entailment might
be easier.  This paper formalizes that idea, gives a complexity class
hierarchy, and a method for reductions.  Cool ideas, but very
theoretical (and only with propositional KB examples).
")    (paper.topic       cadoli2000 "Knowledge Base Compilation")    (paper.instance    cadoli2000b)    (paper.bibtex      cadoli2000b misc)    (paper.author      cadoli2000b "Marco Cadoli and Francesco Donini and Paolo Liberatore and Marco Schaerf")    (paper.title       cadoli2000b "Space Efficiency of Propositional Knowledge Representation Formalisms")    (paper.publication cadoli2000b "Journal of Artificial Intelligence Research")    (paper.volume      cadoli2000b "13")    (paper.startpage   cadoli2000b 1)    (paper.endpage     cadoli2000b 31)    (paper.link        cadoli2000b "http://citeseer.ist.psu.edu/cadoli00space.html")    (paper.year        cadoli2000b 2000)    (paper.rank        cadoli2000b "[*****]")    (paper.description cadoli2000b "
The authors leverage previous work on compilation complexity classes to compare
various propositional knowledge formalisms.  They introduce two axes for comparing
such formalisms under translation: theorem-preservation and model-preservation.
That is, given a set of sentences in one formalism, translate those sentences
into another formalism so that (1) the theorems are preserved across the translation
after a translation of their own or (2) the models are preserved across the
translation after a translation of their own.  The translation of premise sets
is assumed to be polynomial in size; the requirement on the translation of
theorems/models is that the representation of the translation process itself
must be polynomial in size.  

The authors then introduce the notion of 
model-C and thm-C classes.  A formalism belongs to the model-C class iff 
model checking in that formalism is in the compilability class C, where the KB is 
the fixed part and the model is the varying part.  A formalism belongs to
the thm-C class iff inference in that formalism is in the compilability class C,
where the KB is the fixed part and the query is the varying part.

Next the authors prove that two formalisms in the same model-C/thm-C class then
there must be a poly-size reduction between the two formalisms that 
preserves models/theorems.  They also show that if two problems are in different
classes there is no polynomial size reduction satisfying models/theorems.

Finally, the propositional formalisms stable model semantics, circumscription, 
generalized closed world, skeptical default logic, credulous default logic, 
WIDTIO belief revision, and skeptical belief revision are analyzed in this 
framework, giving fairly complete coverage of inference/model space complexity.
")    (paper.topic       cadoli2000b "Knowledge Base Compilation")    (paper.instance    cadoli2001)    (paper.author      cadoli2001 "Marco Cadoli and Andrea Schaerf")    (paper.title       cadoli2001 "Compiling Problem Specifications into SAT")    (paper.publication cadoli2001 "Lecture Notes in Computer Science")    (paper.volume      cadoli2001 "2028")    (paper.link        cadoli2001 "http://citeseer.ist.psu.edu/cadoli01compiling.html")    (paper.rank        cadoli2001 "[***]")    (paper.description cadoli2001 "
Cadoli and Schaerf describe a language, NP-Spec, for specifying problems
in NP, and a technique for translating that language into SAT.  They
mention a previous translation into Prolog.  With SAT, results were
far better than with Prolog; that is, they could solve what they called
medium-sized problems instead of just small problems.  NP-Spec is based
on Prolog without negation (except on some predefined predicates) with
a fixed database. It includes a mix of metalevel constraints and object-level
constraints.
")    (paper.topic       cadoli2001 "Knowledge Base Compilation")    (paper.instance    cadoli2002)    (paper.author      cadoli2002 "Marco Cadoli and Toni Mancini")    (paper.title       cadoli2002 "Knowledge compilation = Query rewriting + View synthesis")    (paper.publication cadoli2002 "Symposium on Principles of Database Systems")    (paper.startpage   cadoli2002 199)    (paper.endpage     cadoli2002 208)    (paper.link        cadoli2002 "http://citeseer.ist.psu.edu/641562.html")    (paper.rank        cadoli2002 "[***]")    (paper.description cadoli2002 "
Cadoli describes a class of propositional knowledge compilation  where
the problem is transformed into a database and a second-order
query over that database.  The database is a straight-forward representation
of the clausal form of the input.  The authors give syntactic conditions
under which the problem can be compiled into P using this technique.
")    (paper.topic       cadoli2002 "Knowledge Base Compilation")    (paper.instance    cadoli97)    (paper.bibtex      cadoli97 article)    (paper.author      cadoli97 "Marco Cadoli and Francesco M. Donini")    (paper.title       cadoli97 "A Survey on Knowledge Compilation")    (paper.publication cadoli97 "AI Communications")    (paper.volume      cadoli97 "10(3-4)")    (paper.startpage   cadoli97 137)    (paper.endpage     cadoli97 150)    (paper.link        cadoli97 "http://citeseer.ist.psu.edu/cadoli98survey.html")    (paper.year        cadoli97 1997)    (paper.rank        cadoli97 "[***]")    (paper.description cadoli97 "
Cadoli and Donini review techniques for propositional knowledge base compilation.  
The goal
is to rewrite a KB in a form that will answer queries more quickly.  They survey
exact compilation: prime implicates and implicants, unit-resolution-complete 
methods, and theory prime implicates.  They look at approximate compilation:
anytime versions of exact methods, horn approximations (can result in unsound
inference).  They consider compiling non-monotonic knowledge bases with
circumscription, default logic, and belief revision.  All those techniques aim
to reduce intractable query answering to tractable query answering.  Lastly,
Cadoli and Donini mention compilations of _polynomial-time_ query answering,
compilations into something other than a KB, and compare compilation to 
fixed-parameter tractability.
")    (paper.topic       cadoli97 "Knowledge Base Compilation")    (paper.instance    caseau94)    (paper.author      caseau94 "Yves Caseau")    (paper.title       caseau94 "Constraint Satisfaction with an Object-Oriented Knowledge
Representation Language")    (paper.publication caseau94 "Applied Intelligence ")    (paper.volume      caseau94 "4(2)")    (paper.startpage   caseau94 157)    (paper.endpage     caseau94 184)    (paper.link        caseau94 "http://citeseer.ist.psu.edu/caseau94constraint.html")    (paper.year        caseau94 1994)    (paper.rank        caseau94 "[***]")    (paper.description caseau94 "
Caseau describes how one might solve constraint satisfaction problems
using Laure, an object-oriented language that combines declarative
and procedural knowledge.  In Laure, a Database is used to model
the world; solving a problem amounts to finding objects to fill
goals and satisfy constraints.  Those objects are completions of
the database.  CSPs can be represented by attaching constraints
to objects; a finite number of these objects allows standard 
CSP techniques, e.g. arc consistency, to find solutions.
")    (paper.topic       caseau94 "OO Languages with Native Constraint Satisfaction")    (paper.instance    caseau96)    (paper.bibtex      caseau96 article)    (paper.author      caseau96 "Yves Caseau, Francois Laburthe")    (paper.title       caseau96 "CLAIRE: a brief overview")    (paper.publication caseau96 "")    (paper.link        caseau96 "http://citeseer.ist.psu.edu/caseau96claire.html")    (paper.year        caseau96 1996)    (paper.rank        caseau96 "[**]")    (paper.description caseau96 "
CLAIRE is a programming language designed to be C++ compliant that  
can compute using constraints natively.
")    (paper.topic       caseau96 "OO Languages with Native Constraint Satisfaction")    (paper.instance    changlee)    (paper.bibtex      changlee book)    (paper.author      changlee "Chin-Liang Chang and Richard Lee")    (paper.title       changlee "Symbolic Logic and Mechanical Theorem Proving")    (paper.publisher   changlee "Academic Press")    (paper.link        changlee "http://www.amazon.com/exec/obidos/ASIN/0121703509/qid%3D965533387/sr%3D1-1/102-6107129-2481765")    (paper.year        changlee 1973)    (paper.rank        changlee "")    (paper.description changlee "
A classic text for automated reasoning.  Chang and Lee cover propositional and
first-order logic, Herbrand's theorem, resolution (standard, semantic, lock,
and linear) and equality.  They discuss an alternative to resolution, 
Prawitz's procedure and V-resolution.  A chapter on program analysis
leads to query answering and program synthesis in the final chapter.
")    (paper.topic       changlee "")    (paper.instance    chen2000)    (paper.author      chen2000 "Hubie Chen")    (paper.title       chen2000 "A Theory of Average-Case Compilability in Knowledge Representation")    (paper.link        chen2000 "http://citeseer.ist.psu.edu/590465.html")    (paper.rank        chen2000 "[***]")    (paper.description chen2000 "Chen gives an overview of the Compilability hierarchy given
by Cadoli2000.  Then Chen gives theorems concerning average-case
compilability, i.e. analogs of the Cadoli hierarchy when there is a 
distribution over the space of knowledge representations within a class.
The results are pretty densely presented.  
")    (paper.topic       chen2000 "Knowledge Base Compilation")    (paper.instance    chirkova2002)    (paper.bibtex      chirkova2002 phdthesis)    (paper.author      chirkova2002 "Rada Chirkova")    (paper.title       chirkova2002 "Automated Database Restructuring")    (paper.publisher   chirkova2002 "Stanford University")    (paper.link        chirkova2002 "")    (paper.year        chirkova2002 2002)    (paper.rank        chirkova2002 "[****]")    (paper.description chirkova2002 "
Chirkova's thesis presents algorithms for computing the set of views
that if materialized would allow queries to be answered optimally.  That is
there is no set of views that if materialized would answer those queries
any more efficiently.  Three results: the problem is decidable, it
has a triply exponential upper bound in the queries, and it has an
exponential lower bound.  Only conjunctions, i.e. SPJ,
are considered, both for view definitions and the queries.
")    (paper.topic       chirkova2002 "Databases")    (paper.instance    choi2000)    (paper.bibtex      choi2000 misc)    (paper.author      choi2000 "Seungyeob Choi")    (paper.title       choi2000 "Semantically Guided Proof Planning")    (paper.link        choi2000 "http://citeseer.ist.psu.edu/464153.html")    (paper.year        choi2000 2000)    (paper.rank        choi2000 "[***]")    (paper.description choi2000 "
This thesis proposal focuses on adding semantic guidance to
automated theorem proving in two ways: 1) by incorporating
the use of models with a clause graph procedure and 2) by
incorporating the use of models with a proof planning
procedure.  This gives a good overview of various theorem
proving techniques and semantic techniques.
")    (paper.topic       choi2000 "Model-Guided Proof Techniques")    (paper.instance    chomicki95)    (paper.author      chomicki95 "Jan Chomicki")    (paper.title       chomicki95 "Efficient Checking of Temporal Integrity Constraints Using Bounded History Encoding")    (paper.publication chomicki95 "ACM Transactions on Database Systems")    (paper.volume      chomicki95 "20(2)")    (paper.startpage   chomicki95 149)    (paper.endpage     chomicki95 186)    (paper.link        chomicki95 "http://citeseer.ist.psu.edu/chomicki95efficient.html")    (paper.year        chomicki95 1995)    (paper.rank        chomicki95 "[**]")    (paper.description chomicki95 "
Chomicki describes an algorithm for maintaining temporal constraints on a database, which are written in past first order temporal logic.  Instead of storing the entire history of the database, Chomicki describes a way to roll up the history (in a lossy way) into auxiliary relations in the database.  She assumes the set of constraints is fixed, which means a single auxiliary relation can be stored for each sentence in the constraints.  She proves this encoding to be polynomially-bounded in the number of domain values that appear in the database over time.  Chomicki extends this work to include real-time constraints, which refer to a clock.
")    (paper.topic       chomicki95 "Hypothetical Temporal DB Queries")    (paper.instance    christian93)    (paper.bibtex      christian93 article)    (paper.author      christian93 "Jim Christian")    (paper.title       christian93 "Flatterms, Discrimination Nets, and Fast Term Rewriting")    (paper.publication christian93 "Journal of Automated Reasoning
")    (paper.volume      christian93 "10")    (paper.startpage   christian93 95)    (paper.endpage     christian93 113)    (paper.link        christian93 "")    (paper.year        christian93 1993)    (paper.rank        christian93 "")    (paper.description christian93 "
One of the early papers on Discrimination Nets, which is argued as
a reason for representing terms as flat terms.
")    (paper.topic       christian93 "Indexing")    (paper.instance    claessen2003)    (paper.bibtex      claessen2003 inproceedings)    (paper.author      claessen2003 "K. Claessen and N. Sorensson")    (paper.title       claessen2003 "New Techniues that Improve MACE-style Finite Model Finding")    (paper.publication claessen2003 "CADE-19 Workshop on Model Computation")    (paper.link        claessen2003 "http://citeseer.ist.psu.edu/claessen03new.html")    (paper.year        claessen2003 2003)    (paper.description claessen2003 "
A serieis of techniques are introduced for constructing a finite model
by conversion to SAT, aka the MACE-style model building.  A/the paradox paper.
")    (paper.topic       claessen2003 "Model Building")    (paper.instance    clarke)    (paper.bibtex      clarke book)    (paper.author      clarke "Edmund M. Clarke Jr. and Orna Grumberg and Doron Peled")    (paper.title       clarke "Model Checking")    (paper.publisher   clarke "The MIT Press")    (paper.link        clarke "http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=3730")    (paper.year        clarke 1999)    (paper.rank        clarke "[*****]")    (paper.description clarke "
An introduction to model checking as performed by the formal methods community.
")    (paper.topic       clarke "")    (paper.instance    cohen90)    (paper.author      cohen90 "P.R. Cohen and H.J. Levesque")    (paper.title       cohen90 "Intention is choice with commitment")    (paper.publication cohen90 "Artificial Intelligence")    (paper.volume      cohen90 "42(3)")    (paper.year        cohen90 1990)    (paper.rank        cohen90 "[*]")    (paper.description cohen90 "
Cohen and Levesque use modal operators without explicit mention of time to describe an agent's persistent goals.  These goals will be worked upon until 1) they are achieved or 2) they are deemed impossible.  No work was done here on planning to achieve these goals. 
")    (paper.topic       cohen90 "Behavioral Goals")    (paper.instance    comon99)    (paper.bibtex      comon99 article)    (paper.author      comon99 "Hubert Comon and Mehmet Dincbas and Jean-pierre Jouannaud and Claude Kirchner")    (paper.title       comon99 "A Methodological View of Constraint Solving")    (paper.publication comon99 "Constraints")    (paper.volume      comon99 "4")    (paper.startpage   comon99 337)    (paper.endpage     comon99 361)    (paper.link        comon99 "http://citeseer.ist.psu.edu/13836.html")    (paper.year        comon99 1999)    (paper.description comon99 "
The authors motivate why sets are described using constraints and go on to survey the various techniques that have been used for processing those constraints, i.e. (1) determining whether an element belongs to a set or (2) determining whether one set is included entirely in another set (entailment).  They discuss 3 types of techniques: syntactic, semantic, and hybrid.  Syntactic techniques translate a formula into another formula.  Semantic techniques translate formulas into automata or another such formalism.  Hybrid techniques use both.  For more info on automata, try http://www.grappa.univ-lille3.fr/tata/.
")    (paper.topic       comon99 "General")    (paper.instance    cook71)    (paper.bibtex      cook71 article)    (paper.author      cook71 "Stephen Cook")    (paper.title       cook71 "The Complexity of Theorem-Proving Procedures")    (paper.publication cook71 "Symposium on the Theory of Computing (STOC)")    (paper.startpage   cook71 151)    (paper.endpage     cook71 158)    (paper.link        cook71 "http://citeseer.ist.psu.edu/context/5446/0")    (paper.year        cook71 1971)    (paper.rank        cook71 "[*]")    (paper.description cook71 "
This is Cook's paper which introduces the theory of NP completeness
and polynomial reductions.  He gives complexity bounds on theorem-
proving procedures relative to the minimum number of copies of 
axioms required for propositional unsatisfiability.
")    (paper.topic       cook71 "Comparative Analysis")    (paper.instance    craig2004)    (paper.bibtex      craig2004 article)    (paper.author      craig2004 "S. Craig and J. Gallagher and M. Leuschel and K. Henriksen")    (paper.title       craig2004 "Fully Automatic Binding-Time Analysis for Prolog")    (paper.publication craig2004 "14th International Symposium, Logic Based Program Synthesis and Trnasformation (LOPSTR)")    (paper.startpage   craig2004 53)    (paper.endpage     craig2004 68)    (paper.link        craig2004 "http://wotan.liu.edu/docis/dbl/lopstr/index.html")    (paper.year        craig2004 2004)    (paper.description craig2004 "
The authors consider the problem of computing modes, i.e. binding
patterns, for prolog relations, and whether to unroll or inline
such relations.  The first computes a set of disjoint types 
from regular types and then propagates them.  The second 
uses binary clause semantics and convex hull abstraction
to determine which relations will not terminate.  Apparently,
very similar work was done in 2001 by Vanhoof and Bruynooghe
in 2001.
")    (paper.topic       craig2004 "Reformulation")    (paper.instance    cyrluk96shostaks)    (paper.bibtex      cyrluk96shostaks inproceedings)    (paper.author      cyrluk96shostaks "David Cyrluk, Patrick Lincoln, and Natarajan Shankar")    (paper.title       cyrluk96shostaks "On Shostak's Decision Procedure for Combinations of Theories")    (paper.publication cyrluk96shostaks "13th International Conference on Automated Deduction
           ")    (paper.link        cyrluk96shostaks "http://citeseer.ist.psu.edu/cyrluk96shostaks.html")    (paper.year        cyrluk96shostaks 1996)    (paper.rank        cyrluk96shostaks "[****]")    (paper.description cyrluk96shostaks "
Cyrluk, Lincoln, and Shankar give a crisp description of the Shostak
procedure and sketch proofs of its properties.  It breaks the algorithm
down into 1) a congruence closure computation and 2) the interpretation
of functions through canonization.  This version of Shostak and all its
counterparts were later shown incomplete.
")    (paper.topic       cyrluk96shostaks "Nelson-Oppen and Shostak")    (paper.related     cyrluk96shostaks shostak84)    (paper.related     cyrluk96shostaks rueb2000deconstructing)    (paper.instance    dantsin2001)    (paper.author      dantsin2001 "Evgeny Dantsin and Thomas Eiter and Georg Gottlob and Andrei Voronkov")    (paper.title       dantsin2001 "Complexity and Expressive Power of Logic Programming")    (paper.publication dantsin2001 "ACM Computing Surveys")    (paper.volume      dantsin2001 "33(3)")    (paper.startpage   dantsin2001 374)    (paper.endpage     dantsin2001 425)    (paper.link        dantsin2001 "http://portal.acm.org/citation.cfm?id=502810&dl=ACM&coll=GUIDE")    (paper.year        dantsin2001 2001)    (paper.description dantsin2001 "
Complexity results for logic programming.
")    (paper.topic       dantsin2001 "Logic Programming")    (paper.instance    darwiche2002)    (paper.bibtex      darwiche2002 article)    (paper.author      darwiche2002 "Adnan Darwiche and Pierre Marquis")    (paper.title       darwiche2002 "A Knowledge Compilation Map")    (paper.publication darwiche2002 "Journal of Artificial Intelligence Research")    (paper.volume      darwiche2002 "17")    (paper.startpage   darwiche2002 229)    (paper.endpage     darwiche2002 264)    (paper.link        darwiche2002 "http://citeseer.ist.psu.edu/497263.html")    (paper.year        darwiche2002 2002)    (paper.description darwiche2002 "
The authors analyze knowledge compilation on two axes: the succinctness of
the target language and the class of queries and transformations that the
language supports in polytime.  They analyze a large number of existing
approaches.
")    (paper.topic       darwiche2002 "Knowledge Base Compilation")    (paper.instance    davis60)    (paper.bibtex      davis60 article)    (paper.author      davis60 "M. Davis and H. Putnam")    (paper.title       davis60 "A Machine Program for Theorem Proving")    (paper.publication davis60 "Journal of the ACM")    (paper.volume      davis60 "7")    (paper.startpage   davis60 201)    (paper.endpage     davis60 215)    (paper.year        davis60 1960)    (paper.description davis60 "
One of the DPLL papers.
")    (paper.topic       davis60 "Calculi")    (paper.instance    davis62)    (paper.bibtex      davis62 article)    (paper.author      davis62 "M. Davis and G. Logemann an D. Loveland")    (paper.title       davis62 "A Computing Procedure for Quantification Theory")    (paper.publication davis62 "Communications of the ACM")    (paper.volume      davis62 "5")    (paper.startpage   davis62 394)    (paper.endpage     davis62 397)    (paper.year        davis62 1962)    (paper.description davis62 "
One of the DPLL papers.
")    (paper.topic       davis62 "Calculi")    (paper.instance    davis94)    (paper.bibtex      davis94 techreport)    (paper.author      davis94 "Ernest Davis")    (paper.title       davis94 "Lucid Representations")    (paper.publisher   davis94 "New York University")    (paper.link        davis94 "http://citeseer.ist.psu.edu/davis94lucid.html")    (paper.year        davis94 1994)    (paper.rank        davis94 "[****]")    (paper.description davis94 "
Davis writes Lucid Representations in part to argue Levesque's
Making Believers Out of Computers paper.  Davis claims Levesque argues
that the only representations one should allow are complete ones,
in the database sense to ensure reasoning is fast.  Davis argues
that no AI system could get by with only complete information.
Incomplete information is imperative.  Davis attempts to propose
a technique for achieving the benefits of complete information
from starting from incomplete information by first instantiating
to complete the theory, then reasoning quickly, and finally 
ignoring those results which were artificially  introduced by
the process of instantiation.  This is a position piece trying to
refute Levesque's vision of the future and replacing
it with a vision of its own.
")    (paper.topic       davis94 "Logical Knowledge Representation")    (paper.instance    debray93)    (paper.bibtex      debray93 article)    (paper.author      debray93 "Saumya Debray and Nai-Wei Lin")    (paper.title       debray93 "Cost Analysis of Logic Programs")    (paper.publication debray93 "ACM Transactions on Programming Languages and Systems>")    (paper.volume      debray93 "15(5)")    (paper.startpage   debray93 826)    (paper.endpage     debray93 875)    (paper.year        debray93 1993)    (paper.description debray93 "
Debray and Lin outline techniques for computing upper bounds on the cost
of a logic program, taking in account multiple solutions and nondeterminism.
Based on relationship between argument size, moded relations, and typed 
relations.
")    (paper.topic       debray93 "Logic Programming")    (paper.instance    dechter92)    (paper.bibtex      dechter92 article)    (paper.author      dechter92 "Rina Dechter and Judea Pearl")    (paper.title       dechter92 "Structure Identificatioin in Relational Data")    (paper.publication dechter92 "Artificial Intelligence")    (paper.volume      dechter92 "58(1-3)")    (paper.startpage   dechter92 237)    (paper.endpage     dechter92 270)    (paper.link        dechter92 "http://citeseer.ist.psu.edu/dechter97structure.html")    (paper.year        dechter92 1992)    (paper.rank        dechter92 "[**]")    (paper.description dechter92 "
Dechter and Pearl describe building a propositional Horn representation
of a given set of models.  Identifying whether such a representation 
exists turns out to be polynomial in the size of the models.  Building
such a representation is polynomial in the size of the models and
the number of variables.  For k-Horn theories, a tightest approximation
can also be calculated in polynomial time.  
")    (paper.topic       dechter92 "Knowledge Base Compilation")    (paper.instance    delval96)    (paper.bibtex      delval96 inproceedings)    (paper.author      delval96 "Alvaro del Val")    (paper.title       delval96 "Approximate Knowledge Compilation: The First Order Case")    (paper.publication delval96 "AAAI")    (paper.startpage   delval96 498)    (paper.endpage     delval96 503)    (paper.link        delval96 "http://citeseer.ist.psu.edu/delval96approximate.html")    (paper.year        delval96 1996)    (paper.rank        delval96 "[****]")    (paper.description delval96 "
This seems to be the seminal work on Knowledge Compilation for FOL.  
del Val introduces a new algorithm for computing the Least Upper
Bound approximation for a propositional knowledge base, at times
exponentially better than the previously known algorithm.  He then
lifts both algorithms to first order logic under some restrictions.
")    (paper.topic       delval96 "Knowledge Base Compilation")    (paper.related     delval96 selman91)    (paper.instance    dershowitz82)    (paper.author      dershowitz82 "Nachum Dershowitz")    (paper.title       dershowitz82 "Orderings for Term-rewriting Systems")    (paper.publication dershowitz82 "Theoretical Computer Science")    (paper.volume      dershowitz82 "17")    (paper.startpage   dershowitz82 279)    (paper.endpage     dershowitz82 301)    (paper.link        dershowitz82 "http://citeseer.ist.psu.edu/context/20249/0")    (paper.year        dershowitz82 1982)    (paper.description dershowitz82 "
This paper confronts termination in term-rewriting systems.  
Dershowitz first defines the notion of a non-terminating rewrite system,
i.e. one with an infinite homeomorphic self-embedding derivation.  Then
he defines a simplification ordering as a partial ordering with
the replacement (monotonicity), subterm, and deletion properties.  If there
is a simplification ordering for a term rewriting system such that l > r for
every rewrite rule l -> r then the rewrite system terminates.  Dershowitz
goes on to define a quasi-simplification ordering, which is weaker than
a simplification ordering, making it easier to prove rewrite systems
terminate.  The reason this paper is located in this section is that
the notion of a recursive path ordering is then defined.  RPO and
variants thereof are used in modern theorem provers when ordering
is necessary.  The last section goes through a series of examples
that demonstrate how to prove rewrite systems terminate.
")    (paper.topic       dershowitz82 "Orderings")    (paper.instance    dershowitz93)    (paper.bibtex      dershowitz93 inbook)    (paper.author      dershowitz93 "Nachum Dershowitz")    (paper.title       dershowitz93 "A Taste of Rewrite Systems")    (paper.publication dershowitz93 "Functional Programming, Concurrency, Simulation, and
Automated Reasoning")    (paper.startpage   dershowitz93 199)    (paper.endpage     dershowitz93 228)    (paper.link        dershowitz93 "http://citeseer.ist.psu.edu/dershowitz93taste.html")    (paper.year        dershowitz93 1993)    (paper.rank        dershowitz93 "")    (paper.description dershowitz93 "
Dershowitz gives a whirlwind tour of rewrite systems, covering the
basic properties termination (existence of normal forms), 
confluence (the uniqueness of those forms), completion (how to construct 
confluent systems), checking entailment and solving equations via 
basically abduction.  He also discusses special techniques for dealing
with associative and commutative systems and conditional rewriting rules.
Applications include theorem proving and programming
")    (paper.topic       dershowitz93 "Theorem Proving")    (paper.instance    doyle91)    (paper.bibtex      doyle91 article)    (paper.author      doyle91 "Jon Doyle and Ramesh Patil")    (paper.title       doyle91 "Two Theses of Knowledge Representation: Language Restrictions, Taxonomic Classification, and the Utility of Representation Services")    (paper.publication doyle91 "Artificial Intelligence")    (paper.volume      doyle91 "48(3)")    (paper.startpage   doyle91 261)    (paper.endpage     doyle91 298)    (paper.link        doyle91 "http://citeseer.ist.psu.edu/doyle91two.html")    (paper.year        doyle91 1991)    (paper.rank        doyle91 "[****]")    (paper.description doyle91 "
Doyle and Patil argue against Levesque and Brachman's position that a
general purpose KB system must be able to answer all questions 
in a bounded amount of time.  In particular, they examine KL-ONE
and its variants, enumerating classes of problems that a restricted
language cannot express but that would be natural for use in a KB
system.  They argue that worst-case complexity is the right measure for
just a small fraction of all potential applications of a Knowledge-based
system.  
")    (paper.topic       doyle91 "Logical Knowledge Representation")    (paper.instance    dymetman91)    (paper.bibtex      dymetman91 article)    (paper.author      dymetman91 "M. Dymetman")    (paper.title       dymetman91 "Inherently Reversible Grammars, Logic Programming and Computability")    (paper.link        dymetman91 "http://acl.ldc.upenn.edu/W/W91/W91-0104.pdf")    (paper.year        dymetman91 1991)    (paper.rank        dymetman91 "[***]")    (paper.description dymetman91 "
Dymetman offers a decoupling of the term reversible grammar into (1)
uniformity of implementation for parsers and generators and (2) 
reversibility as an inherent property of a grammar.  He formalizes
the second idea, which includes various notions about the completeness
and termination of answer extraction for logic programming.
")    (paper.topic       dymetman91 "Natural Language Processing")    (paper.instance    ebbinghaus)    (paper.bibtex      ebbinghaus book)    (paper.author      ebbinghaus "Heinz-Dieter Ebbinghaus and Jorg Flum")    (paper.title       ebbinghaus "Finite Model Theory")    (paper.publisher   ebbinghaus "Springer-Verlag")    (paper.link        ebbinghaus "http://www.amazon.com/Finite-Theory-Springer-Monographs-Mathematics/dp/3540287876")    (paper.year        ebbinghaus 1999)    (paper.rank        ebbinghaus "[*****]")    (paper.description ebbinghaus "
The authors introduce Finite Model Theory, a logic that is identical to first-order
logic but where a model is defined as having a finite universe.  Topics covered
include Ehrenfeucht-Fraisse Method, 01 laws, finite automata, descriptive
complexity theory, fixed-point operators, logic programs, logics for PTIME, 
and logical reductions.
")    (paper.topic       ebbinghaus "")    (paper.instance    eiter2001)    (paper.bibtex      eiter2001 inproceedings)    (paper.author      eiter2001 "Thomas Eiter and Toshihide Ibaraki and Kazuhisa Makino")    (paper.title       eiter2001 "Disjunctions of Horn Theories and their Cores")    (paper.publication eiter2001 "ISAAC")    (paper.link        eiter2001 "http://citeseer.ist.psu.edu/eiter01disjunctions.html")    (paper.year        eiter2001 2001)    (paper.rank        eiter2001 "[*]")    (paper.description eiter2001 "
This paper studies the problem of determining whether a disjunction of 
Horn theories is Horn and if not computing a Horn core and envelope.
Eiter et. al. investigate both the characteristic model representation
of Horn theories and the CNF representation.  For both representations 
the problem is intractable.
")    (paper.topic       eiter2001 "Characteristic Models")    (paper.instance    eiter2004)    (paper.bibtex      eiter2004 inproceedings)    (paper.author      eiter2004 "Thomas Eiter and Thomas Lukasiewicz and Roman Schindlauer and Hans Tompits")    (paper.title       eiter2004 "Combining Answer Set Programming with Description Logics for the Semantic Web")    (paper.publication eiter2004 "Proceedings of Knowledge Representation and Reasoning")    (paper.link        eiter2004 "http://citeseer.ist.psu.edu/727609.html")    (paper.year        eiter2004 2004)    (paper.description eiter2004 "
A language that integrates SHIF(D) and SHOIN(D) with answer set programming
")    (paper.topic       eiter2004 "Semantic Web")    (paper.instance    eiter98)    (paper.bibtex      eiter98 inproceedings)    (paper.author      eiter98 "Thomas Eiter and Thoshihide Ibaraki and Kazuhisa Makino")    (paper.title       eiter98 "Computing Intersections of Horn Theories for Reasoning with Models")    (paper.publication eiter98 "AAAI")    (paper.link        eiter98 "http://citeseer.ist.psu.edu/537461.html")    (paper.year        eiter98 1998)    (paper.rank        eiter98 "[****]")    (paper.description eiter98 "
The authors examine complexity issues that arise when intersecting several
propositional horn knowledge bases, which are represented with 
characteristic models.  Finding
the unique minimal model (which is a characteristic model) can be done
in linear time in the number of KBs.  Finding all-models can be solved
with polynomial delay (a polynomial amount of time between outputting
models).  In a sense, that means the procedure is polynomial, but the 
number of outputs could be exponential.  Finding all characteristic
models of the intersection has no polynomial time algorithm (even if
P = NP).  Answering deductive queries can still be accomplished in
polynomial time by looking at each KB independently.  Finally, abduction
is shown to be intractable, which differs from the result of a single
KB (where it is polynomial).
")    (paper.topic       eiter98 "Characteristic Models")    (paper.instance    enderton)    (paper.bibtex      enderton book)    (paper.author      enderton "Herbert Enderton")    (paper.title       enderton "A Mathematical Introduction to Logic")    (paper.publisher   enderton "Academic Press")    (paper.link        enderton "http://www.amazon.com/exec/obidos/tg/detail/-/0122384520/qid=1062605496/sr=1-6/ref=sr_1_6/102-6107129-2481765?v=glance&s=books")    (paper.year        enderton 2000)    (paper.rank        enderton "[*****]")    (paper.description enderton "
The definitive text on introductory mathematical logic.  Enderton covers both 
propositional (sentential),  and first-order logic including compactness, completeness,
and soundness proofs.  The third chapter covers undecidability, Godel's 
incompleteness proof, and Church's and Tarski's theorems.  The last chapter
covers second-order logic. 
")    (paper.topic       enderton "")    (paper.instance    etherington89)    (paper.bibtex      etherington89 article)    (paper.author      etherington89 "David Etherington and Alex Borgida and Ronald Brachman and Henry Kautz")    (paper.title       etherington89 "Vivid Knowledge and Tractable Reasoning")    (paper.publication etherington89 "IJCAI")    (paper.startpage   etherington89 1146)    (paper.endpage     etherington89 1152)    (paper.link        etherington89 "http://citeseer.ist.psu.edu/etherington89vivid.html")    (paper.year        etherington89 1989)    (paper.rank        etherington89 "[****]")    (paper.description etherington89 "
Etherington, et.al. give a preliminary description of representing a KB with a set of atoms.  They try to motivate the work by appealing to the speed of humans' commonsense reasoning facilities as evidence that much such reasoning is done via lookup.   A set of atoms is not very expressive, so the first extension is to allow definite clauses: a(x)=>b(x).  To deal with this they use a closed world assumption.  Second extension tries to deal with disjunction: age(joe,53) | age(joe,55) is replaced by age(joe,x)^in50s(x).  Third extension uses skolems to remove disjunction: p(a) | p(b) becomes p(k). 
")    (paper.topic       etherington89 "Logical Knowledge Representation")    (paper.instance    fagin82)    (paper.bibtex      fagin82 article)    (paper.author      fagin82 "Ronald Fagin")    (paper.title       fagin82 "Horn Clauses and Database Dependencies")    (paper.publication fagin82 "Journal of the ACM")    (paper.volume      fagin82 "29(4)")    (paper.startpage   fagin82 952)    (paper.endpage     fagin82 985)    (paper.link        fagin82 "http://citeseer.ist.psu.edu/context/62052/0")    (paper.year        fagin82 1982)    (paper.rank        fagin82 "[****]")    (paper.description fagin82 "
Fagin generalizes dependencies in the database literature and
shows the following are all equivalent for a given set of sentences
S.  1) There is an operator O that maps nonempty families of models
into models such that if sigma is a sentence in S and R_i is a
nonempty family of models, then sigma holds for O(R_i) iff sigma
holds for each R_i.  2) Whenever Sigma is a consistent subset of S
and Sigma* is the set of sentences in S that are logical consequences
of Sigma, there is an Armstrong model--it obeys Sigma* and no other
sentences in S.  3) Whenever Sigma is a subset of S and phi_i is a 
nonempty subset of S then Sigma |= V phi_i iff there is some i for
which Sigma |= phi_i.  For FOL in general, there is no Armstrong relation for 
first-order axiom sets.  Take the empty set of axioms, and assume
there is an Armstrong relation R.  R will either entail or not
entail any non-tautology sigma, which means R is not an Armstrong
relation for the axiom set.  A corollary to this theorem shows
that any axiom set Delta in a Horn-related subset of FOL can be
represented with a single, infinite model.
")    (paper.topic       fagin82 "Characteristic Models")    (paper.instance    fikes71)    (paper.author      fikes71 "R.E. Fikes and N.J. Nilsson")    (paper.title       fikes71 "STRIPS: a new approach to the application of theorem proving to problem solving")    (paper.publication fikes71 "Artificial Intelligence")    (paper.volume      fikes71 "2(3-4)")    (paper.startpage   fikes71 189)    (paper.endpage     fikes71 208)    (paper.year        fikes71 1971)    (paper.description fikes71 "
The STRIPS paper.  Fikes and Nilsson allow first-order formulas to describe the action preconditions and effects (add/delete lists) but maintain that each operator instanatiation must be ground.  They note that the case of non-ground instantiations needs more study.  Interestingly, they generate successor states in the search tree using a form of residue from a proof that a given world state achieves a given subgoal.  They implement a world state as being a set of changes from the initial state.
")    (paper.topic       fikes71 "Historical")    (paper.instance    fikes72)    (paper.author      fikes72 "Richard Fikes, Peter Hart and Nils Nilsson")    (paper.title       fikes72 "Learning and Executing Generalized Robot Plans")    (paper.publication fikes72 "Artificial Intelligence")    (paper.volume      fikes72 "3(4)")    (paper.startpage   fikes72 251)    (paper.endpage     fikes72 288)    (paper.year        fikes72 1972)    (paper.description fikes72 "
Fikes, Hart, and Nillson take a STRIPS planner, 1) describe a concise
representation for plans (triangle tables), 2) develop a method for
generalizing plans to make new primitive actions (MACROPs), 3) describe
an execution engine (PLANEX) that uses these Macrops that does replanning and
handles coincidental goal achievement.  They go on to describe needed future
improvements: 1) creating a set of abstractions for the new primitives actions
to avoid dealing with large numbers of preconditions and 2) discarding
subsumed or otherwise unused primitive operators to avoid an ever-growing
operator set.
")    (paper.topic       fikes72 "Historical")    (paper.instance    fikes93)    (paper.author      fikes93 "R.E. Fikes and N.J. Nilsson")    (paper.title       fikes93 "STRIPS, a retrospective")    (paper.publication fikes93 "Artificial Intelligence")    (paper.volume      fikes93 "59(1-2)")    (paper.startpage   fikes93 227)    (paper.endpage     fikes93 232)    (paper.year        fikes93 1993)    (paper.description fikes93 "
This short paper puts the STRIPS work in a historical context, examining both the plan generator and execution monitor.  Green's work on deductive plan synthesis was the first real planner, but most researchers didn't explore monitoring execution (they were assuming computational environments).  Fikes and Nilsson used 'kernels' to represent the sentences that must be true at each stage of the plan for the rest of the plan to succeed.  They also used the notion of a 'triangle table' to represent plans so that serendipitous acts and action-failure-but-that's-all-you-can-do-so-just-try-again events would not require replanning.
")    (paper.topic       fikes93 "Historical")    (paper.instance    fitting2007)    (paper.bibtex      fitting2007 article)    (paper.author      fitting2007 "Melvin Fitting")    (paper.title       fitting2007 "Intensional Logic")    (paper.publication fitting2007 "The Stanford Encyclopedia of Philosophy (Spring 2007 Edition), Edward N. Zalta (ed.)")    (paper.link        fitting2007 "http://plato.stanford.edu/archives/spr2007/entries/logic-intensional/")    (paper.year        fitting2007 2007)    (paper.description fitting2007 "
The mathematical foundations of logical query languages.
")    (paper.topic       fitting2007 "Logical Knowledge Representation")    (paper.instance    follett80)    (paper.bibtex      follett80 article)    (paper.author      follett80 "Ria Follett")    (paper.title       follett80 "Synthesising Recursive Functions with Side Effects")    (paper.publication follett80 "Artificial Intelligence")    (paper.volume      follett80 "13:3")    (paper.startpage   follett80 175)    (paper.endpage     follett80 200)    (paper.year        follett80 1980)    (paper.rank        follett80 "[*]")    (paper.description follett80 "
")    (paper.topic       follett80 "Theorem Proving with Attachments")    (paper.instance    franconi2004)    (paper.bibtex      franconi2004 inproceedings)    (paper.author      franconi2004 "Enrico Franconi and Sergio Tessaris")    (paper.title       franconi2004 "Rules and Queries with Ontologies: A Unified Logical Framework")    (paper.publication franconi2004 "Proceedings of Principles and Practice of Semantic Web Reasoning")    (paper.link        franconi2004 "http://www.springerlink.com/content/1hnwcggw510elp4y/")    (paper.year        franconi2004 2004)    (paper.description franconi2004 "
A comparison of various techniques for integrating logic programming with 
description logics.
")    (paper.topic       franconi2004 "Semantic Web")    (paper.instance    gammer2007)    (paper.bibtex      gammer2007 inproceedings)    (paper.author      gammer2007 "Igor Gammer and Eyal Amir")    (paper.title       gammer2007 "Solving Satisfiability in Ground Logic with Equality by Efficient Conversion to Propositional Logic")    (paper.publication gammer2007 "Proc. 7th Symposium on Abstraction, Reformulation, and Approximation")    (paper.link        gammer2007 "http://reason.cs.uiuc.edu/eyal/paper.html")    (paper.year        gammer2007 2007)    (paper.description gammer2007 "
The authors consider ground first-order logic with equality and show
how one can employ Craig's Interpolation theorem to reduce the cost
of grounding out the usual axiomatization of equality.
")    (paper.topic       gammer2007 "Reformulation")    (paper.instance    geddis95)    (paper.bibtex      geddis95 phdthesis)    (paper.author      geddis95 "Don Geddis")    (paper.title       geddis95 "Caching and Non-Horn Inference in Model Elimination Theorem Provers")    (paper.publisher   geddis95 "Stanford University")    (paper.link        geddis95 "http://library.stanford.edu/depts/mathcs/mathcscoll/techreports.html")    (paper.year        geddis95 1995)    (paper.rank        geddis95 "[***]")    (paper.description geddis95 "
Geddis investigates Nonhorn caching for model elimination.  Astrachan and
Stickel did the seminal work on caching for the Horn case.  Nonhorn
caching is made difficult by the reduction operation, i.e. proofs of
subgoals are context dependent.  Previous work stored the context and
the proven subgoal.  Geddis shows that if a literal has a completion in
one context, it has a completion in every context.  Thus, if a subgoal
cannot be completed, it can be added to a failure cache.  A completion is
not the same as a proof, however, and there are no results for success
caching.  If a depth-limited search-strategy is used, incompleteness can
result; Geddis found no solution for this problem.  Also discussed is an
extension to David Smith's work: postponement caching.  It basically avoids
infinite recursions if possible by 'enslaving' a subgoal to an ancestor
if the two unify.  It is complete for Horn and incomplete for nonHorn.
")    (paper.topic       geddis95 "Theorem Proving with Attachments")    (paper.instance    gelernter63)    (paper.bibtex      gelernter63 article)    (paper.author      gelernter63 "H. Gelernter")    (paper.title       gelernter63 "Realization of a Geometry-Theorem Proving Machine")    (paper.publication gelernter63 "Computers and Thought")    (paper.startpage   gelernter63 134)    (paper.endpage     gelernter63 152)    (paper.year        gelernter63 1963)    (paper.rank        gelernter63 "[*]")    (paper.description gelernter63 "
Gelernter describes the Geometry Machine, an ad hoc theorem prover
for Euclidian geometry.  It uses a diagram as a heuristic to guide
the theorem prover's search, eliminating subgoals when not 
satisfied by the diagram.  Sometimes that diagram prunes too much,
in which case multiple diagrams need to be generated to achieve
completeness.  The system is also unique in the fact that it does
not give the theorem prover a complete axiomitization of geometry,
but rather allows it to add axioms that are 'obvious' from the 
diagram, much as a human might do.
")    (paper.topic       gelernter63 "Graphical Reasoning Techniques")    (paper.instance    gelfond88)    (paper.bibtex      gelfond88 article)    (paper.author      gelfond88 "Michael Gelfond and Vladimir Lifschitz")    (paper.title       gelfond88 "The Stable Model Semantics for Logic Programming")    (paper.publication gelfond88 "Proceedings of the Fifth International Conference on Logic Programming")    (paper.startpage   gelfond88 1070)    (paper.endpage     gelfond88 1080)    (paper.link        gelfond88 "http://citeseer.ist.psu.edu/gelfond88stable.html")    (paper.year        gelfond88 1988)    (paper.description gelfond88 "
Stable model semantics for logic programming.
")    (paper.topic       gelfond88 "Logic Programming")    (paper.instance    genesereth87)    (paper.bibtex      genesereth87 book)    (paper.author      genesereth87 "Michael Genesereth and Nils Nilsson")    (paper.title       genesereth87 "Logical Foundations of Artificial Intelligence")    (paper.publisher   genesereth87 "Morgan Kaufmann Publishers")    (paper.link        genesereth87 "http://www.amazon.com/Foundations-Artificial-Intelligence-Michael-Genesereth/dp/0934613311")    (paper.year        genesereth87 1987)    (paper.rank        genesereth87 "[*****]")    (paper.description genesereth87 "
Genesereth and Nilsson construct the foundations of AI using logic.  Covers logic, resolution and various strategies, nonmonotonic reasoning, induction, uncertainty, belief,
metaknowledge and metareasoning, a changing environment, and agent architectures.
")    (paper.topic       genesereth87 "")    (paper.instance    giacomo2000)    (paper.author      giacomo2000 "G. De Giacomo, Y. Lesperance, and H.J. Levesque")    (paper.title       giacomo2000 "ConGolog, a concurrent prrogramming language based on the situation calculus")    (paper.publication giacomo2000 "Artificial Intelligence")    (paper.volume      giacomo2000 "121")    (paper.startpage   giacomo2000 109)    (paper.endpage     giacomo2000 169)    (paper.link        giacomo2000 "http://www.cs.yorku.ca/~lesperan/publications.html")    (paper.year        giacomo2000 2000)    (paper.description giacomo2000 "
ConGolog extends Golog (both implementations being in Prolog) to include concurrent actions (conceptualized as interleaving actions of different processes that can block, in the OS sense).  The constructs introduced here include synchronized conditionals (test-and-set), synchronized loops, concurrent execution, concurrency with different priorities (the lower priority process can only execute if the higher priority one is blocked or finished), concurrent iteration, and interrupts.  Nondeterminstic iteration can be useful when an unspecified number of programs need to be run concurrently, e.g. an FTP server.  Some more work is required to handle procedures.  The semantics given in the original Golog paper were evaluation semantics.  Here the authors switch to transition semantics and require all the macros of Golog to become embedded in the language.
Exogenous events are handled by allowing the user to specify which primitive actions may occurs outside the control of the program.  A predefined program for choosing a possible exogenous action, checking its preconditions, and executing it if possible is run concurrently with the user-defined program. To handle truly overlapping actions such as filling-bath-tub and singing-do-re-mi, the authors suggest thinking of filling-bath-tub as not an action, but rather as a state, which requires the start-filling action to enter the state and the stop-filling action to exit the state.  This way, interleaving filling-bath-tub and singing-do-re-mi can happen at the same time.
")    (paper.topic       giacomo2000 "Logic Programming")    (paper.instance    giunchiglia92)    (paper.bibtex      giunchiglia92 article)    (paper.author      giunchiglia92 "Fausto Giunchiglia and Toby Walsh")    (paper.title       giunchiglia92 "A theory of abstraction")    (paper.publication giunchiglia92 "Artificial Intelligence")    (paper.volume      giunchiglia92 "57")    (paper.startpage   giunchiglia92 323)    (paper.endpage     giunchiglia92 389)    (paper.link        giunchiglia92 "http://citeseer.ist.psu.edu/27696.html")    (paper.year        giunchiglia92 1992)    (paper.rank        giunchiglia92 "[****]")    (paper.description giunchiglia92 "
Giunchiglia and Walsh describe a theory of abstraction in very general terms,
which is intended to be independent of proof system.  Most generally,
an abstraction is a mapping between a pair of formal systems, where a
formal system consists of a language and a subset of that language defining
the axioms.  Axiomatic formal systems include an extra element: the
proof system.  Abstractions are categorized by whether they increase, decrease, or
leave constant the set of consequences of the theory: TI, TD, TC.  The authors
claim the TI abstractions is the true meaning of abstraction.  TD abstractions
can be used to prove entailment, i.e. if TD(Delta) |= phi then Delta |= phi.
TI abstractions can be used to prove negative entailment, i.e. if
TI(Delta) |/= phi then Delta |/= phi.  Refutation systems are concerned
not with provability but with inconsistency.  Abstractions in this setting
are characterized as NTI, NTD, and NTC.  If the formal system includes
negation and the abstraction f is negation preserving, i.e. f(-phi) = -f(phi)
then NT* and T* are equivalent.  Further classifications are made of abstractions,
depending on how independently an abstraction translates axioms, inference rules, etc.
A large section explaining how various peoples' work fits into this framework follows.
Then sections follow on inconsistent abstract spaces, operations performed
on abstractions, ordering abstractions, hierarchies of abstraction spaces, and building
abstractions.
")    (paper.topic       giunchiglia92 "Abstraction")    (paper.instance    giunchiglia96reasoning)    (paper.bibtex      giunchiglia96reasoning inproceedings)    (paper.author      giunchiglia96reasoning "Fausto Giunchiglia and Paolo Pecchiari and Carolyn Talcott")    (paper.title       giunchiglia96reasoning "Reasoning Theories - Towards an Architecture for Open Mechanized Reasoning Systems")    (paper.publication giunchiglia96reasoning "1st International Workshop: Frontiers of Combining Systems
           ")    (paper.startpage   giunchiglia96reasoning 157)    (paper.endpage     giunchiglia96reasoning 174)    (paper.link        giunchiglia96reasoning "http://citeseer.ist.psu.edu/354680.html")    (paper.year        giunchiglia96reasoning 1996)    (paper.rank        giunchiglia96reasoning "[***]")    (paper.description giunchiglia96reasoning "Giunchiglia et al. describe a formalization of logical reasoners.  Such
reasoners (quoting) might be based on different logics; have different domain models;
use different vocabularies and data structures; use different reasoning
strategies; and have different interaction capabilities.  The authors
put forth a general architecture called Open Mechanized Reasoning Systems
(OMRS) for integrating reasoners described by what they call a
Reasoning Theory.  They use NQTHM as an example to illustrate the construction
of a Reasoning Theory.
")    (paper.topic       giunchiglia96reasoning "Architectures")    (paper.instance    giunchiglia99applying)    (paper.bibtex      giunchiglia99applying inproceedings)    (paper.author      giunchiglia99applying "Enrico Giunchiglia and Roberto Sebastiani")    (paper.title       giunchiglia99applying "Applying the Davis-Putnam Procedure to Non-clausal Formulas")    (paper.publication giunchiglia99applying "Proceedings of the 6th Congress of the Italian Association for Artificial Intelligence on Advances in Artificial Intelligence")    (paper.startpage   giunchiglia99applying 84)    (paper.endpage     giunchiglia99applying 94)    (paper.link        giunchiglia99applying "http://citeseer.ist.psu.edu/giunchiglia99applying.html")    (paper.year        giunchiglia99applying 1999)    (paper.description giunchiglia99applying "
Continuation of their earlier work on building a SAT solver that handles
non-clausal form by translating the problem into CNF in such a way that 
the search space of the non-CNF problem is preserved in some way.
")    (paper.topic       giunchiglia99applying "SAT Solving")    (paper.instance    giunchiglia99planning)    (paper.bibtex      giunchiglia99planning inproceedings)    (paper.author      giunchiglia99planning "Fausto Giunchiglia and Paolo Traverso")    (paper.title       giunchiglia99planning "Planning as Model Checking")    (paper.link        giunchiglia99planning "http://citeseer.ist.psu.edu/giunchiglia99planning.html")    (paper.year        giunchiglia99planning 1999)    (paper.rank        giunchiglia99planning "[*]")    (paper.description giunchiglia99planning "
This paper gives an introduction to Planning as Model Checking for
both deterministic and nondeterministic domains.  It explains
the model checking problem, planning as model checking, 
planning in nondeterministic domains, an implementation, and
related work.  They look at problems formalized in Computation
Tree Logic (CTL).
")    (paper.topic       giunchiglia99planning "Model Checking")    (paper.instance    gramlich2005)    (paper.bibtex      gramlich2005 article)    (paper.author      gramlich2005 "Bernhard Gramlich")    (paper.title       gramlich2005 "Strategic issues, Problems and Challenges in Inductive Theorem Proving")    (paper.link        gramlich2005 "http://www.logic.at/staff/gramlich/ papers/strategies04-entcs05.pdf.gz")    (paper.year        gramlich2005 2005)    (paper.rank        gramlich2005 "[***]")    (paper.description gramlich2005 "
Gramlich outlines the inductive theorem proving, how it differs from
first-order theorem proving, and some of the obstacles that must be
overcome to make automated inductive theorem proving a reality.
")    (paper.topic       gramlich2005 "Herbrand Logic")    (paper.instance    green69)    (paper.author      green69 "C. Green")    (paper.title       green69 "Theorem-proving by resolution as a basis for question-answering systems.")    (paper.publication green69 "Machine Intelligence 4")    (paper.startpage   green69 183)    (paper.endpage     green69 205)    (paper.year        green69 1969)    (paper.description green69 "
Green describes using a theorem prover, QA3, to build a question-answering system.  Both statements and questions are written in FOL.  Questions can be True/False or Constructive (i.e. ask for an x such that ...).  Constructive answers are produced by adding the rule question-of-user(x) => Answer(x).  Green goes on to describe what we now call deductive plan synthesis, i.e. answer extraction of situation calculus axioms.  QA3 uses subsumption, duplicate literal elimination, unit preference, a variant on set of support, and no equality (yet).
")    (paper.topic       green69 "Fundamentals")    (paper.instance    greiner91)    (paper.bibtex      greiner91 article)    (paper.author      greiner91 "Russell Greiner and Charles Elkan")    (paper.title       greiner91 "Measuring and Improving the Effectiveness of Representations")    (paper.publication greiner91 "IJCAI")    (paper.startpage   greiner91 518)    (paper.endpage     greiner91 524)    (paper.link        greiner91 "http://citeseer.ist.psu.edu/114212.html")    (paper.year        greiner91 1991)    (paper.rank        greiner91 "[****]")    (paper.description greiner91 "
Greiner and Elkan give axes upon which a representation, i.e. black box
for answering questions, can be evaluated: accuracy (how often the answer
given is correct), categoricity (how often the answer is I don't know), and
efficiency. A representation is thus evaluated wrt a query stream.  This 
external form of evaluation captures what any system using a representation
system actually cares about -- how well that representation works.  The
internal representation might also be evaluated by, for example, its
conciseness or elegance, but that is not covered here.  They also formalize
all bounded, linearly separable utility measures, i.e. ways of combining
evaluations wrt the 3 axes above.  Finally the authors confront statistical
approaches for performing evaluation, comparing representations, and improving
existing representations.   Well written.
")    (paper.topic       greiner91 "Logical Knowledge Representation")    (paper.instance    greiner98)    (paper.bibtex      greiner98 article)    (paper.author      greiner98 "Russell Greiner and Christian Darken and N. Santoso")    (paper.title       greiner98 "Efficient Reasoning")    (paper.publication greiner98 "ACM Computing Surveys")    (paper.volume      greiner98 "33(1)")    (paper.startpage   greiner98 1)    (paper.endpage     greiner98 30)    (paper.link        greiner98 "http://citeseer.ist.psu.edu/greiner98efficient.html")    (paper.year        greiner98 1998)    (paper.rank        greiner98 "[*]")    (paper.description greiner98 "
Greiner gives a survey of techniques for efficient reasoning: both deductive 
and probabilistic.  He talks about the various techniques available for
dealing with exponential run times: unsound inference, incompleteness, etc.
")    (paper.topic       greiner98 "Logical Knowledge Representation")    (paper.instance    grohe2001)    (paper.bibtex      grohe2001 inproceedings)    (paper.author      grohe2001 "Martin Grohe")    (paper.title       grohe2001 "Generalized Model-Checking Problems for First-Order Logic")    (paper.publication grohe2001 "Symposium on Theoretical Aspects of Computer Science")    (paper.startpage   grohe2001 12)    (paper.endpage     grohe2001 26)    (paper.link        grohe2001 "http://www.dcs.ed.ac.uk/home/grohe/pub.html")    (paper.year        grohe2001 2001)    (paper.rank        grohe2001 "[****]")    (paper.description grohe2001 "
Grohe investigates the parameterized complexity of various model-checking
problems in first-order logic.  In general, the problem is PSPACE-complete
for the combined size of the query and the model.  Under the assumption
of a small query, the parameterized complexity is still AW[*].  However,
some types of formulas and models lend themselves to tractable algorithms:
those with an underlying tree structure.  This paper gives a survey
of various model-checking complexity results for first-order logic.
")    (paper.topic       grohe2001 "Model Checking")    (paper.related     grohe2001 grohe2002)    (paper.instance    grohe2002)    (paper.bibtex      grohe2002 article)    (paper.author      grohe2002 "Martin Grohe")    (paper.title       grohe2002 "Parameterized Complexity for the Database Theorist")    (paper.publication grohe2002 "SIGMOD")    (paper.volume      grohe2002 "31(4)")    (paper.link        grohe2002 "http://www.dcs.ed.ac.uk/home/grohe/pub.html")    (paper.year        grohe2002 2002)    (paper.rank        grohe2002 "[*]")    (paper.description grohe2002 "
Grohe gives a gentle introduction to Parameterized Complexity Theory for
database theorists.  The theory defines a complexity hierarchy for
problems with nonstandard constraints and reductions for proofs
of problem locations within the hierarchy.
These parameterized problems include more information than their standard
counterparts, and thus, can have better solutions.
")    (paper.topic       grohe2002 "Knowledge Base Compilation")    (paper.instance    grosof2003)    (paper.bibtex      grosof2003 inproceedings)    (paper.author      grosof2003 "Benjamin Grosof and Ian Horrocks and Raphael Volz and Stefan Decker")    (paper.title       grosof2003 "Description Logic Programs: Combining Logic Programs with Description Logic")    (paper.publication grosof2003 "Proceedings of the 12th International Conference on the World Wide Web")    (paper.link        grosof2003 "http://citeseer.ist.psu.edu/grosof03description.html")    (paper.year        grosof2003 2003)    (paper.description grosof2003 "
Description Logic Programs are introduced, which lie in the intersection 
of logic programming and OWL DL.  In addition, mechanisms for reasoning
about this fragment with either description logic or logic programming
tools are introduced.
")    (paper.topic       grosof2003 "Semantic Web")    (paper.instance    gurevich90)    (paper.bibtex      gurevich90 article)    (paper.author      gurevich90 "Yuri Gurevich")    (paper.title       gurevich90 "On the Classical Decision Problem")    (paper.publication gurevich90 "Bulletin of the European Association for Theoretical Computer Science")    (paper.volume      gurevich90 "42")    (paper.startpage   gurevich90 140)    (paper.endpage     gurevich90 150)    (paper.link        gurevich90 "http://citeseer.ist.psu.edu/gurevich93classical.html")    (paper.year        gurevich90 1990)    (paper.rank        gurevich90 "[*]")    (paper.description gurevich90 "
This paper takes the form of a dialogue between the Author and Quisani.
It gives an introduction to the problem of determining whether a first-
order formula is satisfiable (or valid).  The classic decision problem
is whether this problem is decidable.  Church and Turing showed
it to be undecidable.  Then the question becomes, for which
formulas is this decidable?  Classes are given based on the 
quantifier prefixes that are decidable.  More interestingly, 
a certain set of 4 prefixes are enough to completely solve the
decision problem for prefix classes.
")    (paper.topic       gurevich90 "Decidable Fragments of First-Order Logic")    (paper.instance    haas86)    (paper.author      haas86 "Andrew R. Haas")    (paper.title       haas86 "A Syntactic Theory of Belief and Action")    (paper.publication haas86 "Artificial Intelligence")    (paper.volume      haas86 "28(3)")    (paper.startpage   haas86 245)    (paper.endpage     haas86 292)    (paper.link        haas86 "http://www.reviews.com/Review/Review_review.cfm?media_id=778843&reviewer=110938&pos=3&page=4")    (paper.year        haas86 1986)    (paper.description haas86 "
Haas describes an approach for reasoning about belief, both an agent's own
beliefs and another agent's belief's.  First, we represent an agent's belief's
with Believes(agent, sentence), where sentence names a sentence of FOL.  Haas
names sentences of FOL by quoting each component, i.e. p(a) is named 'p('a)
(it looks cleaner in prefix notation).  He also gives names for intervals of
time so he can state how long it will take for an agent to believe a certain
sentence; afterall inference is not instantaneous.   In order to give an agent
the ability to determine what another agent will infer, Haas borrows
Konolige's suggestion of implanting an agent's inference rules into the
Believes statements.  Then, Haas introduces The Reflection Schema to do
inference.  Specifically,
Ax1,..,xn. (ClosedTerm x1)^...^(ClosedTerm xn) => IsProof(s)
s is a proof quoted appropriately.  He shows this schema sound and complete.
Haas goes on to define 'Knowing What' in terms of the context of what was asked.
'Knowing How' means a robot has a program to execute.  Finally, Haas connnects
belief and truth using the truth predicate: 'true(quote p) 'iff 'p.  The
Liar's paradox shows up here, but Haas avoids it by proving if p is ground,
true produces no contradictions.
")    (paper.topic       haas86 "Metalevel Reasoning")    (paper.instance    hahnle2002)    (paper.author      hahnle2002 "Reiner Hahnle and Neil Murray and Erik Rosenthal")    (paper.title       hahnle2002 "Ordered Resolution vs. Connection Graph Resolution")    (paper.link        hahnle2002 "http://citeseer.ist.psu.edu/446349.html")    (paper.year        hahnle2002 2002)    (paper.description hahnle2002 "
Hahnle et. al. first describe connection graph resolution for
propositional logic, introduced by Kowalski in 1975.  
CGR begins with a graph of clauses with
edges between complementary literals.  Activating an edge
means performing a step of resolution, producing a new 
clause graph.  Then they show that ordered resolution
is a special case of cg-resolution (for prop logic) and then
that ordered cg-resolution will always terminate.  Ordered
cg-resolution enforces a total ordering on literals so
that resolution is applied only to the maximally ordered
literal in a clause.  It is a little unclear to what extent
the results are new, but apparently
the proofs are simpler than those given in the past.  At the end,
the authors discuss issues for lifting this to first-order and
give Eisinger's example that shows unrestricted cg-resolution
is non-terminal.  
")    (paper.topic       hahnle2002 "Calculi")    (paper.instance    halevy2001)    (paper.bibtex      halevy2001 article)    (paper.author      halevy2001 "Alon Halevy")    (paper.title       halevy2001 "Answering Queries Using Views: A Survey")    (paper.publication halevy2001 "VLDB Journal: Very Large Data Bases")    (paper.volume      halevy2001 "10(4)")    (paper.startpage   halevy2001 270)    (paper.endpage     halevy2001 294)    (paper.link        halevy2001 "http://citeseer.ist.psu.edu/halevy00answering.html")    (paper.year        halevy2001 2001)    (paper.rank        halevy2001 "[****]")    (paper.description halevy2001 "
Halevy's survey splits the work that answers queries using views into two
cateogires: using materialized views to speed up query answering and
data integration.  In the former setting, materialized views have the potential
to speed up query answering because some portion of the query may have already
been computed and stored in the db as a materialized view.  The goal here is
to produce an efficient execution plan that uses base tables and materialized views.
In the latter setting, data integration techniques provide a schema, sometimes
called the mediated schema, for a user to query a large number of separate
databases.  To define how each individual schema relates to the mediated schema,
the tables in the individual schemas are expressed as views of the mediated schema.
Here the work focuses on rewriting the query in terms of the views of the mediated
schema, i.e. in terms of the database schema that actually exist.  This case is
particularly interesting because it requires the views to be treated as
incomplete, e.g. both the database of american cars and the database of foreign
cars contribute to the car table.
")    (paper.topic       halevy2001 "Databases")    (paper.instance    halmos)    (paper.bibtex      halmos book)    (paper.author      halmos "Paul Halmos")    (paper.title       halmos "Naive Set Theory")    (paper.publisher   halmos "Van Nostrand Reinhold Company")    (paper.link        halmos "http://www.amazon.com/Naive-Theory-Undergraduate-Texts-Mathematics/dp/0387900926")    (paper.year        halmos 1960)    (paper.rank        halmos "")    (paper.description halmos "
A 100-page book on naive set theory.
")    (paper.topic       halmos "")    (paper.instance    halpern91)    (paper.author      halpern91 "Joseph Halpern and Moshe Vardi")    (paper.title       halpern91 "Model Checking vs. Theorem Proving: A Manifesto")    (paper.publication halpern91 "Artificial and mathematical Theory of Computation
       (Papers in Honor of John McCarthy)")    (paper.link        halpern91 "http://www.cs.cornell.edu/home/halpern/abstract.html#bookart4")    (paper.year        halpern91 1991)    (paper.rank        halpern91 "[****]")    (paper.description halpern91 "
Halpern and Vardi prescribe using a semantic model to represent an agent's
knowledge and use model checking to determine logical entailment.  This differs
greatly from the standard approach of using logical sentences to represent
knowledge and then doing theorem proving to check entailment.  The authors
outline constructing such models in a few contexts and consider problems with
the model checking.
")    (paper.topic       halpern91 "Model Checking")    (paper.instance    hammer93)    (paper.bibtex      hammer93 article)    (paper.author      hammer93 "Peter Hammer and Alexander Kogan")    (paper.title       hammer93 "Optimal Compression of Propositional Horn Knowledge Bases: Complexity and Approximation")    (paper.publication hammer93 "Artificial Intelligence")    (paper.volume      hammer93 "64(1)")    (paper.startpage   hammer93 131)    (paper.endpage     hammer93 145)    (paper.link        hammer93 "http://citeseer.ist.psu.edu/hammer93optimal.html")    (paper.year        hammer93 1993)    (paper.rank        hammer93 "[*]")    (paper.description hammer93 "
Hammer and Kogan show that finding the minimum propositional Horn
KB of a given Horn KB is NP-Complete.  They also show a previous
O(n^2) approximation algorithm to be a good one since it allows
no more than a linear factor more rules.
")    (paper.topic       hammer93 "Knowledge Base Compilation")    (paper.instance    hayes75)    (paper.author      hayes75 "Philip J. Hayes")    (paper.title       hayes75 "A Representation for Robot Plans")    (paper.publication hayes75 "IJCAI")    (paper.startpage   hayes75 181)    (paper.endpage     hayes75 188)    (paper.year        hayes75 1975)    (paper.description hayes75 "
Due to the need for replanning, Hayes advocates producing plans that consist
of two data structures: a subgoal tree and a decision graph.  The subgoal tree represents the plan in a hierarchical-type planner, i.e. root is the overall
goal; each internal node is a reduction of its parent; each leaf is a
primitive action.  The decision graph records the decisions made by the planner while producing the plan.  Links exist between the decision graph and the
subgoal tree so that when a failure occurs during plan execution, the
appropriate parts of both data structures can be removed.
")    (paper.topic       hayes75 "Historical")    (paper.instance    hentenryck89)    (paper.bibtex      hentenryck89 book)    (paper.author      hentenryck89 "Pascal Van Hentenryck")    (paper.title       hentenryck89 "Constraint Satisfaction in Logic Programming")    (paper.publisher   hentenryck89 "MIT Press")    (paper.year        hentenryck89 1989)    (paper.description hentenryck89 "
This is an extended form of Hentenryck's thesis.  It concerns the integration
of two standard CSP techniques (forward checking and arc consistency) into
Prolog.  First, there is a thorough introduction to Prolog--syntax,
semantics, proof theory.  Then he formally shows how to do the
integration.  Next he explains an implementation of the integration
that allows users the ability to specify when to apply the
new techniques.  
")    (paper.topic       hentenryck89 "Logic Programming")    (paper.instance    heymans2003)    (paper.bibtex      heymans2003 inproceedings)    (paper.author      heymans2003 "S. Heymans and D. Vermeir")    (paper.title       heymans2003 "Integrating Semantic Web Reasoning and Answer Set Programming")    (paper.publication heymans2003 "Proceedings of the 2nd International ASP Workshop")    (paper.link        heymans2003 "http://citeseer.ist.psu.edu/727609.html")    (paper.year        heymans2003 2003)    (paper.description heymans2003 "
Translating another description logic into logic programming, this time
using stable model semantics.
")    (paper.topic       heymans2003 "Semantic Web")    (paper.instance    hinrichs2004)    (paper.bibtex      hinrichs2004 article)    (paper.bibtex      hinrichs2004 proceedings)    (paper.author      hinrichs2004 "T. Hinrichs and N. Love and C. Petrie and L. Ramshaw and A. Sahai and S. Singhal.")    (paper.author      hinrichs2004 "Tim Hinrichs, Nathaniel Love, Charles Petrie, Lyle Ramshaw, Akhil Sahai and Sharad Singhal")    (paper.title       hinrichs2004 "Using Object-Oriented Constraint Satisfaction for Automated Configuration Generation")    (paper.title       hinrichs2004 "Using Object-Oriented Constraint Satisfaction for Automated Configuration Generation")    (paper.publication hinrichs2004 "DSOM")    (paper.publication hinrichs2004 "Utility Computing: 15th IFIP/IEEE International Workshop on Distributed Systems: Operations and Management, DSOM 2004")    (paper.link        hinrichs2004 "http://logic.stanford.edu/~thinrich/publications.htm")    (paper.link        hinrichs2004 "papers/hinrichs2004using.pdf")    (paper.year        hinrichs2004 2004)    (paper.year        hinrichs2004 2004)    (paper.rank        hinrichs2004 "")    (paper.description hinrichs2004 "
Hinrichs, et. al. describe an application of Object-Oriented Constraint 
Satisfaction to configuration management.
")    (paper.description hinrichs2004 "
In this paper, we describe an approach for automatically generating configurations for complex applications. Automated generation of system configurations is required to allow large-scale deployment of custom applications within utility computing environments. Our approach models the configuration management problem as an Object-Oriented Constraint Satisfaction Problem (OOCSP) that can be solved efficiently using a resolution-based theorem-prover. We outline the approach and discuss both the benefits of the approach as well as its limitations, and highlight certain unresolved issues that require further work. We demonstrate the viability of this approach using an e-Commerce site as an example, and provide results on the complexity and time required to solve for the configuration of such an application.
")    (paper.topic       hinrichs2004 "Object Oriented Constraint Satisfaction")    (paper.topic       hinrichs2004 "References")    (paper.instance    hinrichs2005axiom)    (paper.bibtex      hinrichs2005axiom inproceedings)    (paper.author      hinrichs2005axiom "Timothy Hinrichs and Michael Genesereth")    (paper.title       hinrichs2005axiom "Axiom Schemata as Metalevel Axioms: Model Theory")    (paper.publication hinrichs2005axiom "In proceedings of American Association for Aritificial Intelligence")    (paper.link        hinrichs2005axiom "http://logic.stanford.edu/~thinrich/papers/hinrichs2005axiom.pdf")    (paper.year        hinrichs2005axiom 2005)    (paper.description hinrichs2005axiom "
Logicians frequently use axiom schemata to encode (potentially infinite) sets of sentences with particular syntactic form. In this paper we examine a first-order language in which it is possible to write expressions that both describe sentences and assert the truth of the sentences so described. The effect of adding such expressions to a knowledge base is the same as directly including the set of described sentences. 
")    (paper.topic       hinrichs2005axiom "References")    (paper.instance    hinrichs2006herbrand)    (paper.bibtex      hinrichs2006herbrand techreport)    (paper.author      hinrichs2006herbrand "Timothy Hinrichs and Michael Genesereth")    (paper.title       hinrichs2006herbrand "Herbrand Logic")    (paper.publisher   hinrichs2006herbrand "Stanford University")    (paper.link        hinrichs2006herbrand "http://logic.stanford.edu/reports/LG-2006-02.pdf")    (paper.year        hinrichs2006herbrand 2006)    (paper.description hinrichs2006herbrand "
Herbrand logic has the same syntax as first-order logic but has Herbrand semantics. That is, the only models that exist in Herbrand logic are the Herbrand models. This logic is easier to learn than first-order logic and is often better suited for modeling and manipulating today's computer systems, the central concerns of computer science. In Herbrand logic, arithmetic using the natural numbers if finitely axiomatizable; however, neither entailment nor satisfiability are semi-decidable. Nevertheless, four of the most industrially successful applications of logic in computer science have been built within fragments of Herbrand logic: deductive databases, logic programming, constraint satisfaction, and formal verification. In this paper, we define Herbrand logic formally, prove several of its properties, discuss Goedel's incompleteness result with respect to Herbrand logic, and demonstrate how each of the four applications mentioned above can be formalized within Herbrand logic. 
")    (paper.topic       hinrichs2006herbrand "References")    (paper.instance    hinrichs2007extensional)    (paper.bibtex      hinrichs2007extensional inproceedings)    (paper.author      hinrichs2007extensional "Timothy Hinrichs and Michael Genesereth")    (paper.title       hinrichs2007extensional "Extensional Reasoning")    (paper.publication hinrichs2007extensional "In proceedings of CADE Workshop on Empirically Successful Automated Reasoning in Large Theories (ESARLT)")    (paper.link        hinrichs2007extensional "http://logic.stanford.edu/~thinrich/papers/hinrichs2007extensional.pdf")    (paper.year        hinrichs2007extensional 2007)    (paper.description hinrichs2007extensional "
Relational databases are one of the most industrially successful applications of logic in computer science, built for handling massive amounts of data. The power of the paradigm is clear both because of its widespread adoption and theoretical analysis. Today, automated theorem provers are not able to take advantage of database query engines and therefore do not routinely leverage that source of power. Extensional Reasoning is an approach to automated theorem proving where the machine automatically translates a logical entailment query into a database, a set of view definitions, and a database query such that the entailment query can be answered by answering the database query. This paper discusses the framework for Extensional Reasoning, describes algorithms that enable a theorem prover to leverage the power of the database in the case of axiomatically complete theories, and discusses theory resolution for handling incomplete theories. 
")    (paper.topic       hinrichs2007extensional "References")    (paper.instance    hinrichs2007reformulation)    (paper.bibtex      hinrichs2007reformulation inproceedings)    (paper.author      hinrichs2007reformulation "Timothy Hinrichs and Michael Genesereth")    (paper.title       hinrichs2007reformulation "Reformulation for Extensional Reasoning")    (paper.publication hinrichs2007reformulation "In proceedings of the Symposium of Abstraction, Reformulation, and Approximation (SARA)")    (paper.link        hinrichs2007reformulation "http://logic.stanford.edu/~thinrich/papers/hinrichs2007reformulation.pdf")    (paper.year        hinrichs2007reformulation 2007)    (paper.description hinrichs2007reformulation "
Relational databases have had great industrial success in computer science. The power of the paradigm is made clear both by its widespread adoption and by theoretical analysis. Today, automated theorem provers are not able to take advantage of database query engines and therefore do not routinely leverage that source of power. Extensional Reasoning (ER) is an approach to automated theorem proving where the machine automatically translates a logical entailment query into a database, a set of view definitions, and a database query such that the entailment query can be answered by answering the database query. The techniques developed for ER to date are applicable only when the logical theory is axiomatically complete. This paper discusses techniques for reformulating an incomplete theory into a complete theory so that Extensional Reasoning techniques can be applied. 

")    (paper.topic       hinrichs2007reformulation "References")    (paper.instance    hitzler2005)    (paper.bibtex      hitzler2005 inproceedings)    (paper.author      hitzler2005 "Pascal Hitzler and Peter Haase and Markus Krotzsch and York Sure and Rudi Studer")    (paper.title       hitzler2005 "DLP isn't so bad after all")    (paper.publication hitzler2005 "Proceedings of the WS OWL -- Experiences and Directions")    (paper.link        hitzler2005 "http://citeseer.ist.psu.edu/733286.html")    (paper.year        hitzler2005 2005)    (paper.description hitzler2005 "
The authors try to clarify the controversy surrounding Description Logic
Programs, which is a fragment of OWL DL that can be reasoned about effectively
with logic programming engines.  
")    (paper.topic       hitzler2005 "Semantic Web")    (paper.instance    hodgson2002)    (paper.bibtex      hodgson2002 article)    (paper.author      hodgson2002 "Kahlil Hodgson and John Slaney")    (paper.title       hodgson2002 "TPTP, CASC and the development of a semantically guided 
theorem prover")    (paper.publication hodgson2002 "AI Communications")    (paper.volume      hodgson2002 "15")    (paper.startpage   hodgson2002 135)    (paper.endpage     hodgson2002 146)    (paper.link        hodgson2002 "http://citeseer.ist.psu.edu/558093.html")    (paper.year        hodgson2002 2002)    (paper.rank        hodgson2002 "")    (paper.description hodgson2002 "
Hodgson and Slaney describe SCOTT 5, the fifth generation of
semantically constrained otter.  The authors give a quick
description of the differences between the five versions
and explain the algorithm for the newest.  Instead of
maintaining a single model that satisfies some of the
constraints, they maintain several models.  Moreover,
sometimes they use these models to implement the semantic
queue strategy.  Pick the largest set of clauses known
to be satisfiable and if the number of clauses complementary
to a clause in S in the passive list is small, into the
active list. If none of these so called co-NMCSs are small
enough, cycle through them and add clauses to the active
queue.  SCOTT keeps separate the model builder and the theorem
prover from semantic guidance routines to allow, in principle,
a plug and play architecture.
")    (paper.topic       hodgson2002 "System Designs")    (paper.instance    hohfeld88)    (paper.bibtex      hohfeld88 article)    (paper.author      hohfeld88 "M. Hohfeld and Gert Smolka")    (paper.title       hohfeld88 "Definite Relations over Constraint Languages")    (paper.link        hohfeld88 "http://citeseer.ist.psu.edu/hohfeld88definite.html")    (paper.year        hohfeld88 1988)    (paper.rank        hohfeld88 "[****]")    (paper.description hohfeld88 "
Hohfeld and Smolka generalize the previous formal foundations of Constraint
Logic Programming by allowing arbitrary constraints placed on definite
logic programming clauses.  Their definition for constraint states  that
it is simply a restriction on variables; predicate logic is simply a special
case.  They give a generalization of SLD resolution that is sound and 
complete for this broader definition of CLP.  The authors go on to give a 'semantic type discipline' for these CLP clause sets.
")    (paper.topic       hohfeld88 "Constraint Logic Programming")    (paper.instance    horiyama99)    (paper.bibtex      horiyama99 inproceedings)    (paper.author      horiyama99 "Takashi Horiyama and Toshihide ibaraki")    (paper.title       horiyama99 "Ordered Binary Decision Diagrams as Knowledge-Bases")    (paper.publication horiyama99 "International Symposium on Algorithms and Computation")    (paper.link        horiyama99 "http://citeseer.ist.psu.edu/horiyama99ordered.html")    (paper.year        horiyama99 1999)    (paper.rank        horiyama99 "[*]")    (paper.description horiyama99 "
Ordered binary decision diagrams can be used to represent propositional 
knowledge bases.  Deduction can be done from such a representation in 
polynomial time; not surprisingly, some knowledge bases require exponential
space while others require only polynomial space.  On the up-side, the
space requirements are not correlated with either characteristic models
(see kautz and khardon) or standard CNF representations.
")    (paper.topic       horiyama99 "Model-Based Reasoning")    (paper.related     horiyama99 kautz93)    (paper.related     horiyama99 khardon94)    (paper.instance    hower96)    (paper.bibtex      hower96 article)    (paper.author      hower96 "Walter Hower and Winfried Graf")    (paper.title       hower96 "A bibliographical survey of constraint-based approaches to CAD,
graphics, layout, visualization, and related topics")    (paper.publication hower96 "Knowledge-Based Systems
")    (paper.volume      hower96 "9(7)")    (paper.startpage   hower96 449)    (paper.endpage     hower96 464)    (paper.link        hower96 "http://citeseer.ist.psu.edu/hower96bibliographical.html")    (paper.year        hower96 1996)    (paper.rank        hower96 "[**]")    (paper.description hower96 "
Survey paper on just what the title indicates.  There are a few paragraphs
on object-oriented approaches most of which is orthogonal.
")    (paper.topic       hower96 "Miscellaneous")    (paper.instance    jackson98)    (paper.bibtex      jackson98 article)    (paper.author      jackson98 "Daniel Jackson and Somesh Jha and Craig Damon")    (paper.title       jackson98 "Isomorph-free model enumeration: a new method for checking relational specifications")    (paper.publication jackson98 "ACM Transactions on Programming Languages and Systems")    (paper.link        jackson98 "http://portal.acm.org/citation.cfm?id=276396&dl=ACM&coll=portal")    (paper.year        jackson98 1998)    (paper.description jackson98 "
Methods for building models while avoiding the enumeration of isomorphic candidates for the language Nitpick.
")    (paper.topic       jackson98 "Model Building")    (paper.instance    jaffar87)    (paper.bibtex      jaffar87 article)    (paper.author      jaffar87 "J. Jaffar and J.L. Lassez")    (paper.title       jaffar87 "Constraint Logic Programming")    (paper.publication jaffar87 "ACM Symposium on Principles of Programming Languages")    (paper.startpage   jaffar87 111)    (paper.endpage     jaffar87 119)    (paper.link        jaffar87 "http://portal.acm.org/citation.cfm?id=41635&dl=GUIDE&coll=GUIDE")    (paper.year        jaffar87 1987)    (paper.rank        jaffar87 "[****]")    (paper.description jaffar87 "
Jaffar and Lassez introduce constraint logic programming as a class
of logic programming languages, one for each domain of discourse.  
Every rule has associated with it a set of constraints 
over that domain.  Hohfeld and Smolka in '88 generalize this 
framework, but this is the seminal paper.  Solution to a CLP problem
is a set of constraints; thus, logic programming is a special form
of CLP where those constraints must be equality constraints.
")    (paper.topic       jaffar87 "Constraint Logic Programming")    (paper.instance    janicic2002)    (paper.bibtex      janicic2002 article)    (paper.author      janicic2002 "Predrag Janicic and Alan Bundy")    (paper.title       janicic2002 "A General Setting for Flexibly Combining and Augmenting Decision Procedures")    (paper.publication janicic2002 "Journal of Automated Reasoning
           ")    (paper.volume      janicic2002 "28(3)")    (paper.startpage   janicic2002 257)    (paper.endpage     janicic2002 305)    (paper.link        janicic2002 "http://www.inf.ed.ac.uk/publications/report/0095.html")    (paper.year        janicic2002 2002)    (paper.rank        janicic2002 "[***]")    (paper.description janicic2002 "
Janicic and Bundy invent a set of macro rewrite rules that can be used
to build a reasoner out of a set of decision procedures.  They provide
a good overview of the main influences in the field of combining
and augmenting decision procedures.  They build a prototype system
and report comparison results for Nelson-Oppen, Shostak, etc.
")    (paper.topic       janicic2002 "Architectures")    (paper.instance    jeavons99)    (paper.bibtex      jeavons99 article)    (paper.author      jeavons99 "Peter Jeavons and David Cohen and Marc Gyssens")    (paper.title       jeavons99 "How to Determine the Expressive Power of Constraints")    (paper.publication jeavons99 "Constraints")    (paper.volume      jeavons99 "4")    (paper.startpage   jeavons99 113)    (paper.endpage     jeavons99 131)    (paper.link        jeavons99 "http://citeseer.ist.psu.edu/jeavons98how.html")    (paper.year        jeavons99 1999)    (paper.description jeavons99 "
The authors define expresssive power as the set of relations definable
from another set of relations.  They show how to compute which algebraic 
operators are closed for a given set of relations by solving a particular
kind of CSP for the given relations.  Then they show that the set of
relations definable from a given set is equal to the set of relations
that are closed under the same operations as that given set.  If some set
R is closed under the set of operations O and every operation in O is 
what they call essentially unary, the problem is NP-complete.  Moreover,
if O includes an operation that is not essentially unary, it includes
an operation that has arity at most max(3, the size of the domain).
Thus, this paper details sufficient conditions for checking whether
a particular CSP is NP-complete: (1) compute the set of operations
that are closed over the permissible tables in the constraints of the CSP.
(2) If that set includes just essentially-unary operations (which can
be deduced by checking for operations of arity up to max(3, size of domain), 
we are assured the problem is NP-complete.
")    (paper.topic       jeavons99 "General")    (paper.instance    kautz91)    (paper.bibtex      kautz91 inproceedings)    (paper.author      kautz91 "Henry Kautz and Bart Selman")    (paper.title       kautz91 "A General Framework for Knowledge Compilation")    (paper.publication kautz91 "International Workshop on Processing Declarative Knowledge")    (paper.link        kautz91 "http://citeseer.ist.psu.edu/kautz91general.html")    (paper.year        kautz91 1991)    (paper.rank        kautz91 "[****]")    (paper.description kautz91 "
This follows up on selman91 and generalizes the Knowledge Compilation 
idea into a framework.  It says that it lifts the algorithms for
Least Upper Bound and Greatest Lower Bound Horn approximations to first-
order, but the proofs of computability are nonexistent.  
")    (paper.topic       kautz91 "Knowledge Base Compilation")    (paper.related     kautz91 selman91)    (paper.instance    kautz92)    (paper.bibtex      kautz92 inproceedings)    (paper.author      kautz92 "Henry Kautz and Bart Selman")    (paper.title       kautz92 "Forming Concepts for Fast Inference")    (paper.publication kautz92 "ECAI-Workshop on Knowledge Representation and Reasoning
      ")    (paper.startpage   kautz92 200)    (paper.endpage     kautz92 215)    (paper.link        kautz92 "http://citeseer.ist.psu.edu/kautz92forming.html")    (paper.year        kautz92 1992)    (paper.rank        kautz92 "[**]")    (paper.description kautz92 "
Kautz and Selman show the Least Upper Bound Horn approximation to a 
propositional knowledge base can be exponential in size.  In general,
there do exist knowledge bases (quoting) whose LUBs cannot be represented 
in a form that is both small and tractable.  This paper mainly
considers the utility of learning new concepts to reduce the size
of the LUB.  
")    (paper.topic       kautz92 "Knowledge Base Compilation")    (paper.related     kautz92 selman91)    (paper.instance    kautz93)    (paper.bibtex      kautz93 inproceedings)    (paper.author      kautz93 "Henry Kautz and Michael Kearns and Bart Selman")    (paper.title       kautz93 "Reasoning with Characteristic Models")    (paper.publication kautz93 "AAAI")    (paper.link        kautz93 "http://citeseer.ist.psu.edu/kautz93reasoning.html")    (paper.year        kautz93 1993)    (paper.rank        kautz93 "[*****]")    (paper.description kautz93 "
This is the seminal work on characteristic models.  The authors
define the characteristic models of a set M of models as those that cannot 
be derived by intersecting other models of M.  For propositional Horn
theories, these characteristic models are sufficient for computing 
entailment in time linear in the size of the characteristic models and
the conjecture.  In some cases the clausal representation requires 
exponentially more space than the characteristic representation, but
in others just the reverse is true.  Abduction using characteristic
models can be done in time polynomial in the size of the assumption set 
and the size of the characteristic set.  Very clean paper overall.
")    (paper.topic       kautz93 "Characteristic Models")    (paper.instance    kautz95)    (paper.bibtex      kautz95 article)    (paper.author      kautz95 "Henry Kautz and Michael Kearns and Bart Selman")    (paper.title       kautz95 "Horn Approximations of Empirical Data")    (paper.publication kautz95 "Artificial Intelligence")    (paper.volume      kautz95 "74(1)")    (paper.startpage   kautz95 129)    (paper.endpage     kautz95 145)    (paper.link        kautz95 "http://citeseer.ist.psu.edu/kautz95horn.html")    (paper.year        kautz95 1995)    (paper.rank        kautz95 "[****]")    (paper.description kautz95 "
This paper essentially rehashes that of kautz93, except it also looks
at the problem of converting a given set of models into either clausal
form or characteristic model form.  This idea of extracting structure
from 'empirical data' was first addressed in dechter92.
")    (paper.topic       kautz95 "Characteristic Models")    (paper.related     kautz95 kautz93)    (paper.related     kautz95 dechter92)    (paper.instance    kavvadias93)    (paper.bibtex      kavvadias93 inproceedings)    (paper.author      kavvadias93 "D. Kavvadias and C. Papadimitriou and M. Sideri")    (paper.title       kavvadias93 "On Horn Envelopes and Hypergraph Transversals")    (paper.publication kavvadias93 "ISAAC")    (paper.link        kavvadias93 "http://www.aueb.gr/Users/sideri/publicat.htm")    (paper.year        kavvadias93 1993)    (paper.rank        kavvadias93 "[**]")    (paper.description kavvadias93 "
This is the seminal work relating hypergraph transversals to
characteristic models.  Kavvadias et. al. look into the complexity
of computing the Horn envelope of a set of boolean models (the minimal
set of clauses that entail all the models) and the Horn core
(the maximal set of clauses that entail a subset of the models).
")    (paper.topic       kavvadias93 "Characteristic Models")    (paper.instance    khardon94)    (paper.bibtex      khardon94 inproceedings)    (paper.author      khardon94 "Roni Khardon and Dan Roth")    (paper.title       khardon94 "Reasoning with Models")    (paper.publication khardon94 "AAAI")    (paper.link        khardon94 "http://citeseer.ist.psu.edu/khardon96reasoning.html")    (paper.year        khardon94 1994)    (paper.rank        khardon94 "[****]")    (paper.description khardon94 "
This paper describes a method for determining entailment and abduction
through model-
checking for a KB of full propositional logic over a restricted set of 
queries.   It extends the work of Kautz, et. al. who wrote the seminal
work for propositional Horn KBs.  The definition for characteristic
models relies on Monotone Theory, which describes properties of 
Boolean functions.  It is important to note that these characteristic
models are defined with respect to a class of queries; that is, given
a class of queries, one can compute the characteristic models for
a propositional KB.  That set of models is then the optimal set, 
meaning all models are needed for correct entailment and abduction.
This paper does a thorough treatment of the issues it
brings up, but it is hard to penetrate.  Section 4 is a list of definitions
of Monotone theory, with little motivation or intuition as to the
utility of each definition.  Of course, the rest of the paper relies
on these definitions.  Reread! 
")    (paper.topic       khardon94 "Characteristic Models")    (paper.related     khardon94 kautz93)    (paper.instance    khardon94a)    (paper.bibtex      khardon94a inproceedings)    (paper.author      khardon94a "Roni Khardon and Dan Roth")    (paper.title       khardon94a "Exploiting Relevance through Model-Based Reasoning")    (paper.publication khardon94a "AAAI")    (paper.link        khardon94a "http://citeseer.ist.psu.edu/40071.html")    (paper.year        khardon94a 1994)    (paper.rank        khardon94a "[*****]")    (paper.description khardon94a "
Khardon and Roth outline three cases when the relevance can be exploited
to reason more efficiently.  The first deals with context.  Instead of using
all models for logical entailment, use just those that are relevant to 
the current context.  The second use of relevance is in using the Least
Upper Bound approximation of a theory to determine entailment when all 
queries will be answered correctly by the LUB.  Lastly, machine learning
is used to make reasoning easier as it gains experience in the world.
")    (paper.topic       khardon94a "Model-Based Reasoning")    (paper.instance    khardon95)    (paper.bibtex      khardon95 article)    (paper.author      khardon95 "Roni Khardon")    (paper.title       khardon95 "Translating between Horn Representations and their Characteristic Models")    (paper.publication khardon95 "Journal of Artificial Intelligence Research")    (paper.volume      khardon95 "3")    (paper.startpage   khardon95 349)    (paper.endpage     khardon95 372)    (paper.link        khardon95 "http://citeseer.ist.psu.edu/khardon95translating.html")    (paper.year        khardon95 1995)    (paper.rank        khardon95 "[***]")    (paper.description khardon95 "
Khardon builds on the previous work in kautz93 and khardon94 and investigates
the complexity of translating between propositional 
characteristic models and their
horn representations.  He shows that translating either way is polynomial-
reducible to the other, and that both are equivalent to deciding whether
a given set of models is the set of characteristic models of a given
set of horn clauses.  All these problems are at least as hard as converting
a monotone (no negations) CNF formula into a monotone DNF formula.  This
problem has a sub-exponential time solution of n^O(log n).
This paper also gives references to equivalent work in the database 
community under the name Armstrong relations.
")    (paper.topic       khardon95 "Characteristic Models")    (paper.related     khardon95 kautz93)    (paper.related     khardon95 khardon95)    (paper.instance    khardon95b)    (paper.bibtex      khardon95b article)    (paper.author      khardon95b "Roni Khardon and Heikki Mannila and Dan Roth")    (paper.title       khardon95b "Reasoning with Examples: Propositional Formulae and Database Dependencies")    (paper.publication khardon95b "Techical Report: Harvard University")    (paper.link        khardon95b "http://citeseer.ist.psu.edu/170480.html")    (paper.year        khardon95b 1995)    (paper.rank        khardon95b "[***]")    (paper.description khardon95b "
The authors show the connection between characteristic models and Armstrong
relations, i.e. relations that are sufficient for determining logical
entailment of functional dependencies.  They strengthen some of the
results on Armstrong relations and demonstrate a connection between
finding keys and abduction.
")    (paper.topic       khardon95b "Characteristic Models")    (paper.related     khardon95b kautz93)    (paper.related     khardon95b khardon95)    (paper.instance    khardon97)    (paper.bibtex      khardon97 article)    (paper.author      khardon97 "Roni Khardon and Dan Roth")    (paper.title       khardon97 "Defaults and Relevance in Model Based Reasoning")    (paper.publication khardon97 "Artificial Intelligence")    (paper.volume      khardon97 "97(1-2)")    (paper.startpage   khardon97 169)    (paper.endpage     khardon97 193)    (paper.link        khardon97 "http://citeseer.ist.psu.edu/khardon97defaults.html")    (paper.year        khardon97 1997)    (paper.rank        khardon97 "[****]")    (paper.description khardon97 "
This paper explains how model-based reasoning can be used to model reasoning
with context.  Suppose the KB is represented by a set of models M.  As the agent's
context changes, new sentences appear that define that context.  Entailment in
the context then means that the sentence holds in the subset of models M that
satisfy those new sentences.  By representing the theory as a set of models,
as context changes, that set can be reduced online.  In this paper, context
change is represented using default logic.  Algorithms are given for both
skeptical and credulous default reasoning where the knowledge base is represented
as a set of models.  Conditions are given under which these algorithms work
correctly.  Finally, the learning to reason paradigm is brought up as a third
argument for representing a KB as the set of models that satisfy it.  All three,
context, defaults, and learning to reason, sometimes produce computational
benefits because models are used to represent knowledge.
")    (paper.topic       khardon97 "Characteristic Models")    (paper.instance    khardon98)    (paper.bibtex      khardon98 article)    (paper.author      khardon98 "Roni Khardon and Heikki Manilla and Dan Roth")    (paper.title       khardon98 "Reasoning with Examples: Propositional Formulae and Database Dependencies")    (paper.publication khardon98 "Acta Informatica")    (paper.volume      khardon98 "36(4)")    (paper.startpage   khardon98 267)    (paper.endpage     khardon98 286)    (paper.link        khardon98 "http://citeseer.ist.psu.edu/170480.html")    (paper.year        khardon98 1998)    (paper.rank        khardon98 "[***]")    (paper.description khardon98 "
Reasoning with Examples means reasoning with enough models to correctly
determine entailment.  This technique shows up both in the database
literature under the name 'Armstrong Relations' and in the automated
reasoning literature under the name 'Characteristic Models'.  While
the latter refers to a set of examples, the former refers to a single
example.  The number of models needed in Characteristic Models for
propositional logic is bounded by |B|*|DNF(kb)|, where B is the 
basis for the knowledge base kb.  The bulk of this paper shows how the 
two concepts are closely intertwined; it goes on to show that finding 
the keys of a DB is akin to finding abductive explanations in a propositional
knowledge base. 
")    (paper.topic       khardon98 "Characteristic Models")    (paper.related     khardon98 fagin82)    (paper.related     khardon98 khardon95)    (paper.instance    kim87)    (paper.bibtex      kim87 article)    (paper.author      kim87 "Myung Won Kim")    (paper.title       kim87 "On Automatically Generating and Using Examples in a Computational Logic System")    (paper.publication kim87 "Technical Report")    (paper.link        kim87 "http://citeseer.ist.psu.edu/342499.html")    (paper.year        kim87 1987)    (paper.rank        kim87 "[*]")    (paper.description kim87 "
Kim's dissertation explores an algorithm for automatically generating 
examples that satisfy a particular constraint given other such examples.
He also explores how such an algorithm can be used to help prune
a theorem-proving search space.  The logic is the Boyer-Moore theory,
and the theorem prover is the Boyer Moore theorem prover.
")    (paper.topic       kim87 "Reasoning with Examples")    (paper.instance    kim94)    (paper.bibtex      kim94 inproceedings)    (paper.author      kim94 "Sun Kim and Hantao Zhang")    (paper.title       kim94 "ModGen: Theorem Proving by Model Generation")    (paper.publication kim94 "AAAI")    (paper.link        kim94 "http://citeseer.ist.psu.edu/kim94modgen.html")    (paper.year        kim94 1994)    (paper.rank        kim94 "[**]")    (paper.description kim94 "
The authors describe a system that propositionalizes Otter sentences and then
applies model-checking.  The limitation here is the requirement of a 
finite Herbrand universe.
")    (paper.topic       kim94 "Model-Based Reasoning")    (paper.instance    kleer84)    (paper.bibtex      kleer84 inproceedings)    (paper.author      kleer84 "Johan de Kleer")    (paper.title       kleer84 "Choices without Backtracking")    (paper.publication kleer84 "AAAI")    (paper.year        kleer84 1984)    (paper.rank        kleer84 "[**]")    (paper.description kleer84 "
Kleer describes a method for increasing the efficiency of problem solving
by storing the assumptions of a derived fact along with that fact.  Doing
this limits the amount of backtracking necessary.  This might be applicable
when backtracking with data structures.  Here it is situated within a truth
maintenance system.
")    (paper.topic       kleer84 "Theorem Proving with Attachments")    (paper.instance    kowalski69)    (paper.bibtex      kowalski69 inbook)    (paper.author      kowalski69 "Robert Kowalski")    (paper.title       kowalski69 "Search Strategies for Theorem-Proving")    (paper.publication kowalski69 "Machine Intelligence")    (paper.volume      kowalski69 "5")    (paper.startpage   kowalski69 181)    (paper.endpage     kowalski69 201)    (paper.year        kowalski69 1969)    (paper.rank        kowalski69 "[***]")    (paper.description kowalski69 "
Kowalski abstractly defines the theorem-proving problem in terms of an 
'abstract theorem-proving graph'.  He defines the search space by giving
a specification for each state, the operators that act on those 
states, and a termination condition.  In this case, the tp graph initially
consists of a set of nodes, one for each input axiom.  Further nodes 
(states) are reached (generated) by applying an inference rule to a set
of already reached (generated) nodes.   A search strategy is a mapping
from sets of nodes to other sets of nodes.  Kowalski gives sufficient
(abstract) conditions under which a search strategy is complete.  The
last few sections cover heuristics, their optimality and their 
admissibility.
")    (paper.topic       kowalski69 "Comparative Analysis")    (paper.instance    kowalski79)    (paper.author      kowalski79 "Robert Kowalski")    (paper.title       kowalski79 "Algorithm = Logic + Control")    (paper.publication kowalski79 "Communications of the ACM")    (paper.volume      kowalski79 "22(7)")    (paper.startpage   kowalski79 424)    (paper.endpage     kowalski79 436)    (paper.link        kowalski79 "http://portal.acm.org/citation.cfm?id=359136&dl=ACM&coll=portal&CFID=6287538&CFTOKEN=93143078")    (paper.year        kowalski79 1979)    (paper.rank        kowalski79 "[*****]")    (paper.description kowalski79 "
Kowalski defines an algorithm as Logic and Control.  Logic specifies what the
algorithm does or the knowledge used in
solving a problem.  Control determines the problem-solving strategies, namely
top-down, bottom -up, a combination of the two, and
orthogonally parallelization.  Ideally, the bulk of efficiency concerns
should fall under Control; however, we can improve the efficiency of an
algorithm by adjusting either the Logic or the Control.  
Quote: Computer programs
will be more often correct, more easily improved, and more readily adapted to
new problems when programming languages separate logic and control.
")    (paper.topic       kowalski79 "Logical Knowledge Representation")    (paper.instance    krotzsch2005)    (paper.bibtex      krotzsch2005 techreport)    (paper.author      krotzsch2005 "Markus Krotzsch and Pascal Hitzler and Michael Sintek and Denny Vrandecic")    (paper.title       krotzsch2005 "Expressive OWL Reasoning")    (paper.publisher   krotzsch2005 "University of Kalrsruhe")    (paper.link        krotzsch2005 "http://citeseer.ist.psu.edu/733293.html")    (paper.year        krotzsch2005 2005)    (paper.description krotzsch2005 "
The authors enlarge the class of formulas in OWL DL that can be reasoned
about with a logic programming engine; in addition they present
an alternative characterization of Description Logic Programming.
")    (paper.topic       krotzsch2005 "Semantic Web")    (paper.instance    lenat90)    (paper.bibtex      lenat90 book)    (paper.author      lenat90 "D.B. Lenat and R.V. Guha")    (paper.title       lenat90 "Building Large Knowledge-Based Systems: Representation and Inference in the CYC Project")    (paper.publisher   lenat90 "Addison-Wesley")    (paper.year        lenat90 1990)    (paper.description lenat90 "
Book on Cyc, the large common-sense knowledge base in Texas.
")    (paper.topic       lenat90 "Logical Knowledge Representation")    (paper.instance    lenz98)    (paper.bibtex      lenz98 book)    (paper.author      lenz98 "(ed.) Mario Lenz, et. al.")    (paper.title       lenz98 "Case-Based Reasoning Technology:From Foundations to Applications (Lecture Notes in Artificial Intelligence)")    (paper.link        lenz98 "http://www.amazon.com/exec/obidos/tg/detail/-/3540645721/104-6992918-7685512?vi=glance")    (paper.year        lenz98 1998)    (paper.rank        lenz98 "[***]")    (paper.description lenz98 "
Case-based reasoning analogically produces solutions to problems through
the use of a case library.  Chapter 1 tries to situate it in the field of
knowledge representation, explain the fundamental process, and describe
the problems to be solved.
")    (paper.topic       lenz98 "Case-Based Reasoning")    (paper.instance    lesperance95)    (paper.author      lesperance95 "Yves Lesperance, Hector J. Levesque, Fangzhen Lin, and Richard B.
Scherl")    (paper.title       lesperance95 "Ability and Knowing How in the Situation Calculus")    (paper.link        lesperance95 "http://citeseer.ist.psu.edu/lesperance95ability.html")    (paper.year        lesperance95 1995)    (paper.description lesperance95 "
Lesperance et. al. describe 1) a formalism for showing that a given goal can or cannot be accomplished
without simply producing a plan to accomplish it.  This involves discovering whether there is a path of
actions that will lead to the goal at the metalevel.  2) In the plan space that includes sensing actions, nondeterministic
branching, nondeterministic argument-passing, and nondeterministic primitive action choice, the authors formalize the notion that a dumb execution
engine cannot execute all the same plans as an intelligent execution engine.  Building on (1), they
define DumbKnowHow (DKH) and SmartKnowHow (SKH) predicates.  Cool ideas here.  There is also a nice summary
of a formalization of these sensing actions.
")    (paper.topic       lesperance95 "Fundamentals")    (paper.instance    leuschel2002)    (paper.bibtex      leuschel2002 article)    (paper.author      leuschel2002 "Michael Leuschel and Maurice Bruynooghe")    (paper.title       leuschel2002 "Logic Program Specialisation Through Partial Deduction: Control Issues")    (paper.publication leuschel2002 "Theory and Practice of Logic Programming")    (paper.volume      leuschel2002 "2(4-5)")    (paper.startpage   leuschel2002 461)    (paper.endpage     leuschel2002 515)    (paper.link        leuschel2002 "")    (paper.year        leuschel2002 2002)    (paper.description leuschel2002 "
The authors go through some of the work on partial evaluation in the case
of pure logic programming, which is called partial deduction.  (It differs
from partial evaluation in LP because axioms can be partially partially 
evaluated in the case of non-ground static arguments.)  Partial deduction
means constructing a partial SLDNF tree for a query, and using the fringe
to imply the goal as the output.  Interestingly, with negation, most work 
focuses on preserving operational semantics, as the well-founded semantics
are preserved for all groundings of the goal(s).  This paper focuses on
control issues surrounding the constructing of these SLDNF trees: (1)
given a goal atom A, how do we construct a tree, the local control
problem, and (2) what is the set of all goal atoms, the global control. 
The second problem only occurs because of the desire to preserve
procedural semantics.  The process of unfolding, i.e. constructing
the tree, is difficult because applying too many resolutions can 
result in local code explosion, work duplication, and non
termination.  If the unfold function is guaranteed to produce a tree 
with at most one non-failing
branch, it is said to be determinate; determine unfolding will not
produce code explosion or work duplication.  It is undecidable to determine
whether a tree with at most one non-failing branch can be constructed.
Approximations are usually used.   To deal with termination, binding-time
analysis is used (sometimes computed using abstract interpretation), which
creates annotations that have decided whether to unfold or not.  Instead of
handling termination offline, another approach handles it online--during
specialisation.  Well-founded and well-quasi orders are popular here.  
There is also a discussion of global control issues.  Interestingly,
tabling with partial deduction can turn a terminating program into
a nonterminating program.  Long reference list.
")    (paper.topic       leuschel2002 "Reformulation")    (paper.instance    levesque84)    (paper.bibtex      levesque84 chapter)    (paper.author      levesque84 "Hector J. Levesque")    (paper.title       levesque84 "The Logic of Incomplete Knowledge Bases")    (paper.publisher   levesque84 "Springer-Verlag")    (paper.publication levesque84 "On Conceptual Modelling")    (paper.startpage   levesque84 165)    (paper.endpage     levesque84 189)    (paper.link        levesque84 "")    (paper.year        levesque84 1984)    (paper.rank        levesque84 "[**]")    (paper.description levesque84 "
Levesque attacks the problem of querying an incomplete knowledge base about its
incompleteness.  How else does one know how accurately the KB represents the domain
it is modeling?  If a query is not entailed, without being able to ask whether the
query itself is known to the KB, we do not know if the query is false or whether
the KB simply doesn't know.  Levesque introduces a language, KL, based on the operator
K for dealing with this problem.  In the end, he claims the interaction of a KB
with a user should be done using this  language, but that any such query can be
translated into straight-up FOL without K.
")    (paper.topic       levesque84 "Logical Knowledge Representation")    (paper.instance    levesque85)    (paper.bibtex      levesque85 inbook)    (paper.author      levesque85 "Hector J. Levesque and Ronald J. Brachman")    (paper.title       levesque85 "A Fundamental Tradeoff in Knowledge Representation and Reasoning")    (paper.publisher   levesque85 "Morgan Kaufmann Publishers")    (paper.publication levesque85 "Readings in Knowledge Representation and Reasoning
           ")    (paper.link        levesque85 "http://citeseer.ist.psu.edu/context/54197/0")    (paper.year        levesque85 1985)    (paper.rank        levesque85 "[***]")    (paper.description levesque85 "
Levesque and Brachman argue that a perfect representation language does not
exist.  Rather, languages vary in expressiveness and tractability, and some
are only more interesting than others for certain uses.  The authors examine
first-order logic, databases, semantic nets, and frame systems as examples.
")    (paper.topic       levesque85 "Logical Knowledge Representation")    (paper.instance    levesque86)    (paper.author      levesque86 "Hector J. Levesque")    (paper.title       levesque86 "Making Believers Out of Computers")    (paper.publication levesque86 "Artificial Intelligence")    (paper.volume      levesque86 "30(1)")    (paper.startpage   levesque86 81)    (paper.endpage     levesque86 108)    (paper.year        levesque86 1986)    (paper.rank        levesque86 "[***]")    (paper.description levesque86 "
Levesque argues that the combinatorial explosion of exponential-time
algorithms cannot be tolerated for AI systems, except for a 'puzzle-mode'.  
To that end, he advises building knowledge bases using languages that
only allow complete ('vivid') information and using 
reasoning methods that may at times be unsound and/or incomplete.  His
examples invoke the reader's imagination to invent a pictures that model
the example descriptions.  Such descriptions can then be considered vivid.
")    (paper.topic       levesque86 "Logical Knowledge Representation")    (paper.instance    levesque96)    (paper.author      levesque96 "Hector J. Levesque")    (paper.title       levesque96 "What is Planning in the presence of sensing?")    (paper.link        levesque96 "http://www.cs.toronto.edu/cogrobo/Papers/sensing.pdf")    (paper.year        levesque96 1996)    (paper.description levesque96 "

Intro: Classical planning doesn't deal with the problem of sensing or looping, e.g. it cannot produce plans that will successfully achieve the Airport or the Omlette examples.  Sensing is necessary iff there are properties of the world that cannot be deduced at plan time but that are required for achieving the goal.

<br><br>Classical Planning: Use sit calc to formally define the classical planning problem.  It is unclear to me whether the sit calc he is using would somehow allow there to be partial order plans, e.g. return something like (do (parallel a b c) (do (parallel e f) (do h s0))).  For now let's say it can't and sitcalc plans must be total orders.  We might argue that because his definition is tied to sitcalc, which produces only totally ordered plans, his 'definition' of the classical planning problem is wrong.  But it doesn't really matter for the rest of what he is going to say.

Incorporating sensing actions: gives an overview of another one of his papers which introduces sensing actions into sitcalc, and reviews some related work.

<br><br>Robot programs: introduces a language for describing plans that is more expressive than the standard sitcalc (do a (do b (do .... s0)))) and partial order languages.  Includes loops and branches.  He defines the semantics of the language in situation calculus, relying on a new second order construct for defining the semantics of a loop.  Finally, he states the first theorem, which amounts to a set of rewrite rules for each of the new constructs in the robot language, where the rewrite rules allow one to construct an interpreter for the language.  That is, the obvious interpreter for the language does the right thing, and the theorem proves that.

The revised planning task: formalize, using the machinery built up above, the planning problem as it includes loops, branches, and sensing.  Remember this is all sitcalc in first-order logic + a couple second-order constructs.  The rest of the section works through the Airport example.

<br><br>Note: Importantly, all the examples thusfar, as well as the definition of the planning task requires a _deductive_ solution.  That is, the airport example includes the axiom AtGate(a) | AtGate(b).  At plan-time we can prove that the plan (seq goto-airport (if check-gate(a) then goto-gate-a else goto-gate-b)) will work.  This is unlike our room reservation example since there may be no room to reserve, but we won't know that until plan execution time.  At plan-time, there is no plan we can prove will work.

<br><br>Are robot programs enough?: They claim the original plan language is not sufficient for expressing plans for every solvable planning problem.  The reason is that there is no memory available for the program to use.  Asking for a plan that returns the number of eggs it is given at runtime cannot be achieved without (infinite) memory.  Levesque adds 5 actions (left, right, mark, erase, read_mark) that can be used to expand the plans expressible by his robot language to include (he claims) everything that is computable.  That is, his planning language with these additional actions would force a planner to construct a Turing Machine that achieves the given goal.  Of course, that problem is undecidable.  

<br><br>A two sentence summary.  Levesque introduces a plan description language that includes branching and loops (with an extension that he claims makes the language Turing complete), for the simple reason that solving some real world planning problems requires sensing and looping.  He formalizes the planning problem as one of deduction, which has the benefit that deductive solutions will always be right but has the drawback that it eliminates plans that we might expect a machine to produce, e.g. room reservation.


")    (paper.topic       levesque96 "Planning with Sensing Actions")    (paper.instance    levesque97)    (paper.author      levesque97 "Levesque, et al.")    (paper.title       levesque97 "Golog: A Logic Programming Language for Dynamic Domains")    (paper.link        levesque97 "http://citeseer.ist.psu.edu/27198.html")    (paper.year        levesque97 1997)    (paper.description levesque97 "
Golog does macro-replacement into situation calculus from nondeterministic parameter choice, nondeterministic action choice, nondeterministic iteration, test actions, and procedures.  This macro-replacement means answer extraction cannot be used to extract a complex plan using the above constructs.  Instead, given a complex plan, the Golog interpreter produces an equivalent situation calculus plan(I think--still fuzzy here).
")    (paper.topic       levesque97 "Logic Programming")    (paper.instance    levesque98)    (paper.author      levesque98 "Hector Levesque, Fiora Pirri, and Ray Reiter")    (paper.title       levesque98 "Foundations for the Situation Calculus")    (paper.publication levesque98 "Linkoping Electronic Articles in Computer and Information Science")    (paper.volume      levesque98 "3")    (paper.link        levesque98 "http://citeseer.ist.psu.edu/levesque98foundations.html")    (paper.year        levesque98 1998)    (paper.rank        levesque98 "[***]")    (paper.description levesque98 "
This is a rehash of Pirri99, except they add in a
section describing sensing and knowledge in Situation Calculus.  They extend
their definition of a basic action theory to include knowledge-producing
actions.  Namely, a knowledge-action theory D_e = Sigma_e U D_ss U D_ap U D_una
U D_S0 U D_sf U D_k0.  Sigma_e is an expanded set of foundational axioms; D_ss
are the successor state axioms; D_ap are the action precondition axioms;
D_una are the unique name axioms for actions; D_s0 is the initial situation;
D_k0 is an initial knowldge axiom (what the agent knows in S0).  This stuff
gets pretty messy, but eventually they provide a solution to the knowledge
frame problem, i.e. ")    (paper.topic       levesque98 "Fundamentals")    (paper.instance    lifschitz2002)    (paper.bibtex      lifschitz2002 article)    (paper.author      lifschitz2002 "V. Lifschitz")    (paper.title       lifschitz2002 "Answer set programming and plan generation")    (paper.publication lifschitz2002 "Artificial Intelligence")    (paper.volume      lifschitz2002 "138(1-2)")    (paper.startpage   lifschitz2002 39)    (paper.endpage     lifschitz2002 54)    (paper.link        lifschitz2002 "http://citeseer.ist.psu.edu/301403.html")    (paper.year        lifschitz2002 2002)    (paper.description lifschitz2002 "
Answer set programming paper by Lifschitz.
")    (paper.topic       lifschitz2002 "Logic Programming")    (paper.instance    lifschitz85)    (paper.bibtex      lifschitz85 article)    (paper.author      lifschitz85 "Vladimir Lifschitz")    (paper.title       lifschitz85 "Closed World Databases and Circumscription")    (paper.publication lifschitz85 "Artificial Intelligence")    (paper.volume      lifschitz85 "27(2)")    (paper.startpage   lifschitz85 229)    (paper.endpage     lifschitz85 235)    (paper.year        lifschitz85 1985)    (paper.description lifschitz85 "
Lifschitz proves the following theorem.  If CWA is consistent,
then CWA and Circumscription produce 
equivalent theories when applied to universal, closed sentences in a
function-free, equality-free vocabulary with finitely many relation and
object constants whenever the domain closure assumption and unique
names assumption are in play.
")    (paper.topic       lifschitz85 "Non-monotonic Reasoning")    (paper.instance    lifschitz87)    (paper.author      lifschitz87 "V. Lifschitz")    (paper.title       lifschitz87 "On the semantics of STRIPS")    (paper.publication lifschitz87 "Reasoning About Actions and Plans")    (paper.startpage   lifschitz87 1)    (paper.endpage     lifschitz87 9)    (paper.link        lifschitz87 "http://www.cs.utexas.edu/users/vl/papers-old.html")    (paper.year        lifschitz87 1987)    (paper.description lifschitz87 "
Lifschitz gives semantics to Fike's and Nilsson's STRIPS.  He gives one definition of a STRIPS system's soundness that relies on atomic sentences for both the add and delete lists (effectively).  All non-atomic sentences must be true in all worlds.  The preconditions can still be full FOL.  Lifschitz goes on to explain how to deal with peculiarities to improve efficiency, e.g. only including nextTo(a,b) when nextTo(x,y) is symmetric.
")    (paper.topic       lifschitz87 "Historical")    (paper.instance    lifschitz96)    (paper.bibtex      lifschitz96 chapter)    (paper.author      lifschitz96 "Vladimir Lifschitz")    (paper.title       lifschitz96 "Foundations of Logic Programming")    (paper.publication lifschitz96 "Principles of Knowledge Representation")    (paper.startpage   lifschitz96 69)    (paper.endpage     lifschitz96 127)    (paper.link        lifschitz96 "http://citeseer.ist.psu.edu/lifschitz96foundations.html")    (paper.year        lifschitz96 1996)    (paper.description lifschitz96 "
Survey of logic programming theory.
")    (paper.topic       lifschitz96 "Logic Programming")    (paper.instance    lin94)    (paper.author      lin94 "Fangzhen Lin and Ray Reiter")    (paper.title       lin94 "How to Progress a Database (and Why) I. Logical Foundations")    (paper.publication lin94 "Principles of Knowledge Representation and Reasoning ")    (paper.startpage   lin94 425)    (paper.endpage     lin94 436)    (paper.link        lin94 "http://citeseer.ist.psu.edu/11152.html")    (paper.year        lin94 1994)    (paper.rank        lin94 "[****]")    (paper.description lin94 "
Lin and Reiter argue that STRIPS (planning) can be thought of as progressing an initial database S0 with an action sequence A to a final database SA.  They prove the general problem cannot be solved using only FOL, but in fact properly needs induction.  The special cases of 'relatively complete' initial databases and 'context-free' action theories (with certain restrictions on the initial database) yield first-order theories.
")    (paper.topic       lin94 "Theories of Action")    (paper.instance    lin95a)    (paper.author      lin95a "Fangzhen Lin and Ray Reiter")    (paper.title       lin95a "How to Progress a Database II: The STRIPS Connection")    (paper.publication lin95a "IJCAI")    (paper.startpage   lin95a 2001)    (paper.endpage     lin95a 2009)    (paper.link        lin95a "http://citeseer.ist.psu.edu/lin94how.html")    (paper.year        lin95a 1995)    (paper.rank        lin95a "[*]")    (paper.description lin95a "
Lin and Reiter build on the first part of this paper and give certain varieties of STRIPS semantics based on progressing a database.  
")    (paper.topic       lin95a "Theories of Action")    (paper.related     lin95a lin94)    (paper.instance    lin95b)    (paper.author      lin95b "Fangzhen Lin and Yoav Shoham")    (paper.title       lin95b "Provably Correct Theories of Action")    (paper.publication lin95b "Journal of the ACM")    (paper.volume      lin95b "42(2)")    (paper.startpage   lin95b 293)    (paper.endpage     lin95b 320)    (paper.link        lin95b "http://citeseer.ist.psu.edu/lin91provably.html")    (paper.year        lin95b 1995)    (paper.rank        lin95b "[****]")    (paper.description lin95b "
Lin and Shoham give a formal definition for epistemologically complete theories of action.  They explain monotonic and nonmonotonic completions of action theories; the former requires frame axioms and the latter circumscription.   They go on to extend situation calculus to handle concurrent actions by bundling primitive actions together into 'global actions'.  They extend their definition for epistemological completeness to encompass this variation of sitcalc.
")    (paper.topic       lin95b "Theories of Action")    (paper.instance    lloyd)    (paper.bibtex      lloyd book)    (paper.author      lloyd "John Lloyd")    (paper.title       lloyd "Foundations of Logic Programming")    (paper.publisher   lloyd "Springer Verlag")    (paper.link        lloyd "http://www.amazon.com/Foundations-Logic-Programming-Symbolic-Computation/dp/0387181997")    (paper.year        lloyd 1984)    (paper.rank        lloyd "[*****]")    (paper.description lloyd "
Lloyd covers the declarative and procedural semantics of logic programming without
negation and then considers the case of an acyclic logic program with negation,
and proves soundness and completeness via predicate completion.  The classic text.
")    (paper.topic       lloyd "")    (paper.instance    lloyd84)    (paper.bibtex      lloyd84 article)    (paper.author      lloyd84 "J. Lloyd and R. Topor")    (paper.title       lloyd84 "Making Prolog more expressive")    (paper.publication lloyd84 "Journal of Logic Programming")    (paper.volume      lloyd84 "1(3)")    (paper.startpage   lloyd84 225)    (paper.endpage     lloyd84 240)    (paper.year        lloyd84 1984)    (paper.description lloyd84 "
Includes the Lloyd-Topor transformation.
")    (paper.topic       lloyd84 "Logic Programming")    (paper.instance    lobo)    (paper.bibtex      lobo book)    (paper.author      lobo "Jorge Lobo and Jack Minker and Arcot Rajasekar")    (paper.title       lobo "Foundations of Disjunctive Logic Programming")    (paper.publisher   lobo "The MIT Press")    (paper.link        lobo "http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=5834")    (paper.year        lobo 1992)    (paper.description lobo "
Book laying the foundations of disjunctive logic programming.
")    (paper.topic       lobo "")    (paper.instance    lochner02)    (paper.bibtex      lochner02 article)    (paper.author      lochner02 "Bernd Loechner and Thomas Hillenbrand")    (paper.title       lochner02 "A phytography of Waldmeister")    (paper.publication lochner02 "AI Communications")    (paper.volume      lochner02 "15")    (paper.startpage   lochner02 127)    (paper.endpage     lochner02 133)    (paper.link        lochner02 "http://citeseer.ist.psu.edu/558093.html")    (paper.year        lochner02 2002)    (paper.rank        lochner02 "")    (paper.description lochner02 "
Waldmeister is a unit equation theorem prover based on
unfailing completion.  Its architecture is the same
Discount algorithm found in Vampire and E.  Written in C,
it uses perfect discrimination trees (a variation on the trie)
to index terms.  It also includes automated tuning of 
its parameters, based on TPTP experience.
")    (paper.topic       lochner02 "System Designs")    (paper.instance    loechner2004)    (paper.author      loechner2004 "Bernd Loechner")    (paper.title       loechner2004 "Things to know when implementing LPO")    (paper.publication loechner2004 "Workshop on Empirically Successful First Ordering Reasoning")    (paper.link        loechner2004 "")    (paper.year        loechner2004 2004)    (paper.description loechner2004 "
Loechner explains that the naive implementation of Lexicographic Path
Ordering is exponential but then goes on to show how to make it
multiplicative in the size of the two terms.  
")    (paper.topic       loechner2004 "Orderings")    (paper.instance    lopezgarcia2004)    (paper.bibtex      lopezgarcia2004 article)    (paper.author      lopezgarcia2004 "P. Lopez-Garcia and F. Bueno and M. Hermenegildo")    (paper.title       lopezgarcia2004 "Determinacy Analysis for Logic Programs Using Mode and Type Information")    (paper.publication lopezgarcia2004 "14th International Symposium, Logic Based Program Synthesis and Trnasformation (LOPSTR)")    (paper.startpage   lopezgarcia2004 19)    (paper.endpage     lopezgarcia2004 35)    (paper.link        lopezgarcia2004 "http://wotan.liu.edu/docis/dbl/lopstr/index.html")    (paper.year        lopezgarcia2004 2004)    (paper.description lopezgarcia2004 "
In the context of Logic programming, the authors consider two problems:
(1) detecting whether a relation will ever return more than one answer
through all control paths, i.e. whether the relation is determinate,
 and (2) determining whether more than one
rule with for a relation will ever be satisfied for
the same query, i.e. whether the rules defining that relation
are mutually exclusive.  Addressing these questions relies on type
information, i.e. all the possible queries asked for a relation, and
mode information, i.e. whether a variable is used for validation or
for generation.  For r to be determinate, it is sufficient
that every relation that is a subgoal of r be mutually exclusive.
Thus an approximation of (1) can be accomplished by computing (2).  The test
for mutual exclusion offered here will return true only when all the 
rules for a relation contain procedural attachments that can be shown
to be mutually exclusive for all the types of the relation.  Experimental
results are shown for the case where the procedural attachments are
equality/distinction/inequality.
")    (paper.topic       lopezgarcia2004 "Reformulation")    (paper.instance    manna86)    (paper.author      manna86 "Zohar Manna")    (paper.title       manna86 "How to Clear a Block: A Theory of Plans")    (paper.link        manna86 "http://library.stanford.edu/depts/mathcs/mathcscoll/techreports.html")    (paper.year        manna86 1986)    (paper.description manna86 "
This is the recursive plan paper everyone cites; not surprisingly, it shows up in the context of
program synthesis.  Manna uses Deductive Tableaux to produce programs that contain conditionals
(via a special inference rule) and recursion (using a well-founded mathematical
induction axiom).  The problem with Manna's recursion lies in the need for what he calls
Generalization, i.e. automatically strengthening the stated goal.  Manna also deals
with equality by building it into the unification algorithm.
")    (paper.topic       manna86 "Recursive Plan Generation")    (paper.instance    manna94)    (paper.bibtex      manna94 inbook)    (paper.author      manna94 "Zohar Manna et. al.")    (paper.title       manna94 "STeP: the Stanford Temporal Prover")    (paper.publication manna94 "TAPSOFT")    (paper.link        manna94 "http://citeseer.ist.psu.edu/manna94step.html")    (paper.year        manna94 1994)    (paper.rank        manna94 "[*]")    (paper.description manna94 "
STeP is a verification system that proves first order temporal 
formulas valid in concurrent, reactive systems described in SPL.  
It combines equational and arithmetic simplification, model 
checking, interactive first-order theorem proving, and automatic
invariant generation.  The model checking is done through 
Streett automaton and behavior automaton.  The first-order
theorem proving is automated with non-clausal resolution
and paramodulation and utilizes lemmas.  The interactive
theorem proving is handled with a Gentzen-style prover:
both first-order and temporal first-order.  Propositional
temporal formulas can be automatically verified.
This paper is well-written and clear, but assumes knowledge
of temporal logic and transition systems.
")    (paper.topic       manna94 "Model Checking")    (paper.instance    manthey88)    (paper.bibtex      manthey88 inproceedings)    (paper.author      manthey88 "Rainer Manthey and Francois Bry")    (paper.title       manthey88 "SATCHMO: A Theorem Prover Implemented in Prolog")    (paper.publication manthey88 "Proceedings of the Conference on Automated Deduction")    (paper.startpage   manthey88 415)    (paper.endpage     manthey88 434)    (paper.year        manthey88 1988)    (paper.description manthey88 "
Tableaux-style prover implemented in Prolog, using assert and retract.
")    (paper.topic       manthey88 "Calculi")    (paper.instance    marrafa91)    (paper.bibtex      marrafa91 article)    (paper.author      marrafa91 "Palmira Marrafa and Patrick Saint-Dizier")    (paper.title       marrafa91 "Reversibility in a Constraint and Type based Logic Grammar")    (paper.link        marrafa91 "http://acl.ldc.upenn.edu/W/W91/W91-0102.pdf")    (paper.year        marrafa91 1991)    (paper.rank        marrafa91 "[***]")    (paper.description marrafa91 "
In the topic of reversible grammars.
Marrafa and Saint-Dizier introduce a mathematical language for 
describing natural languages.  The mathematical language includes certain
object-oriented concepts. In addition, abstract machines for parsing and
generating languages so described are given.
")    (paper.topic       marrafa91 "Natural Language Processing")    (paper.instance    masters2003)    (paper.bibtex      masters2003 article)    (paper.author      masters2003 "James Masters and Zelai Gungordu")    (paper.title       masters2003 "Semantic Knowledge Source Integration: A Progress Report")    (paper.publication masters2003 "Integration of Knowledge Intensive Multi-Agent Systems")    (paper.year        masters2003 2003)    (paper.rank        masters2003 "")    (paper.description masters2003 "
Paper on using a database to speed up theorem proving.  Here they want togive a TP access to the information contained in several databases.
")    (paper.topic       masters2003 "Theorem Proving with Attachments")    (paper.instance    matzinger97)    (paper.bibtex      matzinger97 techreport)    (paper.author      matzinger97 "Robert Matzinger")    (paper.title       matzinger97 "Comparing Computational Representations of Herbrand Models")    (paper.link        matzinger97 "http://www.kr.tuwien.ac.at/guests/matzi/publications.html")    (paper.year        matzinger97 1997)    (paper.rank        matzinger97 "[*****]")    (paper.description matzinger97 "
Matzinger investigates various representations of models and
answers three complexity questions for each representation:
ground atom entailment, clause entailment, model equivalence.
The two representations of finite models examined are
context-free grammers and atom representations.  Both CFGs
and ground atoms plus ground equations are equivalent
to finite models, i.e. both representations have an associated
finite model and every finite model has a submodel that
is equivalent to both representations.
Different subsets of all infinite models can be represented by 
term schematizations and constrained atoms.  They seem to be
incomparable.  This paper is pretty dense, but also very important.
")    (paper.topic       matzinger97 "Model-Based Reasoning")    (paper.instance    mccarthy59)    (paper.author      mccarthy59 "John McCarthy")    (paper.title       mccarthy59 "Programs with Common Sense")    (paper.publication mccarthy59 "Proceedings of the Teddington Conference on the
                Mechanization of Thought Processes")    (paper.startpage   mccarthy59 77)    (paper.endpage     mccarthy59 81)    (paper.link        mccarthy59 "http://www-formal.stanford.edu/jmc/mcc59.html")    (paper.year        mccarthy59 1958)    (paper.rank        mccarthy59 "[*****]")    (paper.description mccarthy59 "
Most likely the first paper on logical AI, i.e. the advice taker paper.
It describes a system(the advice taker) that can be told sentences in logic
and can draw conclusions about the world from its knowledge base.
")    (paper.topic       mccarthy59 "Logical Knowledge Representation")    (paper.instance    mccarthy80)    (paper.bibtex      mccarthy80 article)    (paper.author      mccarthy80 "John McCarthy")    (paper.title       mccarthy80 "Circumscription -- a form of non-monotonic reasoning")    (paper.publication mccarthy80 "Artificial Intelligence")    (paper.volume      mccarthy80 "13")    (paper.startpage   mccarthy80 27)    (paper.endpage     mccarthy80 39)    (paper.year        mccarthy80 1988)    (paper.description mccarthy80 "
Circumscription.
")    (paper.topic       mccarthy80 "Non-monotonic Reasoning")    (paper.instance    mccarthy82)    (paper.bibtex      mccarthy82 techreport)    (paper.author      mccarthy82 "John McCarthy")    (paper.title       mccarthy82 "Coloring Maps and the Kowalski Doctrine")    (paper.publisher   mccarthy82 "Stanford University")    (paper.year        mccarthy82 1982)    (paper.description mccarthy82 "
Paper that analyzes various formulations of the map coloring problem
written in Prolog.  Looks at various reorderings of subgoals.
")    (paper.topic       mccarthy82 "Logic Programming")    (paper.instance    mcchay69)    (paper.author      mcchay69 "John McCarthy and Patrick J. Hayes")    (paper.title       mcchay69 "Some Philosophical Problems from the Standpoint of Artificial Intelligence")    (paper.publication mcchay69 "Machine Intelligence 4")    (paper.startpage   mcchay69 463)    (paper.endpage     mcchay69 502)    (paper.link        mcchay69 "http://www-formal.stanford.edu/jmc/mcchay69.html")    (paper.year        mcchay69 1969)    (paper.description mcchay69 "
The basic situation calculus paper.  McCarthy and Hayes consider not only the classic planning environment (fully-observable), but also a partially-observable one, introducing what are now referred to as 'sensing actions'.  They attempt a self-proclaimed ad-hoc approach to handling these sensing actions.  They also point out the frame problem, alluding to a state vector as a possible solution.  The qualification problem (though they don't name it that) shows up here, along with the introduction of the modal operators consistent(phi), probably(phi), and normally(phi) as a tentative solution avenue.  Finally, McCarthy and Hayes look into embedding Modal Logic, Logic of Knowledge (Hintikka), Tense Logics, and a couple of others into situation calculus.
")    (paper.topic       mcchay69 "Historical")    (paper.instance    mccune2001)    (paper.bibtex      mccune2001 misc)    (paper.author      mccune2001 "William McCune")    (paper.title       mccune2001 "Mace 2.0 Reference Manual and Guide")    (paper.link        mccune2001 "http://citeseer.ist.psu.edu/464179.html")    (paper.year        mccune2001 2001)    (paper.description mccune2001 "
Manual for Mace 2.0, a finite model builder that grounds out formulas
and relies on a SAT solver.
")    (paper.topic       mccune2001 "Model Building")    (paper.instance    mccune2003)    (paper.bibtex      mccune2003 techreport)    (paper.author      mccune2003 "William McCune")    (paper.title       mccune2003 "Mace4 Reference Manual and Guide")    (paper.publisher   mccune2003 "Argonne National Laboratory")    (paper.link        mccune2003 "http://www-unix.mcs.anl.gov/AR/mace4/July-2005/doc/mace4.pdf")    (paper.year        mccune2003 2003)    (paper.description mccune2003 "
The reference manual for Mace4.  Unlike its predecessors, which flattened
formulas, ground them, and used a SAT solver, Mace4 incrementally constructs
a model and after each instantiation computes the consequences of the
instantiation wrt the sentences of interest.
")    (paper.topic       mccune2003 "Model Building")    (paper.instance    mcdonald91)    (paper.bibtex      mcdonald91 article)    (paper.author      mcdonald91 "D. McDonald")    (paper.title       mcdonald91 "Reversible NLP by Deriving the Grammars from the Knowledge Base")    (paper.link        mcdonald91 "http://acl.ldc.upenn.edu/W/W91/W91-0106.pdf")    (paper.year        mcdonald91 1991)    (paper.rank        mcdonald91 "[***]")    (paper.description mcdonald91 "
Reversible grammars.
McDonald presents an architecture for constructing a parsing grammar and
an acceptance grammar from a single knowledge base and semantic model.  Thus
the strongest techniques for both parsing and generation can be used without
losing the simplicity of maintenance derived from  using a single, 
reversible grammar.
")    (paper.topic       mcdonald91 "Natural Language Processing")    (paper.instance    mcilraith98)    (paper.bibtex      mcilraith98 article)    (paper.author      mcilraith98 "Sheila McIlraith")    (paper.title       mcilraith98 "Logic-based Abductive Inference")    (paper.publication mcilraith98 "Knowledge Systems Lab Technical Report KSL98-19")    (paper.link        mcilraith98 "http://citeseer.ist.psu.edu/mcilraith98logicbased.html")    (paper.year        mcilraith98 1998)    (paper.description mcilraith98 "
McIlraith covers various ways to operationalize abduction, i.e. finding
an augmentation to a logical knowledge base such that a given query
is entailed. 
")    (paper.topic       mcilraith98 "Abduction")    (paper.instance    mckenzie97)    (paper.bibtex      mckenzie97 techreport)    (paper.author      mckenzie97 "Bruce McKenzie")    (paper.title       mckenzie97 "Generating Strings at Random from a Context Free Grammar")    (paper.link        mckenzie97 "http://coscweb2.cosc.canterbury.ac.nz/research/reports/TechReps/#year1997")    (paper.year        mckenzie97 1997)    (paper.rank        mckenzie97 "[***]")    (paper.description mckenzie97 "
McKenzie gives an algorithm for generating strings of length n from a CFG 
so that all strings of length n are equally likely.  The preprocessing
portion of the algorithm requires O(n^2) time and O(n^2) space.  The
generation portion requires O(n) time and space.
")    (paper.topic       mckenzie97 "Context Free Grammars")    (paper.instance    meng20067)    (paper.author      meng20067 "Jia Meng and L.C. Paulson")    (paper.title       meng20067 "Lightweight Relevance Filtering for Machine-Generated Resolution Problems")    (paper.publication meng20067 "ESCoR: Empirically Successful Computerized Reasoning")    (paper.link        meng20067 "http://www.cl.cam.ac.uk/~lp15/papers/frameset.html")    (paper.year        meng20067 2006)    (paper.description meng20067 "
The authors argue for the case that large axiom sets are the norm when
an entailment query is constructed by a machine; moreover, it is often the
case that only a small fraction of those axioms are relevant to the query
at hand, and thus determining which clauses are relevant is a good first
step toward solving the problem.  The criteria for relevance they use
is based on the presence of function symbols in the clauses.
")    (paper.topic       meng20067 "Transformations")    (paper.instance    merz2001)    (paper.bibtex      merz2001 inbook)    (paper.author      merz2001 "Stephan Merz")    (paper.title       merz2001 "Model Checking: A Tutorial Overview")    (paper.publisher   merz2001 "Springer-Verlag")    (paper.publication merz2001 "Modeling and Verification of Parallel Processes")    (paper.volume      merz2001 "2067")    (paper.startpage   merz2001 3)    (paper.endpage     merz2001 38)    (paper.link        merz2001 "http://www.loria.fr/~merz/papers/mc-tutorial.html")    (paper.year        merz2001 2001)    (paper.rank        merz2001 "[****]")    (paper.description merz2001 "
The model checking problem is to determine if a sentence s is entailed
by a model M.  It is mainly used to analyze reactive systems, i.e. systems
that continually interact with their environment.  These systems are usually
modelled with a Kripke structure or an omega(Buchi)-Automaton.  The sentence s 
is usually written in Propositional Temporal Logic (PTL) (which allows only
universal statements), Computation Tree Logic (CTL) (which allows 
universal and existential statements), CTL*, or propositional mu-calculus.
PTL and CTL are not comparable (neither is more expressive than the other).
CTL* is strictly more expressive than CTL and PTL, and propositional
mu-calculus subsumes CTL*.  Model checking algorithms can either be local
(PTL), global (CTL), or symbolic.  Symbolic formulas can be represented
with ordered binary decision diagrams.
")    (paper.topic       merz2001 "Model Checking")    (paper.instance    miller94)    (paper.author      miller94 "Rob Miller and Muray Shanahan")    (paper.title       miller94 "Narratives in the Situation Calculus")    (paper.publication miller94 "Journal of Logic and Computation")    (paper.volume      miller94 "4(5)")    (paper.startpage   miller94 513)    (paper.endpage     miller94 530)    (paper.link        miller94 "http://citeseer.ist.psu.edu/miller94narratives.html")    (paper.year        miller94 1994)    (paper.rank        miller94 "[*]")    (paper.description miller94 "
Miller and Shanahan take a first crack at embedding narratives in the Situation calculus.  They use a new sort for times, define Happens(action, time), and define Duration(action).  Using these, they can represent narratives, noninstantaneous actions, and overlapping actions.   Miller and Shanahan rely heavily on circumscription.  
")    (paper.topic       miller94 "Behavioral Goals")    (paper.instance    mittal90)    (paper.bibtex      mittal90 article)    (paper.author      mittal90 "Sanjay Mittal and Brian Falkenhainer")    (paper.title       mittal90 "Dynamic Constraint Satisfaction Problems ")    (paper.publication mittal90 "AAAI 90 ")    (paper.startpage   mittal90 25)    (paper.endpage     mittal90 32)    (paper.year        mittal90 1990)    (paper.rank        mittal90 "[****]")    (paper.description mittal90 "
Mittal and Falkenhainer describe a version of CSPs where the 
number of variables is partially determined by the values of other
variables.  To accomplish this they provide a constraint language that
can express conditions under which variables must be assigned values
and conditions under which variables need not be assigned values.  For 
every variable vi there is a propositional constant active:vi.  If 
active:vi must hold given the values of other variables, vi must
be assigned a value.  If ~active:vi must hold, vi need not be assigned
a value.  It is unclear what happens if neither active:vi nor ~active:vi 
holds.  They use an assumption-based truth maintenance system to build
a Dynamic CSP solver. 
")    (paper.topic       mittal90 "Various Forms of Constraint Satisfaction")    (paper.instance    moore77)    (paper.author      moore77 "Robert C. Moore")    (paper.title       moore77 "Reasoning about knowledge and action")    (paper.publication moore77 "Proceedings of the 5th International Joint
Conference on Artificial Intelligence (IJCAI-77, MIT)")    (paper.startpage   moore77 223)    (paper.endpage     moore77 227)    (paper.year        moore77 1977)    (paper.description moore77 "
Moore contributes the following formalization (after a little tweaking):
1. T(p, s): p is true in world s.
2. K(Agent, s, s'): Given what Agent knows in s, s' would be another possible world.
3. As,a,p.(True(Know(a, p), s) =def= As'.(K(a, s, s') => T(p, s')): ")    (paper.topic       moore77 "Planning with Sensing Actions")    (paper.instance    moore82)    (paper.bibtex      moore82 article)    (paper.author      moore82 "Robert C. Moore")    (paper.title       moore82 "The Role of Logic in Knowledge Representation and Commonsense Reasoning")    (paper.publication moore82 "AAAI 82")    (paper.link        moore82 "")    (paper.year        moore82 1982)    (paper.rank        moore82 "[**]")    (paper.description moore82 "
Moore tries to convince AI researchers that logic can be a useful method for reasoning and not just the analysis of systems.  He points out that some of the early experiments that supposedly showed logical reasoning too inefficient only showed that a particular method of inference and control were too inefficient to be used for every reasoning task.
")    (paper.topic       moore82 "Logical Knowledge Representation")    (paper.instance    moses96)    (paper.bibtex      moses96 article)    (paper.author      moses96 "Yoram Moses and Moshe Tennenholtz")    (paper.title       moses96 "Off-line reasoning for on-line efficiency: knowledge bases")    (paper.publication moses96 "Artificial Intelligence")    (paper.volume      moses96 "83(2)")    (paper.startpage   moses96 229)    (paper.endpage     moses96 239)    (paper.link        moses96 "http://citeseer.ist.psu.edu/moses96offline.html")    (paper.year        moses96 1996)    (paper.rank        moses96 "[***]")    (paper.description moses96 "
Moses and Tennenholtz give conditions under which compiling a KB is
worthwhile.  The results given focus almost entirely on propositional
logic.  If a KB has a basis under which all queries
can be computed via conjunctions of the basis, compilation is a good
idea if the basis can be computed in polynomial time.  The process
of compilation essentially stores all the elements of the basis
in a binary tree.  Thus, any query can be answered in (log n)*|query|.
")    (paper.topic       moses96 "Knowledge Base Compilation")    (paper.instance    moskewicz2001)    (paper.bibtex      moskewicz2001 article)    (paper.author      moskewicz2001 "Matthew Moskewicz and Conor Madigan and Ying Zhao and Lintao Zhang and Sharad Malik")    (paper.title       moskewicz2001 "Chaff: Engineering an Efficient SAT Solver")    (paper.publication moskewicz2001 "39th Design Automation Conference")    (paper.link        moskewicz2001 "http://citeseer.csail.mit.edu/moskewicz01chaff.html")    (paper.year        moskewicz2001 2001)    (paper.description moskewicz2001 "
The authors give an overview of DPLL and how it is implemented in Chaff.
Nice read.
")    (paper.topic       moskewicz2001 "Solvers")    (paper.instance    motik2007bridging)    (paper.bibtex      motik2007bridging inproceedings)    (paper.author      motik2007bridging "Boris Motik and Ian Horrocks and Ulrike Sattler")    (paper.title       motik2007bridging "Bridging the Gap Between OWL and Relational Databases")    (paper.publication motik2007bridging "Proceedings of the 16th Conference on the World Wide Web ")    (paper.link        motik2007bridging "")    (paper.year        motik2007bridging 2007)    (paper.description motik2007bridging "
The paper attempts to implement integrity constraints as defined in the 
database literature into OWL.
")    (paper.topic       motik2007bridging "Semantic Web")    (paper.instance    motik2007faithful)    (paper.bibtex      motik2007faithful inproceedings)    (paper.author      motik2007faithful "Boris Motik and Riccardo Rosati")    (paper.title       motik2007faithful "A Faithful Integration of Description Logics with Logic Programming")    (paper.publication motik2007faithful "Proceedings of International Joint Conference on Artificial Intelligence")    (paper.link        motik2007faithful "")    (paper.year        motik2007faithful 2007)    (paper.description motik2007faithful "
The paper introduces hybrid MKNF knowledge bases, a formalism that integrates
logic programming and description logics, which reduces to LP semantics
when the DL portion is empty and to DL semantics when the LP portion is empty.
Also gives complexity results for certain classes of the logic.
")    (paper.topic       motik2007faithful "Semantic Web")    (paper.instance    motta91)    (paper.bibtex      motta91 article)    (paper.author      motta91 "Enrico Motta and Arthur Stutt")    (paper.title       motta91 "An Architecture for the Integration of Heterogenous Inference Systems")    (paper.publication motta91 "Open University Technical Report")    (paper.link        motta91 "http://citeseer.ist.psu.edu/motta91architecture.html")    (paper.year        motta91 1991)    (paper.rank        motta91 "[*]")    (paper.description motta91 "
The authors describe an architecture for integrating specialized reasoners
where each reasoner is known to handle a particular vocabulary.
")    (paper.topic       motta91 "Architectures")    (paper.instance    muhammed2006)    (paper.bibtex      muhammed2006 inproceedings)    (paper.author      muhammed2006 "R. Muhammed and P.J. Stuckey")    (paper.title       muhammed2006 "A stochastic Non-CNF SAT Solver")    (paper.publication muhammed2006 "In Proceedings 9th Biennial Pacific Rim International Conference on Artificial Intelligence")    (paper.startpage   muhammed2006 120)    (paper.endpage     muhammed2006 129)    (paper.link        muhammed2006 "http://www.cs.mu.oz.au/~pjs/papers.html")    (paper.year        muhammed2006 2006)    (paper.description muhammed2006 "
Paper on a stochastic SAT solver whose input is allowed to be 
in something other than CNF.
")    (paper.topic       muhammed2006 "SAT Solving")    (paper.instance    myers90)    (paper.bibtex      myers90 inproceedings)    (paper.author      myers90 "Karen Myers")    (paper.title       myers90 "Automatically Generating Universal Attachments through Compilation")    (paper.publication myers90 "AAAI")    (paper.link        myers90 "http://www.ai.sri.com/~myers/")    (paper.year        myers90 1990)    (paper.rank        myers90 "[**]")    (paper.description myers90 "
Myers describes a method for generating new universal attachments.
")    (paper.topic       myers90 "Theorem Proving with Attachments")    (paper.instance    myers90t)    (paper.bibtex      myers90t phdthesis)    (paper.author      myers90t "Karen Myers")    (paper.title       myers90t "Universal Attachments: A Logical Framework for Hybrid Reasoning")    (paper.publisher   myers90t "Stanford University")    (paper.link        myers90t "http://library.stanford.edu/depts/mathcs/mathcscoll/techreports.html")    (paper.year        myers90t 1990)    (paper.rank        myers90t "[***]")    (paper.description myers90t "
")    (paper.topic       myers90t "Theorem Proving with Attachments")    (paper.instance    nareyek99)    (paper.bibtex      nareyek99 article)    (paper.author      nareyek99 "Alexander Nareyek")    (paper.title       nareyek99 "Structural Constraint Satisfaction")    (paper.publication nareyek99 "AAAI Workshop on Configurations")    (paper.startpage   nareyek99 76)    (paper.endpage     nareyek99 82)    (paper.link        nareyek99 "http://citeseer.ist.psu.edu/nareyek99structural.html")    (paper.year        nareyek99 1999)    (paper.rank        nareyek99 "[***]")    (paper.description nareyek99 "
It is a little hard to follow what's happening here.  Nareyek seems
to be introducing generalization of CSPs where in addition to constraints
on tuples of variables, there are constraints on the structure of those
constraints.  These meta constraints are implemented via Graph Grammars.
Narayek states that solving these problems is undecidable, but gives
no proof.
")    (paper.topic       nareyek99 "Various Forms of Constraint Satisfaction")    (paper.instance    nayak95)    (paper.bibtex      nayak95 proceedings)    (paper.author      nayak95 "P. Nayak and Alon Levy")    (paper.title       nayak95 "A Semantic Theory of Abstractions")    (paper.publication nayak95 "IJCAI")    (paper.link        nayak95 "http://citeseer.ist.psu.edu/nayak94semantic.html")    (paper.year        nayak95 1995)    (paper.rank        nayak95 "[***]")    (paper.description nayak95 "
Nayak and Levy explain a new approach to producing abstractions of
axioms.  Instead of simply mapping axioms to axioms as is done in
previous work, first abstract the axioms to an intended domain
model, and then find sentences that capture that model.  The
theoretical component of this paper, which accounts for the
majority of it, is very clean.  It gives model-theoretic
explainations for good (allowing no 'false proofs') and bad
(allowing 'false proofs') abstractions.
")    (paper.topic       nayak95 "Abstraction")    (paper.instance    nelson79)    (paper.bibtex      nelson79 inproceedings)    (paper.author      nelson79 "Greg Nelson and Derek Oppen")    (paper.title       nelson79 "Simplification by Cooperating Decision Procedures")    (paper.publication nelson79 "ACM Transactions on Programming Languages and Systems
           ")    (paper.volume      nelson79 "1(2)")    (paper.startpage   nelson79 245)    (paper.endpage     nelson79 257)    (paper.link        nelson79 "")    (paper.year        nelson79 1979)    (paper.rank        nelson79 "[****]")    (paper.description nelson79 "
Nelson and Oppen describe a method for combining decision procedures
for disjoint equational theories.
To determine the satisfiability of a set of quantifier-free formulas,
partition based on the theory to which the vocabulary belongs.  If 
a term from one theory is embedded in a relation from another theory,
introduce a new variable, replace the term with that variable, and include
the sentence where that variable is set to the term in the appropriate
partition.  If any of the partitions are unsatisfiable, the entire thing
is unsatisfiable.  Otherwise, compute all the strongest equality theorems
including only variables for each of the partitions and pass to all the 
other partitions.  If the theory is nonconvex, case analysis is
necessary.
Check for unsatisfiability and repeat.  Nelson and Oppen prove correctness
and termination.
")    (paper.topic       nelson79 "Nelson-Oppen and Shostak")    (paper.instance    newell63)    (paper.author      newell63 "Allen Newell and H.A. Simon")    (paper.title       newell63 "GPS, A Program that Simulates Human Thought")    (paper.publication newell63 "Computers and Thought, Feigenbaum and Feldman (eds.)")    (paper.startpage   newell63 279)    (paper.endpage     newell63 293)    (paper.year        newell63 1963)    (paper.description newell63 "
Description of probably the first planner. Newell et. al. compare an inexperienced student doing a logic proof to how GPS solves the same problem.  GPS takes a goal and can perform any of four operations: transform object A into object B (unification?), reduce the difference D between object A and object B (satisfaction of a subgoal of A?), and apply operator Q to object A (reduction of A to subgoals?).
")    (paper.topic       newell63 "Historical")    (paper.instance    nieuwenhuis2003)    (paper.author      nieuwenhuis2003 "Robert Nieuwenhuis and Thomas Hillenbrand and Alexandre Riazanov and Andrei Voronkov")    (paper.title       nieuwenhuis2003 "On the Evaluation of Indexing Techniques for Theorem Proving")    (paper.link        nieuwenhuis2003 "http://citeseer.ist.psu.edu/nieuwenhuis03evaluation.html")    (paper.year        nieuwenhuis2003 2003)    (paper.description nieuwenhuis2003 "
Nieuwenhuis, et. al. describe a methodology for comparing indexing schemes.
To demonstrate it, they compare Waldmeister's discrimination trees,
Context trees, and Vampire's code trees for term indexing.  The scenario tested
involved both additions and deletions from the index and queries where
only one answer need be reported (e.g. for forward subsumption).  They
reported both space and time results.
")    (paper.topic       nieuwenhuis2003 "Indexing")    (paper.instance    niles2001)    (paper.bibtex      niles2001 inproceedings)    (paper.author      niles2001 "Ian Niles and Adam Pease")    (paper.title       niles2001 "Formal Ontology in Information Systems")    (paper.publication niles2001 "Proceedings of the International Conference on Formal Ontology in Information Systems")    (paper.startpage   niles2001 2)    (paper.endpage     niles2001 9)    (paper.link        niles2001 "http://portal.acm.org/citation.cfm?id=505170")    (paper.year        niles2001 2001)    (paper.description niles2001 "
Primary paper on SUMO, the Suggested Upper Merged Ontology.
")    (paper.topic       niles2001 "Logical Knowledge Representation")    (paper.instance    orman98)    (paper.bibtex      orman98 article)    (paper.author      orman98 "Levent Orman")    (paper.title       orman98 "Differential Relational Calculus for Integrity Maintenance")    (paper.publication orman98 "IEEE Transactions of Knowledge and Data Engineering")    (paper.volume      orman98 "10(2)")    (paper.startpage   orman98 328)    (paper.endpage     orman98 341)    (paper.link        orman98 "http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=683760&isnumber=15039")    (paper.year        orman98 1998)    (paper.rank        orman98 "[****]")    (paper.description orman98 "
Orman defines differential relational calculus.  Given a 
database constraint V such that V is true if and only if there is an integrity
constraint violation and a transaction T, what are the necessary and
sufficient conditions for computing whether there is would be an integrity
violation after execution T, assuming there was no violation to start with?
")    (paper.topic       orman98 "Databases")    (paper.instance    owre96)    (paper.bibtex      owre96 inproceedings)    (paper.author      owre96 "S. Owre et. al.")    (paper.title       owre96 "PVS: Combining Specification, Proof Checking, and Model Checking")    (paper.publication owre96 "Computer Aided Verification")    (paper.startpage   owre96 411)    (paper.endpage     owre96 414)    (paper.link        owre96 "http://citeseer.ist.psu.edu/246180.html")    (paper.year        owre96 1996)    (paper.rank        owre96 "[*]")    (paper.description owre96 "
PVS (Prototype Verification System) allows a user to construct a
machine-verified proof.  It combines interactive theorem
proving, typechecking, a set of decision procedures including
a model checker, and strategies to avoid tedious proof step 
repetitions for the user.   The strategies can be extended
if necessary.
")    (paper.topic       owre96 "Model Checking")    (paper.instance    paltrinieri94)    (paper.bibtex      paltrinieri94 article)    (paper.author      paltrinieri94 "Massimo Paltrinieri")    (paper.title       paltrinieri94 "Some Remarks on the Design of Constraint Satisfaction
Problems")    (paper.publication paltrinieri94 "Second International Workshop on the Principles
and Practice of Constraint Programming")    (paper.startpage   paltrinieri94 299)    (paper.endpage     paltrinieri94 311)    (paper.link        paltrinieri94 "http://citeseer.ist.psu.edu/paltrinieri94some.html")    (paper.year        paltrinieri94 1994)    (paper.rank        paltrinieri94 "[****]")    (paper.description paltrinieri94 "
Paltrinieri gives a loose definition for an object-oriented constraint
satisfaction problem and goes on to show its utility for modelling
some CSPs.  His formulation of an OOCSP is no more expressive than
a standard CSP; solving an OOCSP then amounts to translating it into
a CSP.  To do this, find all the instances of objects needed to solve
the OOCSP and replicate the class constraints for each instance.
")    (paper.topic       paltrinieri94 "Object Oriented Constraint Satisfaction")    (paper.instance    paltrinieri95)    (paper.bibtex      paltrinieri95 article)    (paper.author      paltrinieri95 "Massimo Paltrinieri")    (paper.title       paltrinieri95 "A Visual Environment for Constraint Programming")    (paper.publication paltrinieri95 "Proceedings 11th International IEEE Symposium on Visual
Langauges")    (paper.startpage   paltrinieri95 118)    (paper.endpage     paltrinieri95 119)    (paper.link        paltrinieri95 "
http://citeseer.ist.psu.edu/paltrinieri95visual.html")    (paper.year        paltrinieri95 1995)    (paper.rank        paltrinieri95 "[****]")    (paper.description paltrinieri95 "
Paltrinieri gives a 2-page description of a generalization of CSPs to
object-oriented CSPs.  He also describes a visual environment for 
dealing with OOCSPs.
")    (paper.topic       paltrinieri95 "Object Oriented Constraint Satisfaction")    (paper.instance    pastre78)    (paper.bibtex      pastre78 article)    (paper.author      pastre78 "D. Pastre")    (paper.title       pastre78 "Automatic Theorem Proving in Set Theory")    (paper.publication pastre78 "Artificial Intelligence")    (paper.volume      pastre78 "10(1)")    (paper.startpage   pastre78 1)    (paper.endpage     pastre78 27)    (paper.link        pastre78 "")    (paper.year        pastre78 1978)    (paper.rank        pastre78 "[**]")    (paper.description pastre78 "
Pastre describes PROVER, a theorem prover for set theory that builds
diagrams from axioms in order to prune the search space of theorem proving.
Pastre admits the methods to be incomplete, but claims an incomplete
method may prove more theorems than a complete one in practice.
")    (paper.topic       pastre78 "Graphical Reasoning Techniques")    (paper.instance    peltier2000)    (paper.bibtex      peltier2000 article)    (paper.author      peltier2000 "Nicolas Peltier")    (paper.title       peltier2000 "Model Building with Ordered Resolution")    (paper.publication peltier2000 "International Workshop on First Order Theorem Proving (FTP)")    (paper.link        peltier2000 "http://citeseer.ist.psu.edu/peltier00model.html")    (paper.year        peltier2000 2000)    (paper.rank        peltier2000 "[****]")    (paper.description peltier2000 "
Peltier describes two algorithms that can be used after an ordered
resolution proof has failed: (1) to extract from the resolution closure
a set of formulas that have a single Herbrand model where that model
satisfies the original clauseset and (2) to turn those formulas into
a set of clauses.  Peltier's Herbrand model is represented by a clauseset,
and since the clauseset is guaranteed to have exactly one Herbrand model,
proof by consistency can be used to check whether that model satisfies
a particular sentence; moreover, for ground literals, satisfaction is decidable
since either the literal or its negation must be consistent and the 
clauseset is satisfiable, exactly one is consistent, which ensures one
is inconsistent.  Any off-the- shelf first-order entailment algorithm will 
find an inconsistency.  The technique is based on predicate completion,
which only works when there is a strict ordering among the clauses. 
")    (paper.topic       peltier2000 "Model Building")    (paper.instance    pereira80)    (paper.bibtex      pereira80 inproceedings)    (paper.author      pereira80 "Pereira and Porto")    (paper.title       pereira80 "Selective Backtracking for Logic Programs")    (paper.publication pereira80 "In Proceedings of the Conference on Automated Deduction")    (paper.year        pereira80 1980)    (paper.description pereira80 "
One of the papers that uses the Logic Programming formulation of map coloring.
")    (paper.topic       pereira80 "Logic Programming")    (paper.instance    peterson98)    (paper.bibtex      peterson98 inproceedings)    (paper.author      peterson98 "Brian Peterson and William Anderson and Joshua Engel")    (paper.title       peterson98 "Knowledge Bus: Generating Application-focused Databases from Large Ontologies")    (paper.publication peterson98 "Knowledge Representation Meets Databases")    (paper.link        peterson98 "http://citeseer.ist.psu.edu/peterson98knowledge.html")    (paper.year        peterson98 1998)    (paper.description peterson98 "
The authors attempt to construct a logic program from Cyc for answering a
given set of queries.  Their transformation is lossy, and it is unclear
to what extent the logical consequences are preserved.
")    (paper.topic       peterson98 "Reformulation")    (paper.instance    petrie2004)    (paper.bibtex      petrie2004 article)    (paper.author      petrie2004 "Charles J. Petrie, Michael R. Genesereth, Hans Bjornsson, Rada Chirkova, Martin Ekstrom, Hidehito Gomi, Tim Hinrichs, Rob Hoskins, Michael Kassoff, Daishi Kato, Kyohei Kawazoe, Jung Ung Min, and Waqar Mohsin")    (paper.title       petrie2004 "Adding AI to Web Services")    (paper.publication petrie2004 "Lecture Notes in Artificial Intelligence")    (paper.volume      petrie2004 "2926")    (paper.startpage   petrie2004 322)    (paper.endpage     petrie2004 338)    (paper.link        petrie2004 "papers/petrie2004adding.pdf")    (paper.year        petrie2004 2004)    (paper.description petrie2004 "
The FX-Agents project consisted of members of the Stanford Logic Group 
and industrial visitors from NEC and Intec Web and Genome working together 
to develop new technologies based upon the combination of Web services and 
techniques from artificial intelligence, using our experience in AI-based 
software agents. This two-year project ran from April 2001 until March 2002 
and explored the then emerging functionality of Web services. This paper is 
a result of our findings. In particular, this paper discusses the shortcomings 
of current Web service standards like WSDL and how logical AI techniques 
like declarative commands, agents, and planning can be used to address some 
of these shortcomings. The primary problems that we address are automated 
Web service discovery and composition of Web services. 
")    (paper.topic       petrie2004 "References")    (paper.instance    pichler2000)    (paper.bibtex      pichler2000 article)    (paper.author      pichler2000 "Reinhard Pichler")    (paper.title       pichler2000 "On the Complexity of Equational Problems in CNF over a Finite Domain")    (paper.link        pichler2000 "http://citeseer.ist.psu.edu/478741.html")    (paper.year        pichler2000 2000)    (paper.rank        pichler2000 "[**]")    (paper.description pichler2000 "
Pichler shows that the complexity of satisfiability of a purely equational 
Herbrand logic sentence E*A*.M, where M is in CNF, is Sigma_2^p-complete.
")    (paper.topic       pichler2000 "Herbrand Logic")    (paper.instance    pinto94)    (paper.author      pinto94 "Javier Pinto")    (paper.title       pinto94 "Temporal Reasoning in the Situation Calculus")    (paper.link        pinto94 "http://citeseer.ist.psu.edu/pinto94temporal.html")    (paper.year        pinto94 1994)    (paper.rank        pinto94 "[**]")    (paper.description pinto94 "
Pinto's thesis extends situation calculus to cover temporal reasoning.  He includes ACTUAL PATHS (narratives), concurrent actions, and continuous time with natural events.  He goes on to compare his situation calculus with interval temporal logic, the calculus of events, and modal temporal logics.  He later reworked his approach to narratives in [pinto98], which does not call for circumscription.  Reiter published a paper on 'Natural Actions, Concurrency, and Continuous Time' [reiter96] after Pinto--should probably read that one instead.
")    (paper.topic       pinto94 "Behavioral Goals")    (paper.related     pinto94 reiter96)    (paper.related     pinto94 pinto98)    (paper.instance    pinto98)    (paper.author      pinto98 "Javier Pinto")    (paper.title       pinto98 "Occurrences and Narratives as Constraints in the Branching Structure of the Situation Calculus")    (paper.publication pinto98 "Journal of Logic and Computation")    (paper.volume      pinto98 "8(6)")    (paper.startpage   pinto98 777)    (paper.endpage     pinto98 808)    (paper.link        pinto98 "http://citeseer.ist.psu.edu/pinto96occurrences.html")    (paper.year        pinto98 1998)    (paper.rank        pinto98 "[*****]")    (paper.description pinto98 "
Pinto tackles the problem of expressing occurrences in situation calculus.  Occurrences are sequences of events that really do happen in every hypothetical branch of the situation tree.  In deductive plan synthesis via the Green method, we want all plans returned to include all the occurrences given.  Pinto names two specific types of occurrences: narratives (a partial specification of a sequence of events known to happen) and triggers (as in the database sense).  His formulation assigns time values to situations, a second order induction axiom, along with an extension to the sitcalc ontology to define 'nonpreventable occurrences', 'conditional occurrences', and 'simple occurrences'.  He defines 'legal(s)' in terms of these 3 occurrence types to mean that a situation s includes all the mentioned occurrences and is a valid sitcalc situation.  Pinto goes on to define when one situation is preferred to another: s preferred-to s' iff s' has the same actions as s with extra actions appended to the end.  For planning, Pinto shows that instead of finding an s s.t. goal(s) is true, we can find an s s.t. goal(s) ^ legal(s) is true. 
")    (paper.topic       pinto98 "Behavioral Goals")    (paper.instance    pirri99)    (paper.author      pirri99 "Fiora Pirri and Ray Reiter")    (paper.title       pirri99 "Some Contributions to the Metatheory of the Situation Calculus")    (paper.publication pirri99 "Journal of the ACM")    (paper.volume      pirri99 "46(3)")    (paper.startpage   pirri99 325)    (paper.endpage     pirri99 361)    (paper.link        pirri99 "http://citeseer.ist.psu.edu/pirri99some.html")    (paper.year        pirri99 1999)    (paper.rank        pirri99 "[****]")    (paper.description pirri99 "
Pirri and Reiter formalize the theory of situation calculus and define basic
action theories as D = Sigma U D_ss U D_ap U D_una U D_s0.  Sigma are the
foundational axioms (induction, situation ordering, equality, unique names
for situations); D_ss are
successor state axioms; D_ap are action precondition axioms; D_una are unique
name axioms for actions; D_s0 are initial state axioms.  This definition
appears throughout later work on the situation calculus.  Well written.
They also define Regression (in a clearer way than reiter91--the
Frame problem...), which means rewriting a regressable formula so that the only
situations referenced are S0.  Entailment of a regressed formula requires
only knowledge
of D_s0 and D_una.  Pirri and Reiter also give conditions when induction is
not needed for entailment, as well as when other components of Sigma are
unnecessary.  They go on to show that Sigma is unnecessary for GOLOG programs.
")    (paper.topic       pirri99 "Fundamentals")    (paper.instance    plaisted81)    (paper.bibtex      plaisted81 article)    (paper.author      plaisted81 "David Plaisted")    (paper.title       plaisted81 "Theorem Proving with Abstraction")    (paper.publication plaisted81 "Artificial Intelligence")    (paper.volume      plaisted81 "16(1)")    (paper.startpage   plaisted81 47)    (paper.endpage     plaisted81 107)    (paper.link        plaisted81 "")    (paper.year        plaisted81 1981)    (paper.description plaisted81 "
Plaisted defines the notion of an abstraction of a literal and a clause and
shows that under this definition, the union and composition of two
abstractions is another abstraction.  He also gives local conditions
under which a mapping is an abstraction, which can be practically used.
He goes on to discuss abstract resolution proofs and multi-clauses (which
admit simpler proof procedures than regular clauses).  He discusses
using more than one abstraction at a time.  Using these abstractions admits
a complete proof procedure.  Finally, he introduces a variant
of the multi-clause notion and considers a version of completeness for it.
")    (paper.topic       plaisted81 "Abstraction")    (paper.instance    plaisted94)    (paper.bibtex      plaisted94 inproceedings)    (paper.author      plaisted94 "David Plaisted")    (paper.title       plaisted94 "The Search Efficiency of Theorem Proving Strategies: An Analytical Comparison")    (paper.publication plaisted94 "CADE")    (paper.startpage   plaisted94 1)    (paper.endpage     plaisted94 40)    (paper.link        plaisted94 "http://citeseer.ist.psu.edu/plaisted94search.html")    (paper.year        plaisted94 1994)    (paper.rank        plaisted94 "[***]")    (paper.description plaisted94 "
Plaisted does a comparison of a slew of clausal, refutation 
theorem-proving strategies for
propositional Horn clauses.  He gives three measures for the search
spaces generated by each strategy: iterated duplication, case-analysis
duplication, and combination duplication, the produce of which bounds
the total size of the search space.  The first measures the longest path,
the second measures the number of paths, and the third measures the
maximum size of clauses generated in any state. 
")    (paper.topic       plaisted94 "Comparative Analysis")    (paper.instance    plaisted97)    (paper.bibtex      plaisted97 inproceedings)    (paper.author      plaisted97 "David Plaisted and Yunshan Zhu")    (paper.title       plaisted97 "Ordered Semantic Hyper Linking")    (paper.publication plaisted97 "AAAI")    (paper.link        plaisted97 "http://citeseer.ist.psu.edu/218207.html")    (paper.year        plaisted97 1997)    (paper.rank        plaisted97 "[***]")    (paper.description plaisted97 "
Ordered Semantic Hyper Linking is a proof procedure based on
model-building.  In includes a total ordering on
formulae to remove redundant interpretations.  OSHL allows the user
to give an initial interpretation to guide the search.
It propositionalizes first order theories to build its models.  
The paper claims soundness and completeness, but no proofs are
supplied (probably due to lack of space).
")    (paper.topic       plaisted97 "Resolution Variants")    (paper.instance    plummer95)    (paper.bibtex      plummer95 misc)    (paper.author      plummer95 "Dave Barker-Plummer and Sidney Bailin")    (paper.title       plummer95 "Proofs and Pictures: Proving the Diamond lemma with the GROVER Theorem Proving System")    (paper.link        plummer95 "http://citeseer.ist.psu.edu/193199.html")    (paper.year        plummer95 1995)    (paper.rank        plummer95 "[*]")    (paper.description plummer95 "
The authors describe a theorem proving system that takes a set of axioms, Delta,
a diagram, D, and a conjecture phi.  It proves Delta |= phi by 1) examining D and 
determining that D |= phi, and 2) proving Delta |= D.  The method seems to be 
ad hoc with no guarantees of soundness or completeness; however, given enough
hints, GROVER was able to prove the Diamond Lemma.
")    (paper.topic       plummer95 "Graphical Reasoning Techniques")    (paper.instance    prasad2005)    (paper.bibtex      prasad2005 article)    (paper.author      prasad2005 "Mukul Prasad and Armin Biere and Aarti Gupta")    (paper.title       prasad2005 "A Survey of Recent Advances in SAT-Based Formal Verification")    (paper.publication prasad2005 "Software Tools for Technology Transfer")    (paper.volume      prasad2005 "7(2)")    (paper.startpage   prasad2005 156)    (paper.endpage     prasad2005 173)    (paper.link        prasad2005 "http://www.cerc.utexas.edu/~jay/fv_surveys/")    (paper.year        prasad2005 2005)    (paper.description prasad2005 "
A survey paper discussing how boolean SAT solvers are being used
for formal verification.
")    (paper.topic       prasad2005 "Formal Verification")    (paper.instance    puebla2004)    (paper.bibtex      puebla2004 article)    (paper.author      puebla2004 "G. Puebla and E. Albert and M. Hermenegildo")    (paper.title       puebla2004 "Efficient Local Unfolding with Ancestor Stacks for Full Prolog")    (paper.publication puebla2004 "14th International Symposium, Logic Based Program Synthesis and Transformation (LOPSTR)")    (paper.startpage   puebla2004 1)    (paper.endpage     puebla2004 18)    (paper.link        puebla2004 "http://wotan.liu.edu/docis/dbl/lopstr/index.html")    (paper.year        puebla2004 2004)    (paper.description puebla2004 "
In the context of Horn prolog with built-ins, the authors address the
problem of efficiently unfolding of recursive relations.  That is,
one implementation of partial evaluation/partial deduction is to produce
a partial SLD tree and using the fringe as the body of a rule for proving
the goal.  The problem with this approach is that the recursive relations 
produce infinite SLD trees, unless the recursion is cut off.  To determine
when the recursion should be cut off, a common practice is to apply a
recursive rule only when the subgoal is smaller, according to some
ordering, e.g. well-founded, well-quasi, than all its ancestors.  To
implement that, all the ancestors of each goal have in the past 
been stored, which is expensive both in time to check and in space
to store.  This paper tackles this problem by expanding the tree in a 
variant of depth-first search so that only the current call stack need
be stored.  They also consider the case of built-in predicates.
")    (paper.topic       puebla2004 "Reformulation")    (paper.instance    ramachandran2005)    (paper.bibtex      ramachandran2005 article)    (paper.author      ramachandran2005 "D. Ramachandran and E. Amir")    (paper.title       ramachandran2005 "Compact Propositional Encodings of First-Order Theories")    (paper.publication ramachandran2005 "20th National Conference on Artificial Intelligence")    (paper.link        ramachandran2005 "http://reason.cs.uiuc.edu/eyal/paper.html")    (paper.year        ramachandran2005 2005)    (paper.description ramachandran2005 "
The authors introduce polynomial-time algorithms for converting
first-order logic (sometimes with a DCA) into propositional logic,
where the goal is a small output size.  Relies on a certain
structure within the FOL theory.
")    (paper.topic       ramachandran2005 "Reformulation")    (paper.instance    rao2004)    (paper.bibtex      rao2004 article)    (paper.author      rao2004 "M.R.K. Krishna Rao")    (paper.title       rao2004 "Input-Termination of Logic Programs")    (paper.publication rao2004 "14th International Symposium, Logic Based Program Synthesis and Transformation (LOPSTR)")    (paper.startpage   rao2004 215)    (paper.endpage     rao2004 230)    (paper.link        rao2004 "http://wotan.liu.edu/docis/dbl/lopstr/index.html")    (paper.year        rao2004 2004)    (paper.description rao2004 "
Rao defines a class of logic programs, linear bounded programs,
and show that such programs are input-terminating under Prolog's 
selection rule.  Rao also shows that all simply-moded linear bounded
programs are input-terminating.  Importantly, determining whether
a program is linearly bounded is decidable, unlike previous work
which gave necessary and sufficient conditions for a class of 
decidable programs, but checking those conditions is undecidable.  Input-
consuming derivations do not instantiate variables that only occur
in the input positions of the initial query, and input-termination is the
property that all input-consuming derivations are finite.
")    (paper.topic       rao2004 "Reformulation")    (paper.instance    reiter2000)    (paper.author      reiter2000 "Ray Reiter")    (paper.title       reiter2000 "Narratives as Programs")    (paper.publication reiter2000 "Principles of Knowledge Representation and Reasoning")    (paper.startpage   reiter2000 99)    (paper.endpage     reiter2000 108)    (paper.link        reiter2000 "http://citeseer.ist.psu.edu/reiter00narratives.html")    (paper.year        reiter2000 2000)    (paper.rank        reiter2000 "[*]")    (paper.description reiter2000 "
Reiter argues that narratives (a sequence of events known to occur) are actually programs.  He uses GOLOG to represent narratives and shows that making a query Q(s) over a narrative is identical to proving a property about a program, as in program verification.  In GOLOG terms, we want to ask whether Q holds on a program v; we simply show that SitcalcAxioms |= As.(Do(v,S0,s)=>Q(s)).
")    (paper.topic       reiter2000 "Behavioral Goals")    (paper.instance    reiter73)    (paper.bibtex      reiter73 inproceedings)    (paper.author      reiter73 "Ray Reiter")    (paper.title       reiter73 "A Semantically Guided Deductive System for Automatic Theorem-Proving")    (paper.publication reiter73 "IJCAI")    (paper.year        reiter73 1973)    (paper.rank        reiter73 "[**]")    (paper.description reiter73 "
Reiter describes a system for natural deduction that intertwines model-
building and deduction, where each affects the other.  The system itself
is composed of 12 inference rules which manipulate both axioms and 
models.  This may have been the seminal work on using an explicit 
model to aid theorem-proving; previous work used heuristics, which
Reiter characterized as implicit semantics. 
")    (paper.topic       reiter73 "Model-Guided Proof Techniques")    (paper.instance    reiter78)    (paper.bibtex      reiter78 inproceedings)    (paper.author      reiter78 "Ray Reiter")    (paper.title       reiter78 "On Closed World Databases")    (paper.publication reiter78 "In proceedings of 1978 ACM SIGMOD International Conference on Management of Data")    (paper.year        reiter78 1978)    (paper.description reiter78 "
The closed world assumption (CWA).
")    (paper.topic       reiter78 "Logical Knowledge Representation")    (paper.instance    reiter80)    (paper.bibtex      reiter80 article)    (paper.author      reiter80 "Raymond Reiter")    (paper.title       reiter80 "Equality and Domain Closure in First-Order Databases")    (paper.publication reiter80 "Journal of the ACM")    (paper.volume      reiter80 "27(2)")    (paper.startpage   reiter80 235)    (paper.endpage     reiter80 249)    (paper.link        reiter80 "http://portal.acm.org/citation.cfm?id=322189")    (paper.year        reiter80 1980)    (paper.rank        reiter80 "[**]")    (paper.description reiter80 "
Reiter explores techniques for doing first-order reasoning with 
a Domain Closure Axiom
(x=a1 | x=a2 | ... | x=an).  He focuses on axiom sets for which there
are a finite number of object constants, no function constants, and when
written in prenex form include no existential quantifiers.
To do so he (1) shows that existential queries will never require
the DCA, for algorithms exploiting Herbrand's theorem, and 
(2) how to build on this fact 
to answer any kind of query by peeling away quantifiers to build an 
existential query.  The second step uses a generalization of the 
relational database division operator for universal quantifiers and a 
generalization of the projection operator for existential quantifiers. 
Next it is shown that Horn databases and positive existential queries never 
require computing disjunctive answers. 
Then conditions are given for the removal of all but the x=x equality axiom: 
E-saturation, i.e.
unique names over all the constants.  Lastly it is
shown that extending DCA to include infinitely many constants can result in
an infinite disjunction as an answer to a query.  Very thorough, as always.
")    (paper.topic       reiter80 "Logical Knowledge Representation")    (paper.instance    reiter84)    (paper.bibtex      reiter84 chapter)    (paper.author      reiter84 "Raymond Reiter")    (paper.title       reiter84 "Towards a Logical Reconstruction of Relational Database Theory")    (paper.publisher   reiter84 "Springer-Verlag")    (paper.publication reiter84 "On Conceptual Modelling")    (paper.startpage   reiter84 191)    (paper.endpage     reiter84 238)    (paper.link        reiter84 "")    (paper.year        reiter84 1984)    (paper.rank        reiter84 "[***]")    (paper.description reiter84 "
Reiter formalizes Database theory from a logical perspective.  Databases in this view are first-order models, and integrity constraints are sentences that such a model satisfies.  A relational theory (the set of sentences entailed by a relational database) can be defined as the set of ground atoms in the DB, a completion axiom for each table ensuring that those ground atoms not in the table are false, unique-names axioms, a domain closure axiom, and equality/substitution axioms.   Proof is provided.  Next Reiter goes on to generalize this definition to handle positive disjunction, which requires altering the completion axioms.  Lastly he shows how to deal with null values by changing the DCA and the completion axioms. 
")    (paper.topic       reiter84 "Logical Knowledge Representation")    (paper.instance    reiter91)    (paper.author      reiter91 "Raymond Reiter")    (paper.title       reiter91 "The Frame Problem in the Situation Calculus: A Simple Solution (Sometimes) and a Completeness Result for Goal Regression")    (paper.publication reiter91 "Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy")    (paper.link        reiter91 "http://ftp.cs.toronto.edu/pub/cogrob/README.html")    (paper.year        reiter91 1991)    (paper.rank        reiter91 "[**]")    (paper.description reiter91 "
Reiter describes his solution to the frame problem, namely successor state
axioms of the form
Poss(a,s)=>[R(do(a,s)) =def= r_pos(a,s) V R(s) ^ ~r_neg(a,s)].  Poss(a,s) is
the action precondition axiom; R(do(a,s)) is the fluent, r_pos(a,s) is the
positive effect axiom for R; r_neg(a,s) is the negative effect axiom for R.
This works when also given a single definition for Poss(A,s) for each action A,
unique naming axioms for actions and states.  This technique requires F+A
axioms instead of 2*A*F axioms to describe frame and effect axioms for a
domain, where F is the number of fluents and A the number of actions.
Reiter also discusses a regression operator, which does deductive plan synthesis by taking a normal situation calculus axiomitization (using successor state axioms) and rewrites it so that every fluent only mentions S0.  Under certain conditions, Reiter shows that the rewriting can be done iff Es.G(s).
")    (paper.topic       reiter91 "Fundamentals")    (paper.instance    reiter93)    (paper.author      reiter93 "Raymond Reiter")    (paper.title       reiter93 "Proving Properties of States in the Situation Calculus")    (paper.publication reiter93 "Artificial Intelligence")    (paper.volume      reiter93 "64(2)")    (paper.startpage   reiter93 337)    (paper.endpage     reiter93 351)    (paper.link        reiter93 "http://citeseer.ist.psu.edu/reiter93proving.html")    (paper.year        reiter93 1993)    (paper.rank        reiter93 "[****]")    (paper.description reiter93 "
Reiter motivates the need to prove properties about states by citing the examples in the physical world (once broken, an object stays that way), planning (showing there is no plan to achieve a goal), and integrity constraints on a database.  He formulates an induction axiom sufficient for proving P(s) for all states s>=S0.  He relies on explanation closure axioms, proposed by Schubert and Haas, which are logically equivalent to frame axioms.
")    (paper.topic       reiter93 "Theories of Action")    (paper.instance    reiter95)    (paper.author      reiter95 "Ray Reiter")    (paper.title       reiter95 "On Specifying Database Updates")    (paper.publication reiter95 "Journal of Logic Programming")    (paper.volume      reiter95 "25(1)")    (paper.startpage   reiter95 53)    (paper.endpage     reiter95 91)    (paper.link        reiter95 "http://citeseer.ist.psu.edu/reiter92specifying.html")    (paper.year        reiter95 1995)    (paper.rank        reiter95 "[*]")    (paper.description reiter95 "
Reiter summarizes several previous pieces of work including proving properties of states in Situation Calculus and formalizing the querying of a database transaction.  He also compares this approach to other database query techniques.
")    (paper.topic       reiter95 "Hypothetical Temporal DB Queries")    (paper.instance    reiter96)    (paper.author      reiter96 "Ray Reiter")    (paper.title       reiter96 "Natural Actions, Concurrency and Continuous Time in the Situation Calculus")    (paper.publication reiter96 "Principles of Knowledge Representation and Reasoning: Proceedings of the Fifth International Conference")    (paper.startpage   reiter96 2)    (paper.endpage     reiter96 13)    (paper.link        reiter96 "http://ftp.cs.toronto.edu/pub/cogrob/README.html")    (paper.year        reiter96 1996)    (paper.rank        reiter96 "[*****]")    (paper.description reiter96 "
Reiter gives an axiomitization for Concurrent, Temporal Situation Calculus.  He mentions any solution to the Precondition Interaction Problem (where two actions preconditions may both be satisfied yet the 2 actions are mutually exclusive) works within the given framework.  The axiomitization allows actions following the laws of physics to occur at the same time an agent is acting in the world.  Plans can consequently be produced that take these laws into account.
")    (paper.topic       reiter96 "Behavioral Goals")    (paper.instance    riazanov02)    (paper.bibtex      riazanov02 article)    (paper.author      riazanov02 "Alexandre Riazanov and Andrei Voronkov")    (paper.title       riazanov02 "The design and implementation of Vampire")    (paper.publication riazanov02 "AI Communications")    (paper.volume      riazanov02 "15")    (paper.startpage   riazanov02 91)    (paper.endpage     riazanov02 110)    (paper.link        riazanov02 "")    (paper.year        riazanov02 2002)    (paper.rank        riazanov02 "")    (paper.description riazanov02 "
Riazanov and Voronkov describe Vampire v2.0.  They first give an overview
of the high-level functionality, then give a more detailed discussion
of the architecture based on DISCOUNT, talk about term-indexing, 
and delve into representational
issues (e.g. shared terms) along with memory management.  
")    (paper.topic       riazanov02 "System Designs")    (paper.instance    riazanov2001)    (paper.author      riazanov2001 "Alexandre Riazanov and Andrei Voronkov")    (paper.title       riazanov2001 "Splitting without Backtracking")    (paper.publication riazanov2001 "IJCAI")    (paper.startpage   riazanov2001 611)    (paper.endpage     riazanov2001 617)    (paper.link        riazanov2001 "http://rpc25.cs.man.ac.uk/voronkov/all_publications.html")    (paper.year        riazanov2001 2001)    (paper.description riazanov2001 "
The authors describe a technique for resolution that allows resolution
to break a clause apart when variables are not shared between pieces.
Suppose the clause C can be partitioned into 2 parts D and E, where each
part shares no variables with any other part.  Splitting replaces
C with the clauses D | p and E | -p.  This only makes sense when there
is an ordering imposed on the literals so that p and -p are not immediately
resolved together undoing the work of splitting.  The authors investigate
two orders: blocking and parallel.  In blocking, the -p literals 
are ranked highest; the non p literals are ranked next with negative
bigger than positive; the p atoms are ranked lowest.  In parallel,
the p literals (positive or negative) are ranked last.  Also, 
these p literals are considered names.  If there is another clause 
C' with partition D and F, then C' is split into F | -p.  Some
experimental results from TPTP show that splitting is a bad idea
with equality but that Parallel ordering with Naming is a good idea
without equaity. 
")    (paper.topic       riazanov2001 "Calculi")    (paper.instance    riazanov2003)    (paper.author      riazanov2003 "Alexandre Riazanov and Andrei Voronkov")    (paper.title       riazanov2003 "Efficient Instance Retrieval with Standard and Relational Path Indexing
")    (paper.link        riazanov2003 "http://citeseer.ist.psu.edu/594779.html")    (paper.year        riazanov2003 2003)    (paper.description riazanov2003 "
Riazanov and Voronkov follow on their first indexing paper with this one.
They compare discrimination trees and path indexing for the case of instance
retrieval.  Given a term t, find all terms u and substitutions sigma such that 
t.sigma = u.  Discrimination trees represent terms as strings; path
indexing represents terms as trees.  The authors show that with the
standard implementation of path indexing is slower than DT.  Then
they augment PI with skip lists, early cleaning to more closely
approximate perfect filtering, and a couple of other improvements.  Then
they argue for relational path indexing, utilizing database theory
to conceptualize the integration of cleaning after imperfect filtering
into the imperfect filtering.  They show how this idea makes
commutative indexing and backward subsumption on multiliteral
clauses.
")    (paper.topic       riazanov2003 "Indexing")    (paper.instance    robinson)    (paper.bibtex      robinson book)    (paper.author      robinson "Alan Robinson and Andrei Voronkov")    (paper.title       robinson "Handbook of Automated Reasoning")    (paper.publisher   robinson "MIT Press and Elsevier Science")    (paper.link        robinson "http://www.amazon.com/Handbook-Automated-Reasoning-Alan-Robinson/dp/0262182238")    (paper.year        robinson 2001)    (paper.rank        robinson "[*****]")    (paper.description robinson "
Robinson and Voronkov edit this two volume handbook.  The first volume covers
a history of the field, methods for first-order logic, methods for equality and
other built-in theories, and induction.  The second volume covers higher-order
logics, nonclassical logics, decidable classes and model building, and implementational
details.
")    (paper.topic       robinson "")    (paper.instance    robinson65machine)    (paper.bibtex      robinson65machine article)    (paper.author      robinson65machine "J.A. Robinson")    (paper.title       robinson65machine "A Machine-Oriented Logic Based on the Resolution Principle")    (paper.publication robinson65machine "Journal of the ACM")    (paper.volume      robinson65machine "12(1)")    (paper.startpage   robinson65machine 23)    (paper.endpage     robinson65machine 41)    (paper.year        robinson65machine 1965)    (paper.rank        robinson65machine "")    (paper.description robinson65machine "
The resolution paper.
")    (paper.topic       robinson65machine "Theorem Proving")    (paper.instance    roy2000)    (paper.bibtex      roy2000 article)    (paper.author      roy2000 "Pierre Roy, Anne Liret, Francois Pachet")    (paper.title       roy2000 "The Framework Approach for Constraint Satisfaction")    (paper.publication roy2000 "ACM Computing Surveys")    (paper.volume      roy2000 "32")    (paper.startpage   roy2000 13)    (paper.endpage     roy2000 16)    (paper.link        roy2000 "http://citeseer.ist.psu.edu/roy00framework.html")    (paper.year        roy2000 2000)    (paper.rank        roy2000 "[**]")    (paper.description roy2000 "
The authors argue that a Framework is a better paradigm for
allowing a user to solve CSPs than either a Library of generic
constraints or a Language built for writing solvers of CSPs. 
The former approach is too coarsely-grained, and the latter is
too finely-grained.  The framework approach sits nicely in the
middle. 
")    (paper.topic       roy2000 "OO Languages with Native Constraint Satisfaction")    (paper.instance    roy97)    (paper.author      roy97 "Pierre Roy, Francois Pachet")    (paper.title       roy97 "Reifying Constraint Satisfaction in Smalltalk")    (paper.publication roy97 "Journal of Object-Oriented Programming")    (paper.volume      roy97 "10(4)")    (paper.startpage   roy97 43)    (paper.endpage     roy97 51)    (paper.link        roy97 "http://citeseer.ist.psu.edu/roy97reifying.html")    (paper.year        roy97 1997)    (paper.rank        roy97 "[**]")    (paper.description roy97 "
Roy describes Backtalk, a system that allows a library of
generic CSP algorithms to be written.
Doing so requires reifying
the notion of a constraint, a CSP problem, a solution, etc..
")    (paper.topic       roy97 "OO Languages with Native Constraint Satisfaction")    (paper.instance    rueb2000deconstructing)    (paper.bibtex      rueb2000deconstructing inproceedings)    (paper.author      rueb2000deconstructing "Harald Rueb and Nataranjan Shankar")    (paper.title       rueb2000deconstructing "Deconstructing Shostak")    (paper.publication rueb2000deconstructing "17th International Conference on Automated Deduction
           ")    (paper.link        rueb2000deconstructing "http://citeseer.ist.psu.edu/290082.html")    (paper.year        rueb2000deconstructing 2000)    (paper.rank        rueb2000deconstructing "[****]")    (paper.description rueb2000deconstructing "
Rueb and Shankar show the crisply outlined Shostak procedure outlined
in cyrluk96shostaks and all its variants to date are incomplete.  They
go further and give a complete version of the Shostak procedure and
finally give soundness and completeness proofs.
")    (paper.topic       rueb2000deconstructing "Nelson-Oppen and Shostak")    (paper.related     rueb2000deconstructing cyrluk96shostaks)    (paper.related     rueb2000deconstructing shostak84)    (paper.instance    sabin96)    (paper.bibtex      sabin96 article)    (paper.author      sabin96 "D. Sabin and E. Freuder")    (paper.title       sabin96 "Configuration as Composite Constraint Satisfaction")    (paper.publication sabin96 " AAAI Configuration Workshop")    (paper.startpage   sabin96 28)    (paper.endpage     sabin96 36)    (paper.link        sabin96 "http://citeseer.ist.psu.edu/sabin96configuration.html")    (paper.year        sabin96 1996)    (paper.rank        sabin96 "[****]")    (paper.description sabin96 "
Sabin and Freuder define a new form of CSP which generalizes 
Meta CSPs, conditional CSPs, and hierarchical domain CSPs.
Composite CSP domain values are the set of solutions to an
entire subproblem.
")    (paper.topic       sabin96 "Various Forms of Constraint Satisfaction")    (paper.instance    sacerdoti74)    (paper.author      sacerdoti74 "Earl D. Sacerdoti")    (paper.title       sacerdoti74 "Planning in a Hierarchy of Abstraction Spaces")    (paper.publication sacerdoti74 "Artificial Intelligence")    (paper.volume      sacerdoti74 "5")    (paper.startpage   sacerdoti74 115)    (paper.endpage     sacerdoti74 135)    (paper.year        sacerdoti74 1974)    (paper.description sacerdoti74 "
The ABSTRIPS (Abstraction-based STRIPS) paper.  In 1969, McCarthy and Hayes' asserted that knowledge representation should be epistemologically adequate and also define heuristic adequacy.  Sufficiently complex problems contain too many details to be epistemologically adequate and have adequeate heuristics.  To that end, Sacerdoti first plans through an abstract plan space and then refines a solution in that space to ensure the details support the abstraction.  To implement this idea, Sacerdoti extends STRIPS by assigning 'criticality' measures to each precondition of an action.  These measures define a hierarchy of abstraction spaces.  The planner finds a solution at the highest level of abstraction, and proceeds recursively to the next highest level.  Control of the search (i.e. the cost assigned to nodes in the search space) changes based on the level of abstraction.
")    (paper.topic       sacerdoti74 "Historical")    (paper.instance    sacerdoti75)    (paper.author      sacerdoti75 "Earl D. Sacerdoti")    (paper.title       sacerdoti75 "The Nonlinear Nature of Plans")    (paper.publication sacerdoti75 "Aritificial Intelligence")    (paper.volume      sacerdoti75 "32")    (paper.startpage   sacerdoti75 333)    (paper.endpage     sacerdoti75 377)    (paper.year        sacerdoti75 1975)    (paper.description sacerdoti75 "
Sacerdoti describes NOAH (Nets Of Action Hierarchies), the first partial order
planner that relies heavily on the concept of a procedural net.  A 'procedural
net' is the data structure that maintains the
partially-ordered plan.  Much like his earlier work on
Hierarchical Planning, NOAH begins with an abstract
plan and continually refines it, alleviating conflicts through the use of
critics, until the plan consists of primitive actions.
")    (paper.topic       sacerdoti75 "Historical")    (paper.instance    sagonas94)    (paper.bibtex      sagonas94 inproceedings)    (paper.author      sagonas94 "K. Sagonas and T. Swift and D. S. Warren")    (paper.title       sagonas94 "XSB as an Efficient Deductive Database Engine")    (paper.publication sagonas94 "Proceedings of the ACM SIGMOD International Conference on the Management of Data>")    (paper.startpage   sagonas94 442)    (paper.endpage     sagonas94 453)    (paper.year        sagonas94 1994)    (paper.description sagonas94 "
XSB is Warren's current Prolog implementation, which includes tabling.
")    (paper.topic       sagonas94 "Logic Programming")    (paper.instance    sais94)    (paper.bibtex      sais94 inproceedings)    (paper.author      sais94 "Lakhdar Sais")    (paper.title       sais94 "Characterization of the Set of Models by Means of Symmetries")    (paper.publication sais94 "Second International Workshop on Principles and Practice of Constraint Programming")    (paper.link        sais94 "http://citeseer.ist.psu.edu/162645.html")    (paper.year        sais94 1994)    (paper.rank        sais94 "[**]")    (paper.description sais94 "
Sais gives an algorithm that seems to compute the characteristic models
of a given propositional, CNF theory.  There is no proof that the models
generated are exactly a set of characteristic models, but rather that
the models generated are non-symmetric.  [Maybe the equivalence is obvious,
but there is no theorem stating it.]
")    (paper.topic       sais94 "Characteristic Models")    (paper.instance    sandford80)    (paper.bibtex      sandford80 article)    (paper.author      sandford80 "David Sandford")    (paper.title       sandford80 "Using Sophisticated Models in Resolution Theorem Proving")    (paper.publisher   sandford80 "Springer-Verlag")    (paper.publication sandford80 "Lecture Notes in Computer Science")    (paper.volume      sandford80 "90")    (paper.link        sandford80 "")    (paper.year        sandford80 1980)    (paper.rank        sandford80 "[**]")    (paper.description sandford80 "
Sandford describes a new refinement of semantic resolution, hereditary 
lock resolution, and a new approach for representing models as required 
in semantic resolution.  The pertinent part of the work is the model 
representation.  Semantic resolution puts clauses into two bins throughout 
the resolution process.  Part of the input to the procedure is a model, 
any model, and those clauses that are satisfied by the model are put into 
bin 1 while clauses not satisfied by the model are put into bin 2.  
This partitioning of clauses shrinks the search space since every resolution 
must use at least one clause from the set of unsatisfied clauses.  
Sandford describes a method for using a set of sentences to define that 
model (in possibly a totally different language) and using a satisfaction 
test to determine whether a clause belongs in bin 1 or 2.   Of course, 
the computational and complexity properties could be horrible, but so 
long as the satisfaction test is sound (only returns true when the sentences 
and clause are satisfiable), we retain completeness. 
")    (paper.topic       sandford80 "Model-Guided Proof Techniques")    (paper.instance    schulz2001)    (paper.bibtex      schulz2001 inproceedings)    (paper.author      schulz2001 "S. Schulz")    (paper.title       schulz2001 "A Comparison of Different Techniques for Grounding Near-Propositional CNF Formulae")    (paper.publication schulz2001 "Proc. 15th International FLAIRS Conference")    (paper.startpage   schulz2001 72)    (paper.endpage     schulz2001 76)    (paper.link        schulz2001 "http://citeseer.ist.psu.edu/565512.html")    (paper.year        schulz2001 2002)    (paper.description schulz2001 "
Paper comparing various approaches for grounding a set of first-order, function-free clauses.  Won best-paper at FLAIR 2002.
")    (paper.topic       schulz2001 "Reformulation")    (paper.instance    schulz2002)    (paper.bibtex      schulz2002 inproceedings)    (paper.bibtex      schulz2002 article)    (paper.author      schulz2002 "Stephan Schulz")    (paper.author      schulz2002 "Stephan Schulz")    (paper.title       schulz2002 "A Comparison of Different Techniques for Grounding Near-Propositional CNF Formulae")    (paper.title       schulz2002 "E - A brainiac theorem prover")    (paper.publication schulz2002 "Proceedings, 15th International FLAIRS Conference")    (paper.publication schulz2002 "AI Communications")    (paper.volume      schulz2002 "15")    (paper.startpage   schulz2002 72)    (paper.startpage   schulz2002 111)    (paper.endpage     schulz2002 76)    (paper.endpage     schulz2002 126)    (paper.link        schulz2002 "http://citeseer.ist.psu.edu/565512.html")    (paper.link        schulz2002 "http://citeseer.ist.psu.edu/560030.html")    (paper.year        schulz2002 2002)    (paper.year        schulz2002 2002)    (paper.rank        schulz2002 "")    (paper.description schulz2002 "
Schulz compares three techniques for reducing the cost of generating 
the grounding of universal clause sets without function symbols.  The
first is hypersplitting, a generalization of which is (now) used in
the model builders that ground and use SAT solvers, structural constraints,
restricting instantiations to avoid the creation of pure literals, and 
post-processing the grounded sentences via unit subsumption, unit
propagation, and tautology deletion.  The combination of all three
outperformed any one of the techniques.
")    (paper.description schulz2002 "
Schulz describes E, a full first-order theorem prover with
equality.  It is a saturation-based prover from a purely 
equational view. It is based on the superposition calculus
with literal selection and was one of the first to use
the DISCOUNT algorithm.  The paper first describes the 
calculus, gives the proof search algorithm, discusses
search control by way of clause and literal selection,
term orderings, and auto configuration.  It delves into
implementation details: shared terms and indexing.
")    (paper.topic       schulz2002 "Herbrand Logic")    (paper.topic       schulz2002 "System Designs")    (paper.instance    schutz96)    (paper.bibtex      schutz96 article)    (paper.author      schutz96 "Heribert Schutz")    (paper.title       schutz96 "Comparison of Two Complementary Herbrand Model Generators")    (paper.link        schutz96 "http://citeseer.ist.psu.edu/122874.html")    (paper.year        schutz96 1996)    (paper.rank        schutz96 "[**]")    (paper.description schutz96 "
Schutz describes and analyzes two different Herbrand model-building 
techniques built on hyperresolution.  He shows that some of the 
strengths of one can be used to help on cases where the other is weak
and vice versa.  
")    (paper.topic       schutz96 "Model Building")    (paper.instance    selman91)    (paper.bibtex      selman91 inproceedings)    (paper.author      selman91 "Bart Selman and Henry Kautz")    (paper.title       selman91 "Knowledge Compilation Using Horn Approximations")    (paper.publication selman91 "AAAI")    (paper.link        selman91 "http://citeseer.ist.psu.edu/selman91knowledge.html")    (paper.year        selman91 1991)    (paper.rank        selman91 "[****]")    (paper.description selman91 "
This seems to be the seminal work on Knowledge Compilation.  Selman and
Kautz investigate finding two Horn approximations for a propositional 
knowledge base: one that is too strong and one that is too weak.  The 
algorithms for computing those approximations are anytime algorithms. 
To determine whether a given sentence p is entailed by the original
knowledge base, return yes if the strong approximation entails it and
no if the weak one doesn't.  If neither of these two cases hold, resort
to the original knowledge base.  Since entailment for propositional Horn
requires linear time, the worst case wastes O(2n) computation time, but
the best case might save exponential time.
")    (paper.topic       selman91 "Knowledge Base Compilation")    (paper.instance    selman96)    (paper.bibtex      selman96 article)    (paper.author      selman96 "Bart Selman and Henry Kautz")    (paper.title       selman96 "Knowledge Compilation and Theory Approximation")    (paper.publication selman96 "Journal of the ACM ")    (paper.volume      selman96 "43(2)")    (paper.startpage   selman96 193)    (paper.endpage     selman96 224)    (paper.link        selman96 "http://citeseer.ist.psu.edu/41052.html")    (paper.year        selman96 1996)    (paper.rank        selman96 "[*****]")    (paper.description selman96 "
This paper essentially glues together the work of kautz91, selman91,
kautz92,  and kautz95.  Especially noteworthy is the thorough related
work section.  It also brought together the fact that the Greatest
Upper Bound approximation of a propositional theory is always linear
in the size of the theory, but the Least Upper Bound can be exponential
in the size of the theory.
")    (paper.topic       selman96 "Knowledge Base Compilation")    (paper.related     selman96 selman91)    (paper.related     selman96 kautz91)    (paper.related     selman96 kautz92)    (paper.related     selman96 kautz95)    (paper.instance    sherl93)    (paper.author      sherl93 "R. Sherl and H. Levesque")    (paper.title       sherl93 "The frame problem and knowledge-producing actions")    (paper.link        sherl93 "http://citeseer.ist.psu.edu/scherl93frame.html")    (paper.year        sherl93 1993)    (paper.description sherl93 "None yet.
")    (paper.topic       sherl93 "Planning with Sensing Actions")    (paper.instance    shostak84)    (paper.bibtex      shostak84 article)    (paper.author      shostak84 "Robert Shostak")    (paper.title       shostak84 "Deciding Combinations of Theories")    (paper.publication shostak84 "Journal of the ACM
           ")    (paper.volume      shostak84 "31")    (paper.startpage   shostak84 1)    (paper.endpage     shostak84 12)    (paper.link        shostak84 "http://portal.acm.org/citation.cfm?id=322411&dl=ACM&coll=portal")    (paper.year        shostak84 1984)    (paper.rank        shostak84 "[****]")    (paper.description shostak84 "
Shostak describes what is now commonly known as Shostak's decision procedure.
The decision procedure works on algebraically solvable and canonizable 
equational theories.  It functions by computing the congruence closure of 
the terms in the language representing each one by their canonical forms.  
The canonization takes into account both interpreted and unintepreted
functions.  Unlike Nelson Oppen, Shostak shows how to merge the decision
procedures for two theories by taking a solver and canonizer for each and
building a single solver and canonizer pair for the union of the theories.
")    (paper.topic       shostak84 "Nelson-Oppen and Shostak")    (paper.related     shostak84 nelson79)    (paper.instance    sikka96)    (paper.bibtex      sikka96 phdthesis)    (paper.author      sikka96 "Vishal Sikka")    (paper.title       sikka96 "Integrating Specialized Procedures into Proof Systems")    (paper.publisher   sikka96 "Stanford University")    (paper.link        sikka96 "http://library.stanford.edu/depts/mathcs/mathcscoll/techreports.html")    (paper.year        sikka96 1996)    (paper.rank        sikka96 "[*****]")    (paper.description sikka96 "
Compiled code can be used by a theorem prover to expedite a proof.  
Sikka describes a general method for integrating such specialized 
procedures into proof systems using function constants, data 
structure constants, and the 'apply' operator.  
")    (paper.topic       sikka96 "Theorem Proving with Attachments")    (paper.instance    sipser)    (paper.bibtex      sipser book)    (paper.author      sipser "Michael Sipser")    (paper.title       sipser "Introduction to the Theory of Computation")    (paper.publisher   sipser "Brooks Cole")    (paper.link        sipser "http://www.amazon.com/exec/obidos/tg/detail/-/053494728X/qid=1062605920/sr=1-1/ref=sr_1_1/102-6107129-2481765?v=glance&s=books")    (paper.year        sipser 1996)    (paper.rank        sipser "[*****]")    (paper.description sipser "
One of the two definitive introductions to the theory of computation.  Sipser
covers automata and languages, computability theory, and complexity theory.
")    (paper.topic       sipser "")    (paper.instance    slaney93)    (paper.bibtex      slaney93 inproceedings)    (paper.author      slaney93 "John Slaney")    (paper.title       slaney93 "SCOTT: A Model-Guided Theorem Prover")    (paper.publication slaney93 "IJCAI")    (paper.link        slaney93 "http://citeseer.ist.psu.edu/slaney93scott.html")    (paper.year        slaney93 1993)    (paper.rank        slaney93 "[***]")    (paper.description slaney93 "
Slaney describes a case study of SCOTT (Semantically Constrained
Otter).  SCOTT consists of OTTER augmented with a model-generator
FINDER.  It restricts the resolutions done by Otter to include one
parent clause not true in the model being generated.  A new model
is generated every time a new clause is generated that is false
in the current model (up to 100 clauses).  If no new model can 
be found, the old one is used.  SCOTT retains completeness
for binary resolution since every model generated is a model of the
usable list (those axioms assumed to be consistent).  
")    (paper.topic       slaney93 "Model-Guided Proof Techniques")    (paper.instance    slaney94)    (paper.bibtex      slaney94 inproceedings)    (paper.bibtex      slaney94 inproceedings)    (paper.author      slaney94 "John Slaney")    (paper.author      slaney94 "John Slaney")    (paper.title       slaney94 "Finder: Finite Domain Enumerator System Description")    (paper.title       slaney94 "The Crisis in Finite Mathematics: Automated Reasoning as Cause and Cure")    (paper.publication slaney94 "In proceedings, CADE-12")    (paper.publication slaney94 "CADE")    (paper.link        slaney94 "http://citeseer.ist.psu.edu/252758.html")    (paper.link        slaney94 "http://arp.anu.edu.au/~jks/constraints.html")    (paper.year        slaney94 1994)    (paper.year        slaney94 1994)    (paper.rank        slaney94 "")    (paper.description slaney94 "
Very short description of Finder, a model builder for FOL.  It grounds and
then uses a combination of tableaux and negative hyperresolution.  Used
in SCOTT to constrain the search space of Otter.
")    (paper.description slaney94 "
Slaney describes the problems associated with using machines to exhaustively
check a solution space as proof of the non-existence of a solution.  Is that
really a proof?  Afterall, no one will actually verify such a proof.
But, if a theorem prover outputs a trace, we can check the trace with an
independent proof checker.  This trace-checking may be much easier than
proving the correctness of a theorem prover.
")    (paper.topic       slaney94 "Model Building")    (paper.topic       slaney94 "Theorem Proving")    (paper.instance    soutchanski2000)    (paper.author      soutchanski2000 "Mikhail Soutchanski")    (paper.title       soutchanski2000 "An On-line Decision-Theoretic Golog Interpreter")    (paper.publication soutchanski2000 "AAAI")    (paper.link        soutchanski2000 "http://www.cs.toronto.edu/~cebly/papers.html")    (paper.year        soutchanski2000 2000)    (paper.description soutchanski2000 "
Building on previous work, Southchanski couples Golog with a decision-theoretic planner based on Markov decision processes.  The original work found a optimal conditional policy, the probability that the policy could be executed, and the expected utility of the policy all offline.  This is really only possible in an environment without sensing actions with continuous return values.  Soutchanski also points out that only a single policy can be computed for an entire program, even if the program could be easily partitioned into two independent portions.  Soutchanski introduces optimize(p1);p2 to indicate that a single policy should be found for program p1 and another for program p2.  He also gives a new formulation of sensing actions of the form sense(act, value, s), meaning the result of executing the action act is value in situation s.  Since Golog is an online interpreter, sensing actions with continuous return values can be handled.
")    (paper.topic       soutchanski2000 "Logic Programming")    (paper.instance    sqalli2001)    (paper.author      sqalli2001 "Mohammed Sqalli and Eugene Freuder")    (paper.title       sqalli2001 "Solving InterOperability Problems Using Object-Oriented CSP 
 Modeling")    (paper.link        sqalli2001 "http://citeseer.ist.psu.edu/487285.html")    (paper.year        sqalli2001 2001)    (paper.rank        sqalli2001 "[**]")    (paper.description sqalli2001 "
Sqalli and Freuder describe a combination of Case-based reasoning
and constraint satisfaction.  They use Paltrinieri's OOCSP definition
except that they add methods to classes.  To solve an OOCSP, they
convert it to a standard CSP.  
")    (paper.topic       sqalli2001 "Object Oriented Constraint Satisfaction")    (paper.instance    stickel85)    (paper.bibtex      stickel85 article)    (paper.author      stickel85 "Mark Stickel")    (paper.title       stickel85 "Automated Deduction by Theory Resolution")    (paper.publication stickel85 "Journal of Automated Reasoning")    (paper.volume      stickel85 "1")    (paper.startpage   stickel85 333)    (paper.endpage     stickel85 356)    (paper.link        stickel85 "http://citeseer.ist.psu.edu/stickel85automated.html")    (paper.year        stickel85 1985)    (paper.rank        stickel85 "[***]")    (paper.description stickel85 "
The theory resolution paper.
")    (paper.topic       stickel85 "Theorem Proving with Attachments")    (paper.instance    stickel87)    (paper.author      stickel87 "Mark Stickel")    (paper.title       stickel87 "A Prolog Technology Theorem Prover")    (paper.link        stickel87 "http://citeseer.ist.psu.edu/stickel87prolog.html")    (paper.year        stickel87 1987)    (paper.description stickel87 "
")    (paper.topic       stickel87 "Calculi")    (paper.instance    stickel94)    (paper.bibtex      stickel94 article)    (paper.author      stickel94 "Mark Stickel and Richard Waldinger and Michael Lowry and Thomas Pressburger and Ian Underwood")    (paper.title       stickel94 "Deductive Composition of Astronomical Software from Subroutine Libraries")    (paper.publication stickel94 "Conference on Automated Deduction")    (paper.startpage   stickel94 341)    (paper.endpage     stickel94 355)    (paper.link        stickel94 "http://citeseer.ist.psu.edu/67907.html")    (paper.year        stickel94 1994)    (paper.rank        stickel94 "[*]")    (paper.description stickel94 "
The authors describe a system that allows a user to graphically describe
a specification for a system relying on subroutine libraries and then uses
automated deduction techniques to produce a proof of the specification from
axioms describing the library.  That proof is then translated into a 
Fortran program.
")    (paper.topic       stickel94 "Miscellaneous")    (paper.instance    stone)    (paper.bibtex      stone article)    (paper.author      stone "Nicholas Stone")    (paper.title       stone "Object-Oriented Constraint Satisfaction Planning for Whole Farm
Management")    (paper.publication stone "AI Applications")    (paper.link        stone "http://everest.ento.vt.edu/Publications/NDS-Paper-.html")    (paper.year        stone nil)    (paper.rank        stone "[*]")    (paper.description stone "
Stone describes an Object-oriented approach to writing a planner.
")    (paper.topic       stone "Miscellaneous")    (paper.instance    strzalkowski91)    (paper.bibtex      strzalkowski91 article)    (paper.author      strzalkowski91 "Tomek Strzalkowski")    (paper.title       strzalkowski91 "A General Computational Method for Grammar Inversion")    (paper.link        strzalkowski91 "http://acl.ldc.upenn.edu/W/W91/W91-0112.pdf")    (paper.year        strzalkowski91 1991)    (paper.rank        strzalkowski91 "[***]")    (paper.description strzalkowski91 "
Strzalkowski describes a procedure for converting a unification grammar defined in Prolog, which was built for sentence parsing, into another set of Prolog rules for sentence generation.
")    (paper.topic       strzalkowski91 "Natural Language Processing")    (paper.instance    suchenek93)    (paper.bibtex      suchenek93 article)    (paper.author      suchenek93 "Marek Suchenek")    (paper.title       suchenek93 "First-Order Syntactic Characterizations of Minimal Entailment, Domain-Minimal Entailment, and Herbrand Entailment")    (paper.link        suchenek93 "www.springerlink.com/index/L14483241892Q613.pdf")    (paper.year        suchenek93 1993)    (paper.rank        suchenek93 "[***]")    (paper.description suchenek93 "
Suchenek treats formally three types of minimal-model theory: minimal
entailment (cwa), Herbrand entailment (only the Herbrand models exist),
and domain-minimal entailment (only elements in the domain are those
that must be in the domain).  Touches on domain closure, unique-names,
deductive databases, closed world, non-mon.
")    (paper.topic       suchenek93 "Herbrand Logic")    (paper.instance    sussman74)    (paper.author      sussman74 "Gerald J. Sussman")    (paper.title       sussman74 "The Virtuous Nature of Bugs")    (paper.year        sussman74 1974)    (paper.description sussman74 "
Sussman describes HACKER, a total-order planner that composes stored subroutines to achieve its goal.  If subgoals conflict, HACKER identifies a bug in the plan it has generated and debugs it.  To do so, it matches a model of the behavior of the misbehaving plan with prototypes of bugs.  Once a match is found, a 'critic' is compiled with what HACKER has learned from the debugging.  In future plans, critics are used to avoid pitfalls already seen.
")    (paper.topic       sussman74 "Historical")    (paper.instance    sutcliffe98)    (paper.bibtex      sutcliffe98 article)    (paper.author      sutcliffe98 "G. Sutcliffe and C.B. Suttner")    (paper.title       sutcliffe98 "The TPTP Problem Library: CNF Release v1.2.1")    (paper.publication sutcliffe98 "Journal of Automated Reasoning")    (paper.volume      sutcliffe98 "21(2)")    (paper.startpage   sutcliffe98 177)    (paper.endpage     sutcliffe98 203)    (paper.link        sutcliffe98 "http://citeseer.ist.psu.edu/30379.html")    (paper.year        sutcliffe98 1998)    (paper.rank        sutcliffe98 "")    (paper.description sutcliffe98 "
Paper on the TPTP library.
")    (paper.topic       sutcliffe98 "Theorem Proving")    (paper.instance    sutcliffe98cade)    (paper.bibtex      sutcliffe98cade article)    (paper.author      sutcliffe98cade "Christian Suttner and Geoff Sutcliffe")    (paper.title       sutcliffe98cade "The CADE-14 ATP System Competition")    (paper.publication sutcliffe98cade "Journal of Automated Reasoning")    (paper.volume      sutcliffe98cade "21(1)")    (paper.startpage   sutcliffe98cade 99)    (paper.endpage     sutcliffe98cade 134)    (paper.link        sutcliffe98cade "http://citeseer.ist.psu.edu/87162.html")    (paper.year        sutcliffe98cade 1998)    (paper.rank        sutcliffe98cade "")    (paper.description sutcliffe98cade "
Paper on the results of one of the CASC competitions: the yearly first-order theorem proving competition held at CADE.
")    (paper.topic       sutcliffe98cade "Theorem Proving")    (paper.instance    swamy2000)    (paper.bibtex      swamy2000 phdthesis)    (paper.author      swamy2000 "Nikhil Swamy")    (paper.title       swamy2000 "A Study in Automated Reasoning about Abstract Algebra")    (paper.link        swamy2000 "http://www.cs.umd.edu/~nswamy/thesis/ch2-IntroToATP.ps")    (paper.year        swamy2000 2000)    (paper.rank        swamy2000 "")    (paper.description swamy2000 "
Swamy's second chapter (Advanced theorem proving techniques) gives a good
overview of resolution-style theorem-proving and definitions/proofs
of UR-Resolution, Hyperresolution, factoring, paramodulation,
subsumption, demodulation, set of support, and Knuth-Bendix completion.
Link goes to said chapter.
")    (paper.topic       swamy2000 "Theorem Proving")    (paper.instance    tammet2001)    (paper.bibtex      tammet2001 misc)    (paper.author      tammet2001 "Tanel Tammet")    (paper.title       tammet2001 "Finite model building: improvements and comparisons")    (paper.link        tammet2001 "http://citeseer.ist.psu.edu/675660.html")    (paper.year        tammet2001 2001)    (paper.description tammet2001 "
Comparison of Mace 2.0 and Falcon-style finite model builders, along
with some enhancements to Falcon to produce Gandolf.  Mace 2.0 used flattening,
grounding, and boolean sat solvers.  Falcon used consequence finding during
incremental model generation, much like arc consistency in CSPs.
")    (paper.topic       tammet2001 "Model Building")    (paper.instance    tammet2002)    (paper.bibtex      tammet2002 article)    (paper.author      tammet2002 "Tanel Tammete")    (paper.title       tammet2002 "Finite model building: improvements and comparisons")    (paper.link        tammet2002 "http://citeseer.ist.psu.edu/675660.html")    (paper.year        tammet2002 2002)    (paper.rank        tammet2002 "")    (paper.description tammet2002 "
Tammet gives an overview of two different styles of finite model builders
after situating the topic in the context of automated reasoning: Mace and
Falcon.  Mace operates by reducing the problem to one of propositional 
satisfiability and employing a SAT solver.  Tammet describes an augmentation
of Mace (nonground splitting) to reduce the number of distinct variables
in a clause (suboptimally).  Falcon works almost as a naive model builder
but makes certain inferences to prune the search.  Both the Mace-style
and Falcon-style builder were implemented  in Gandalf, a theorem-prover/
model checker written in Scheme and compiled into C.  Various versions
of the two builders were run over the 2002 satisfiable CASC problems,
and some conclusions were drawn.
")    (paper.topic       tammet2002 "Model Building")    (paper.instance    thiffault2004)    (paper.bibtex      thiffault2004 inproceedings)    (paper.author      thiffault2004 "Christian Thiffault and Fahiem Bacchus and Toby Walsh")    (paper.title       thiffault2004 "Solving Non-clausal Formulas with DPLL Search")    (paper.publication thiffault2004 "Proceedings of the 10th International Conference on Principles and Practice of Constraint Programming")    (paper.startpage   thiffault2004 663)    (paper.endpage     thiffault2004 678)    (paper.link        thiffault2004 "http://www.satisfiability.org/SAT04/accepted/71.html")    (paper.year        thiffault2004 2004)    (paper.description thiffault2004 "
A paper that adopts DPLL to handle Non-CNF directly.
")    (paper.topic       thiffault2004 "SAT Solving")    (paper.instance    tin94)    (paper.bibtex      tin94 article)    (paper.author      tin94 "Erkan Tin and Varol Akman")    (paper.title       tin94 "Computatinal Situation Theory")    (paper.publication tin94 "SIGART Bulletin")    (paper.volume      tin94 "5(4)")    (paper.startpage   tin94 4)    (paper.endpage     tin94 17)    (paper.link        tin94 "http://portal.acm.org/citation.cfm?id=191604.191608")    (paper.year        tin94 1994)    (paper.rank        tin94 "[*]")    (paper.description tin94 "
Tin and Akman give an overview of Situation Theory, which allows
one to describe states of the world relative to spatio and temporal
positions.  This theory is used to give semantics to natural language
utterances.  The authors then overview several systems that deal
with situation theory.
")    (paper.topic       tin94 "Miscellaneous")    (paper.instance    tinelli2000)    (paper.bibtex      tinelli2000 inproceedings)    (paper.author      tinelli2000 "Cesare Tinelli")    (paper.title       tinelli2000 "Cooperation of Background Reasoners in Theory Reasoning by
Residue Sharing")    (paper.publication tinelli2000 "International Workshop on First Order Theorem Proving (FTP)
           ")    (paper.link        tinelli2000 "http://citeseer.ist.psu.edu/572497.html")    (paper.year        tinelli2000 2000)    (paper.rank        tinelli2000 "[****]")    (paper.description tinelli2000 "
Tinelli shows how to use theory reasoning to combine specialized reasoners in
a sound and complete way in the context of semantic tableaus.  The theories
those reasoners embody must be universal, a requirement for theory reasoning.
Tinelli's results hinge on an interpolation lemma that
requires all the background theory signatures to include all the
same functions.   This lemma shows that determining unsatisfiability of
wrt a set of theories requires only passing disjunctions of literals in
the common signature among the theories.  One can thus alter the tableau
procedure by allowing a new type of derivation: hand off a subset of the
literals on a particular branch which are of the appropriate signature
to the corresponding background reasoner.  If the reasoner returns
'unsatisfiable', close the branch; otherwise, add the residue to the
end of the branch.
")    (paper.topic       tinelli2000 "Theorem Proving with Attachments")    (paper.instance    torlak2006)    (paper.bibtex      torlak2006 techreport)    (paper.author      torlak2006 "Emina Torlak and Daniel Jackson")    (paper.title       torlak2006 "The Design of a Relational Engine")    (paper.publisher   torlak2006 "MIT-CSAIL")    (paper.link        torlak2006 "http://web.mit.edu/emina/www/kodkod.html")    (paper.year        torlak2006 2006)    (paper.description torlak2006 "
The authors describe a model-builder for a relational logic with transitive
closure and a finite domain.  The system does symmetry detection, compresses
expressions using compact boolean circuits, and
translates to propositional logic to answer queries using a SAT solver.
")    (paper.topic       torlak2006 "Model Building")    (paper.instance    ullman)    (paper.bibtex      ullman book)    (paper.author      ullman "J. Hopcroft and J. Ullman")    (paper.title       ullman "Introduction to Automata Theory, Languages, and Computation")    (paper.publisher   ullman "Addison Wesley")    (paper.link        ullman "http://www-db.stanford.edu/~ullman/ialc.html")    (paper.year        ullman 1979)    (paper.rank        ullman "[*****]")    (paper.description ullman "
One of the two definitive introductions to the theory of computation. Hopcroft
and Ullman--the classic.
")    (paper.topic       ullman "")    (paper.instance    ullman96)    (paper.bibtex      ullman96 article)    (paper.author      ullman96 "Jeffrey Ullman")    (paper.title       ullman96 "The Database Approach to Knowledge Representation")    (paper.publication ullman96 "AAAI")    (paper.link        ullman96 "http://citeseer.ist.psu.edu/ullman96database.html")    (paper.year        ullman96 1996)    (paper.rank        ullman96 "[****]")    (paper.description ullman96 "
Ullman explains to the AI community the database perspective on
knowledge representation: start at low expressiveness and efficient computation
and work your way up.  Datalog (prolog w/o negation or functions)
is given the fixed-point semantics.  When extended with negation,
the least-fixed-point semantics are troublesome; stratified negation
and well-founded semantics have been developed to handle this.
Query containment of conjunctive queries is NP complete in general,
but is linear if no predicate appears more than twice in any
query.  Whether a conjunctive query is contained in a Datalog
program is exponential, and the reverse is decidable.  Rewriting
a query in terms of views has been done when the views are 
conjunctive queries as well as when the views are described
by Datalog programs.  
")    (paper.topic       ullman96 "Logical Knowledge Representation")    (paper.instance    ullmandb)    (paper.bibtex      ullmandb book)    (paper.author      ullmandb "Jeffrey Ullman")    (paper.title       ullmandb "Principles of Database and Knowledge-Base Systems")    (paper.publisher   ullmandb "Computer Science Press")    (paper.link        ullmandb "http://www.amazon.com/exec/obidos/tg/detail/-/0716781581/qid=1110684085/sr=8-3/ref=sr_8_xs_ap_i3_xgl14/104-0791189-4759116?v=glance&s=books&n=507846")    (paper.year        ullmandb 1989)    (paper.rank        ullmandb "[*****]")    (paper.description ullmandb "
This two-volume set describes how database principles can be applied to 
produce knowledge-based systems.
")    (paper.topic       ullmandb "")    (paper.instance    uribe2000)    (paper.bibtex      uribe2000 inproceedings)    (paper.author      uribe2000 "Tomas Uribe")    (paper.title       uribe2000 "Combinations of Model Checking and Theorem Proving")    (paper.publication uribe2000 "Frontiers of Combining Systems (FroCoS)")    (paper.startpage   uribe2000 151)    (paper.endpage     uribe2000 170)    (paper.link        uribe2000 "http://citeseer.ist.psu.edu/uribe00combinations.html")    (paper.year        uribe2000 2000)    (paper.rank        uribe2000 "[****]")    (paper.description uribe2000 "
Uribe surveys combinations of model checkers and theorem provers in the
context of formal verification.  The paper is pretty much self-contained,
explaining transition systems, fair transition systems,
finite-state model checking, deductive verification, abstraction, and
invariant generation.   Then Uribe discusses loosely coupled systems,
which treat the model checker/theorem prover as a black box. Modularity
and abstraction, general deductive environments (debugging, incremental
verification, and formal decomposition), and abstraction generation using
theorem proving have all been investigated in this loosely-coupled way.  
Lastly he considers tightly coupled systems.  Diagram-based
formalisms provide abstractions.  Model checking infinite-state systems
can also be attacked with abstraction.  A handful of tightly integrated
systems are also mentioned.  This survey feels like it is made up of a
hodge-podge of attempts--whether that is characteristic of the field
or the paper is unclear.
")    (paper.topic       uribe2000 "Model Checking")    (paper.instance    vangelder91)    (paper.bibtex      vangelder91 article)    (paper.author      vangelder91 "Allen van Gelder and Kenneth Ross and John Schlipf")    (paper.title       vangelder91 "The Well-Founded Semantics for General Logic Programs")    (paper.publication vangelder91 "Journal of the ACM")    (paper.volume      vangelder91 "38(3)")    (paper.startpage   vangelder91 620)    (paper.endpage     vangelder91 650)    (paper.link        vangelder91 "http://citeseer.ist.psu.edu/gelder91wellfounded.html")    (paper.year        vangelder91 1991)    (paper.description vangelder91 "
Well-founded semantics for logic programming.
")    (paper.topic       vangelder91 "Logic Programming")    (paper.instance    vanhoof2004)    (paper.bibtex      vanhoof2004 article)    (paper.author      vanhoof2004 "Wim Vanhoof")    (paper.title       vanhoof2004 "Searching Semantically Equivalent Code Fragments in Logic Programs")    (paper.publication vanhoof2004 "14th International Symposium, Logic Based Program Synthesis and Trnasformation (LOPSTR)")    (paper.startpage   vanhoof2004 1)    (paper.endpage     vanhoof2004 18)    (paper.link        vanhoof2004 "http://wotan.liu.edu/docis/dbl/lopstr/index.html")    (paper.year        vanhoof2004 2004)    (paper.description vanhoof2004 "
In the context of logic programming over Horn rules, 
Vanhoof considers three problems: (1) determining when some portion
of a rule body is equivalent to a portion of another rule body, (2) 
determining when two relations are identical up to argument permutation,
and (3) whether two portions of rule bodies share enough functionality
to be generalized as a higher-order rule, e.g. mapcar.  The nice
part about this topic of semantic equivalence in LP is that two
relations/conjunctions are equivalent iff they have the same extensions.
Vanhoof gives approximation algorithms, i.e. algorithms that are 
sufficient, for determining various kinds of semantic equivalences.
")    (paper.topic       vanhoof2004 "Reformulation")    (paper.instance    vardi82)    (paper.bibtex      vardi82 inproceedings)    (paper.author      vardi82 "Moshe Vardi")    (paper.title       vardi82 "The complexity of relational query languages")    (paper.publication vardi82 "Proceedings of the fourteenth annual ACM symposium on Theory of computing")    (paper.startpage   vardi82 137)    (paper.endpage     vardi82 146)    (paper.link        vardi82 "http://portal.acm.org/citation.cfm?id=802186&dl=ACM&coll=GUIDE")    (paper.year        vardi82 1982)    (paper.description vardi82 "
Data and expression complexity results for various database query languages.
")    (paper.topic       vardi82 "Databases")    (paper.instance    vorobyov98)    (paper.bibtex      vorobyov98 inproceedings)    (paper.author      vorobyov98 "Sergei Vorobyov and Andrei Voronkov")    (paper.title       vorobyov98 "Complexity of Nonrecursive Logic Programs with Complex Values")    (paper.publication vorobyov98 "Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems")    (paper.startpage   vorobyov98 244)    (paper.endpage     vorobyov98 253)    (paper.link        vorobyov98 "http://citeseer.ist.psu.edu/258671.html")    (paper.year        vorobyov98 1998)    (paper.description vorobyov98 "
Breaks down the complexity of nonrecursive logic programs based on the 
signature/vocabulary of the program.
")    (paper.topic       vorobyov98 "Logic Programming")    (paper.instance    waal93)    (paper.bibtex      waal93 article)    (paper.author      waal93 "D.A. de Waal and J. Gallagher")    (paper.title       waal93 "The Applicability of Logic Program Analysis and Transformation to Theorem Proving")    (paper.publication waal93 "CADE")    (paper.link        waal93 "http://citeseer.ist.psu.edu/dewaal93applicability.html")    (paper.year        waal93 1993)    (paper.rank        waal93 "[****]")    (paper.description waal93 "
de Waal and Gallagher show how to employ the results of (1) approximation
of logic programs and (2) partial evaluation to produce strategies
for pruning the search space of theorem proving procedures.  They model
a theorem proving algorithm as a logic program, add in the theory and query
in question, and run approximation techniques after partially evaluating
the constructed logic program.  Regular unary clauses (r(f(x1,...,xn) <= 
t1(x1) ^ ... ^ tn(xn))are used to
do the approximations, but the technique can be applied to different
approximations as well.  The authors demonstrate this technique on
a model-elimination procedure and a Naive nH-Prolog proof system.
")    (paper.topic       waal93 "Metalevel Reasoning")    (paper.instance    waldinger75)    (paper.author      waldinger75 "R. Waldinger")    (paper.title       waldinger75 "Achieving several goals simultaneously")    (paper.publication waldinger75 "Machine Intelligence 8")    (paper.startpage   waldinger75 94)    (paper.endpage     waldinger75 138)    (paper.year        waldinger75 1975)    (paper.description waldinger75 "
Waldinger examines planning where sugoals interfere with one another.  He introduces regression  planning, where totally ordered plan steps are reordered.  A solution is constructed incrementally for each subgoal, but when a later subgoal interferes, the position of that subgoal is moved earlier in the plan.  He also touches on the ramification problem (but doesn't call it that) in an effort to show the STRIPS assumption as an unsatisfactory solution to the frame problem.
")    (paper.topic       waldinger75 "Historical")    (paper.instance    walsh2000)    (paper.bibtex      walsh2000 article)    (paper.author      walsh2000 "Toby Walsh")    (paper.title       walsh2000 "Reformulating Propositional Satisfiability as Constraint Satisfaction")    (paper.publication walsh2000 "Symposium on Abstraction, Reformulation, and Approximation (SARA)")    (paper.startpage   walsh2000 233)    (paper.endpage     walsh2000 246)    (paper.link        walsh2000 "http://citeseer.ifi.unizh.ch/walsh00reformulating.html")    (paper.year        walsh2000 2000)    (paper.description walsh2000 "
Walsh gives an overview of various approaches to encoding propositional
satisfiability as CSPs.  Then he goes on to discuss how various
algorithms operate on those CSPs.
")    (paper.topic       walsh2000 "Reformulation")    (paper.instance    wam)    (paper.bibtex      wam book)    (paper.author      wam "Hassan Ait-Kaci")    (paper.title       wam "Warren's Abstract Machine: A Tutorial Reconstruction")    (paper.publisher   wam "MIT Press")    (paper.link        wam "http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=7292")    (paper.year        wam 1991)    (paper.rank        wam "[*****]")    (paper.description wam "
Description of the original Prolog compiler.
")    (paper.topic       wam "")    (paper.instance    wang85)    (paper.bibtex      wang85 inproceedings)    (paper.author      wang85 "Tie Cheng Wang")    (paper.title       wang85 "Designing Examples for Semantically Guided Hierarchical Deduction")    (paper.publication wang85 "IJCAI")    (paper.year        wang85 1985)    (paper.rank        wang85 "[*]")    (paper.description wang85 "
This paper describes a hierarchical theorem prover guided by a model
and gives guidelines for humans to pick a good model.
")    (paper.topic       wang85 "Model-Guided Proof Techniques")    (paper.instance    warren74)    (paper.author      warren74 "D.H.D. Warren")    (paper.title       warren74 "Extract from APIC Studies in Data Processing")    (paper.year        warren74 1974)    (paper.description warren74 "
Quick synopsis of the Prolog implementation of Warren's WARPLAN.  WARPLAN was the first planner implemented in Prolog.  It was not optimal, sometimes finding longer plans than needed.
")    (paper.topic       warren74 "Historical")    (paper.instance    weyhrauch80)    (paper.bibtex      weyhrauch80 article)    (paper.author      weyhrauch80 "Richard Weyhrauch")    (paper.title       weyhrauch80 "Prolegomena to a theory of mechanized formal reasoning")    (paper.publication weyhrauch80 "Artificial Intelligence")    (paper.volume      weyhrauch80 "13")    (paper.startpage   weyhrauch80 133)    (paper.endpage     weyhrauch80 170)    (paper.year        weyhrauch80 1980)    (paper.rank        weyhrauch80 "[***]")    (paper.description weyhrauch80 "
Weyhrauch describes the FOL system.  FOL allows rules to be written that
produce semantic attachments, i.e. attaching Lisp addition to the function
symbol '+'.  It also allows rules that rewrite expressions.  FOL includes
an evaluator for first order logic that combines the rewrite rules with
the semantic attachments to do proofs.  FOL is a system that operates on
'LS pairs': a set of logical sentences, a set of attachments, and a set
of facts.  Because this is done uniformly, FOL allows multiple LS pairs
to be defined at the same time.  A special LS pair, Meta, is a theory
of LS pairs; reflection is achieved in FOL by allowing Meta to
operate on another LS pair.   Metametalevel reasoning
can be accomplished in the same way.  Self-reflection is another special
case of this mechanism--give Meta the LS pair Meta to operate on.
")    (paper.topic       weyhrauch80 "Theorem Proving with Attachments")    (paper.instance    yamatomoto2000)    (paper.author      yamatomoto2000 "Akihiro Yamatomoto, Bertram Fronhofer")    (paper.title       yamatomoto2000 "Hypothesis Finding via Residue Hypotheses with the Resolution Principle")    (paper.link        yamatomoto2000 "http://link.springer.de/link/service/series/0558/bibs/1968/19680156.htm")    (paper.year        yamatomoto2000 2000)    (paper.description yamatomoto2000 "
Given a set of background sentences B that do not entail a positive example E, we want to find a hypothesis H such that
B and H entail E.  The authors argue for the use of anti-subsumption over anti-instantiation as a method for
finding residue hypotheses.
")    (paper.topic       yamatomoto2000 "Theorem Proving Applications")    (paper.instance    zhang90)    (paper.bibtex      zhang90 article)    (paper.author      zhang90 "Weining Zhang and Clement Yu and Daniel Troy")    (paper.title       zhang90 "Necessary and Sufficient Conditions to Linearize Doubly Recursive Programs in Logic Databases")    (paper.publication zhang90 "ACM Transactions on Database Systems")    (paper.volume      zhang90 "15(3)")    (paper.startpage   zhang90 459)    (paper.endpage     zhang90 482)    (paper.link        zhang90 "http://portal.acm.org/citation.cfm?coll=GUIDE&dl=GUIDE&id=89237")    (paper.year        zhang90 1990)    (paper.description zhang90 "
The authors examine conditions under which a doubly recursive program, i.e. a program of the form r(xbar) if s(xbar) and r(xbar) if r(ybar) ^ q(ybar') ^ r(ybar''), can be expressed as a linear recursive program, i.e. where the first recursive r definition is resolved with s(xbar).  The only assumption is that 
all the variables in the head appear somewhere in the body.  This result
strengthens previous results.  All rules are horn, and the semantics are LP. 
The necessary and sufficient conditions are nasty, as you might expect.
")    (paper.topic       zhang90 "Reformulation")    (paper.instance    zhang95)    (paper.bibtex      zhang95 inproceedings)    (paper.author      zhang95 "J. Zhang and H. Zhang")    (paper.title       zhang95 "SEM: A System for Enumerating Models")    (paper.publication zhang95 "In proceedings, IJCAI-95, Morgan Kaufmann")    (paper.link        zhang95 "http://citeseer.ist.psu.edu/context/34167/0")    (paper.year        zhang95 1995)    (paper.description zhang95 "
Model builder based on searching for a model directly and using the Least Number Heuristic, a method for reducing symmetry in the search space.
")    (paper.topic       zhang95 "Model Building")    (paper.instance    zhang96)    (paper.bibtex      zhang96 article)    (paper.author      zhang96 "J. Zhang")    (paper.title       zhang96 "Constructing Finite Algebras with FALCON")    (paper.publication zhang96 "Journal of Automated Reasoning")    (paper.volume      zhang96 "17(1)")    (paper.startpage   zhang96 1)    (paper.endpage     zhang96 22)    (paper.link        zhang96 "http://citeseer.ist.psu.edu/context/254103/0")    (paper.year        zhang96 1996)    (paper.description zhang96 "
Model builder based on searching for a model directly, the so-called
Falcon-style approach.
")    (paper.topic       zhang96 "Model Building")    (paper.instance    zhang97)    (paper.bibtex      zhang97 article)    (paper.author      zhang97 "Yan Zhang and Norman Foo")    (paper.title       zhang97 "Deriving Invariants and Constraints from Action Theories")    (paper.publication zhang97 "Fundamenta Informaticae")    (paper.volume      zhang97 "30(1)")    (paper.startpage   zhang97 109)    (paper.endpage     zhang97 123)    (paper.link        zhang97 "http://citeseer.ist.psu.edu/zhang96deriving.html")    (paper.year        zhang97 1997)    (paper.rank        zhang97 "[*****]")    (paper.description zhang97 "
Zhang and Foo describe an approach to the construction of action invariants
(statements that hold in the predecessor and successor states when
executing a particular action) and state constraints (statements
true in all states reachable from an initial state).   It is based
on a state-based persistence formalism (Zhang's work), and assumes
a domain-closure axiom.  It considers actions with disjunctive effects
and preconditions, but does not provide complete results for those
cases.  It relies on the use of induction to prove a candidate
state constraint is actually a state constraint, but the generation
of these candidates is done algorithmically.
")    (paper.topic       zhang97 "Theories of Action")    (paper.instance    zhu98)    (paper.bibtex      zhu98 phdthesis)    (paper.author      zhu98 "Yunshan Zhu")    (paper.title       zhu98 "Efficient First-Order Semantic Deduction Techniques")    (paper.link        zhu98 "http://citeseer.ist.psu.edu/zhu98efficient.html")    (paper.year        zhu98 1998)    (paper.rank        zhu98 "[*]")    (paper.description zhu98 "
This is Zhu's thesis on Ordered Semantic Hyperlinking, complexity
measures of theorem proving, and OSHL applied to planning.
Contains some good citations to semantically driven proof techniques
in section 2.1.
")    (paper.topic       zhu98 "Resolution Variants")    (paper.related     zhu98 plaisted97);;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;